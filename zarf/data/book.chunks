Welcome Back in August 2019 Hoanh An started a project in Github called the Ultimate Go Study Guide It was a collection of notes he took after taking the Ultimate Go class Surprisingly it got a lot of attention and eventually had more stars and activity than the actual repo for the class This shows the power of open sourcing material Then Hoanh decided to publish a book from his notes and repo When I saw what Hoanh had written and the excitement his followers had I reached out to him We decided I would review and refactor his original work and we would publish a book together This is that book and it represents the notes I would like any student to make while taking the class I want to thank everyone in the Go community for their support and help over the years in creating this material When I started learning Go in March 2013 I didnt have any idea I would be able to accumulate all this knowledge and share it with all of you Learning is a journey that takes time and effort If this material can help jump start your learning about Go then the time and effort was worth every minute Thanks  Bill Kennedy  3  
Intended Audience This notebook has been written and designed to provide a reference to everything that I say in the Ultimate Go class Its not necessarily a beginners Go book since it doesnt focus on the specifics of Gos syntax I would recommend the Go In Action book I wrote back in 2015 for that type of content Its still accurate and relevant Many of the things I say in the classroom over the 20 plus hours of instruction has been incorporated Ive tried to capture all the guidelines design philosophy whiteboarding and notes I share at the same moments I share them If you have taken the class before I believe this notebook will be invaluable for reminders on the content If you have never taken the class I still believe there is value in this book It covers more advanced topics not found in other books today Ive tried to provide a well rounded curriculum of topics from types to profiling I have also been able to provide examples for writing generic function and types in Go which will be available in version 118 of Go The book is written in the first person to drive home the idea that this is my book of notes from the Ultimate Go class The first chapter provides a set of design philosophies quotes and extra reading to help prepare your mind for the material Chapters 213 provide the core content from the class Chapter 14 provides a reediting of important blog posts Ive written in the past These posts are presented here to enhance some of the more technical chapters like garbage collection and concurrency If you are struggling with this book please provide me any feedback over email at billardanlabscom I will always do my best to correct and teach anything that is not obvious or clear Thanks  Bill Kennedy  4  
Acknowledgements WILLIAM KENNEDY I would like to dedicate this book to my kids Brianna Melissa Amanda Jarrod Thomas and the love of my life Alejandra Thank you for the constant support and understanding you provide me on a daily basis I dont believe this project would have been completed if I didnt have all of you in my life I would also like to thank my Ardan family Ed John Miguel and Erick Without your support over the past 10 years I wouldnt be capable of spending the time on projects like this  HOANH AN I want to thank my family and my partner Dani for always being there supporting me and encouraging me to keep working on the project since the very early days You are the most beautiful kind and smartest person Ive known and loved I want to thank Bill and the Ardan team for your understanding and support over the last year Additionally I want to thank all of the Ultimate Go Study Guides supporters for taking the time and making the effort to not just contribute to the project but help share and send your detailed feedback This book exists because of all of you  
Chapter 1 Introduction Prototype Driven Development with Data Oriented Design Its important that I prepare my mind for the material Im about to review This introduction provides thoughts and ideas to stimulate my initial understanding of the language its roots and general design philosophy Its written as a set of notes and not fluid content like I will find in the remaining chapters Somewhere Along The Line   We became impressed with programs that contain large amounts of code    We strived to create large abstractions in our code base    We forgot that the hardware is the platform    We lost the understanding that every decision comes with a cost  These Days Are Gone   We can throw more hardware at the problem    We can throw more developers at the problem  Open My Mind   Technology changes quickly but peoples minds change slowly    Easy to adopt new technology but hard to adopt new ways of thinking  Interesting Questions  What do they mean to me   Is it a good program    Is it an efficient program    Is it correct    Was it done on time    What did it cost  Aspire To   Be a champion for quality efficiency and simplicity    Have a point of view    Value introspection and selfreview  
11 Reading Code Go is a language that focuses on code being readable as a first principle Quotes   If most computer people lack understanding and knowledge then what they will select will also be lacking  Alan Kay    The software business is one of the few places we teach people to write before we teach them to read  Tom Love inventor of Objective C    Code is read many more times than it is written  Dave Cheney    Programming is among other things a kind of writing One way to learn writing is to write but in all other forms of writing one also reads We read  10  examples both good and bad to facilitate  learning But how many  programmers learn to write programs by reading programs  Gerald M Weinberg   Skill develops when we produce not consume  Katrina Owen  
12 Legacy Software Do I care about the legacy Im leaving behind Quotes   There are two kinds of software projects those that fail and those that turn into legacy horrors  Peter Weinberger inventor of AWK    Legacy software is an unappreciated but serious problem Legacy code may be the downfall of our civilization  Chuck Moore inventor of Forth    Few programmers of any experience would contradict the assertion that most programs are modified in their lifetime Why then do we rarely find a program that contains any evidence of having been written with an eye to subsequent modification  Gerald M Weinberg    We think awful code is written by awful devs But in reality its written by reasonable devs in awful circumstances  Sarah Mei    There are many reasons why programs are built the way they are although we may fail to recognize the multiplicity of reasons because we usually look at code from the outside rather than by reading it When we do read code we find that some of it gets written because of machine limitations some because of language limitations some because of programmer limitations some because of historical accidents and some because of specifications both essential and inessential  Gerald M Weinberg  
13 Mental Models I must constantly make sure my mental model of the code Im writing and maintaining is clear When I cant remember where a piece of logic is or I cant remember how something works Im losing my mental model of the code This is a clear indication that I need to refactor the code Focus time on structuring code that provides the best mental model possible and during code reviews validate my mental models are still intact How much code do I think I can maintain in my head I believe asking a single developer to maintain a mental model of more than one ream of copy paper 10k lines of code is asking a lot If I do the math it takes a team of 100 people to work on a code base that hits a million lines of code Thats 100 people that need to be coordinated grouped tracked and in a constant feedback loop of communication  11  Quotes   Lets imagine a project thats going to end up with a million lines of code or more The probability of those projects being successful in the United States these days is very low well under 50 Thats debatable  Tom Love inventor of Objective C    100k lines of code fit inside a box of paper  Tom Love inventor of Objective C    One of our many problems with thinking is cognitive load the number of things we can pay attention to at once The cliche is 72 but for many things it is even less We make progress by making those few things be more powerful  Alan Kay    The hardest bugs are those where your mental model of the situation is just wrong so you cant see the problem at all  Brian Kernighan    Everyone knows that debugging is twice as hard as writing a program in the first place So if youre as clever as you can be when you write it how will you ever debug it  Brian Kernighan    Debuggers dont remove bugs They only show them in slow motion Unknown    Fixing bugs is just a side effect Debuggers are for exploration  Deech Twitter  Reading   The Magical Number Seven Plus or Minus Two  Wikipedia    Psychology of Code Readability  Egon Elbre  
14 Productivity vs Performance Productivity and performance both matter but in the past I couldnt have both I needed to choose one over the other We naturally gravitated to productivity with the idea or hope that the hardware would resolve our performance problems for free  This  movement  towards  productivity  has  resulted  in  the  design  of  programming languages that produce sluggish software that is outpacing the hardwares ability to make them faster By following Gos idioms and a few guidelines I can write code that can be reasoned about by average developers I can write software that simplifies minimizes and reduces the amount of code we need to write to solve the problems we are working on I dont have to choose productivity over performance or performance over productivity anymore I can have both  12  Quotes   The hope is that the progress in hardware will cure all software ills However a critical observer may observe that software manages to outgrow hardware in size and sluggishness Other observers had noted this for some time before indeed the trend was becoming obvious as early as 1987 Niklaus Wirth    The most amazing achievement of the computer software industry is its continuing cancellation of the steady and staggering gains made by the computer hardware industry  Henry Petroski 2015    The hardware folks will not put more cores into their hardware if the software isnt going to use them so it is this balancing act of each other staring at each other and we are hoping that Go is going to break through on the software side  Rick Hudson 2015    C is the best balance Ive ever seen between power and expressiveness You can  do  almost  anything  you  want  to  do  by  programming  fairly  straightforwardly and you will have a very good mental model of whats going to happen on the machine you can predict reasonably well how quickly its going to run you understand whats going on  Brian Kernighan 2000   The trend in programming language design has been to create languages that enhance software reliability and programmer productivity What we should do is develop languages alongside sound software engineering practices  so  the  task  of  developing  reliable  programs  is distributed  throughout the software lifecycle especially into the early phases of system design  Al Aho 2009  
15 Correctness vs Performance I want to write code that is optimized for correctness Dont make coding decisions based on what I think might perform better I must benchmark or profile to know if code is not fast enough Then and only then should I optimize for performance This cant be done until I have something working Improvement comes from writing code and thinking about the code I write Then refactoring the code to make it better This requires the help of other people to also read the code Im writing Prototype ideas first to validate them Try different approaches or ask others to attempt a solution Then compare what I have learned Too many developers are not prototyping their ideas first before writing production code Its through prototyping that I can validate my thoughts ideas and designs This is the time when I can break down walls and figure out how things work Prototype in the concrete and consider contracts after I have a working prototype Refactoring must become part of the development cycle Refactoring is the process  13  of improving the code from the things that I learn on a daily basis Without time to refactor code will become impossible to manage and maintain over time This creates the legacy issues we are seeing today Quotes   Make it correct make it clear make it concise make it fast In that order Wes Dyer    Make it work then make it beautiful then if you really really have to make it fast 90 percent of the time if you make it beautiful it will already be fast So really just make it beautiful  Joe Armstrong    Good engineering is less about finding the perfect solution and more about understanding the tradeoffs and being able to explain them  JBD    Choosing the right limitations for a certain problem domain is often much more powerful than allowing anything  Jason Moiron    The correctness of the implementation is the most important concern but there is no royal road to correctness It involves diverse tasks such as thinking of invariants testing and code reviews Optimization should be done but not prematurely  Al Aho inventor of AWK    The basic ideas of good style which are fundamental to write clearly and simply are just as important now as they were 35 years ago Simple straightforward code is just plain easier to work with and less likely to have problems As programs get bigger and more complicated its even more important to have clean simple code  Brian Kernighan    Problems can usually be solved with simple mundane solutions That means theres no glamorous work You dont get to show off your amazing skills You just build something that gets the job done and then move on This approach may not earn you oohs and aahs but it lets you get on with it  Jason Fried  Reading   Prototype your design  Robert Griesemer  
16 Understanding Rules What should I understand about rules  14    Rules have costs    Rules must pull their weight  Dont be clever high level    Value the standard dont idolize it    Be consistent    Semantics convey ownership  Quotes   An architecture isnt a set of pieces its a set of rules about what you can expect of them  Michael Feathers  Reading   The Philosophy of Googles C Code  Titus Winters  
17 Differences Between Senior vs Junior Developers What is the difference between a Senior and Junior developer Quotes   You are personally responsible for the software you write  Stephen Bourne Bourne shell    And the difference between juniorsseniors to those who are inbetween is the confidence to ask dumb questions  Natalie Pistunovich    Mistakes are an inevitable consequence of doing something new and as such should be seen as valuable without them wed have no originality Ed Catmull President of Pixar    It takes considerable knowledge just to realize the extent of your own ignorance  Thomas Sowell    If you dont make mistakes youre not working on hard enough problems Frank Wilczek    Dont cling to a mistake because you spent so much time making it Aubrey de Grey  
18 Design Philosophy I cant look at a piece of code and determine if it smells good or bad without a design philosophy These four major categories are the basis for code reviews and should be prioritized in this order Integrity Readability Simplicity and then Performance I must consciously and with great reason be able to explain the category Im choosing  
181 Integrity I need to become very serious about reliability There are two driving forces behind integrity   Integrity is about every allocation read and write of memory being accurate consistent and efficient The type system is critical to making sure we have this micro level of integrity    Integrity is about every data transformation being accurate consistent and efficient Writing less code and error handling is critical to making sure we 15  have this macro level of integrity Write Less Code There have been studies that have researched the number of bugs I can expect to have in my software The industry average is around 15 to 50 bugs per 1000 lines of code One simple way to reduce the number of bugs and increase the integrity of my software is to write less code Bjarne Stroustrup stated that writing more code than I need results in Ugly Large and Slow code   Ugly Leaves places for bugs to hide    Large Ensures incomplete tests    Slow Encourages the use of shortcuts and dirty tricks  Error Handling When error handling is treated as an exception and not part of the main code path I can expect the majority of my critical failures to be due to error handling There was a study that looked at a couple hundred bugs in Cassandra HBase HDFS MapReduce and Redis The study identified 48 critical failures that fell into these categories   92 Failures from bad error handling       35 Incorrect handling   25 Simply ignoring an error    8 Catching the wrong exception    2 Incomplete TODOs  57 System specific   23 Easily detectable    34 Complex bugs  8 Failures from latent human errors  Quotes   Failure is expected failure is not an odd case Design systems that help you identify failure Design systems that can recover from failure  JBD    Product excellence is the difference between something that only works under certain conditions and something that only breaks under certain conditions  Kelsey Hightower    16  Instability is a drag on innovation  Yehudah Katz  Reading   Software Development for Infrastructure  Bjarne Stroustrup    Normalization of Deviance in Software  danluucom    Lessons learned from reading postmortems  danluucom    Technical Debt Quadrant  Martin Fowler    Design Philosophy On Integrity  William Kennedy    Ratio of bugs per line of code  Dan Mayer    Developing Software The Right Way with Intent and Carefulness   David  Gee   What bugs live in the Cloud  usenixorg    Masterminds of Programming  Federico Biancuzzi and Shane Warden  
182 Readability I must structure my systems to be more comprehensible This is about writing simple code that is easier to read and understand without the need of mental exhaustion Just as important its about not hiding the costimpact of the code per line function package and the overall ecosystem it runs in Code Must Never Lie It doesnt matter how fast the code might be if no one can understand or maintain it moving forward Average Developer I must be aware of who I am on my team When hiring new people I must be aware of where the new person falls Code must be written for the average developer to comprehend If Im below average for my team I have the responsibility to work to be average If Im above average I have the responsibility to reduce writing clever code and coachmentor Real Machine In Go the underlying machine is a real machine unlike what I would find in Java or C with their virtual machine layer The model of computation is that of the computer Here is the key Go gives me direct access to the machine while still providing abstraction mechanisms to allow higherlevel ideas to be expressed Quotes   This is a cardinal sin amongst programmers If code looks like its doing one thing when its actually doing something else someone down the road will read that code and misunderstand it and use it or alter it in a way that causes bugs That someone might be you even if it was your code in the first place  Nate Finch    Can you explain it to the median user developer as opposed to will the smartest user developer figure it out  Peter Weinberger inventor of 17  AWK   Making things easy to do is a false economy Focus on making things easy to understand and the rest will follow  Peter Bourgon  Reading   Code Must Never Lie  Nate Finch  
183 Simplicity I must understand that simplicity is hard to design and complicated to build This is about hiding complexity A lot of care and design must go into simplicity because it can cause more problems than it solves It can create issues with readability and it can cause issues with performance Complexity Sells Better Focus on encapsulation and validate that Im not generalizing or even being too concise I need to valid my code is still easy to use understand debug and maintain Encapsulation Encapsulation is what the industry has been trying to figure out for 40 years Go is taking a slightly new approach with packaging Bringing encapsulation up a level and providing richer support at the language level Quotes   Simplicity is a great virtue but it requires hard work to achieve it and education to appreciate it And to make matters worse complexity sells better  Edsger W Dijkstra    Everything should be made as simple as possible but not simpler  Albert Einstein    You wake up and say I will be productive not simple today  Dave Cheney    Paraphrasing Encapsulation and the separation of concerns are drivers for designing software This is largely based on how other industries handle complexity There seems to be a human pattern of using encapsulation to wrestle complexity to the ground  Brad Cox inventor of Objective C    The purpose of abstraction is not to be vague but to create a new semantic level in which one can be absolutely precise  Edsger W Dijkstra    A proper abstraction decouples the code so that every change doesnt echo throughout the entire code base  Ronna Steinburg   18  A good API is not just easy to use but also hard to misuse  JBD    Computing is all about abstractions Those below yours are just details Those above yours are limiting complicated crazy town  Joe Beda  Reading   Simplicity is Complicated  Rob Pike    What did Alan Kay mean by Lisp is the greatest single programming language ever designed  Alan Kay  
184 Performance I must compute less to get the results we need This is about not wasting effort and achieving execution efficiency Writing code that is mechanically sympathetic with the runtime operating system and hardware Achieving performance by writing less and more efficient code but staying within the idioms and framework of the language Rules of Performance   Never guess about performance    Measurements must be relevant    Profile before I decide something is performance critical    Test to know Im correct  Rules Of Optimization Club Broad Engineering Performance is important but it cant be my priority unless the code is not running fast enough I only know this once I have a working program and I have validated it The industry places those who we think know how to write performant code on a pedestal I need to put those who write code that is optimized for correctness and performs fast enough on those pedestals Quotes   Programmers waste enormous amounts of time thinking about or worrying about the speed of noncritical parts of their programs and these attempts at efficiency actually have a strong negative impact when debugging and maintenance are considered We should forget about small efficiencies say about 97 of the time premature optimization is the root of all evil Yet we should not pass up our opportunities in that critical 3  Donald E Knuth    I dont trust anything until it runs In fact I dont trust anything until it runs twice  Andrew Gelman one of the greatest living statisticians at Columbia University    When were computer programmers were concentrating on the intricate little fascinating details of programming and we dont take a broad 19  engineering point of view about trying to optimize the total system You try to optimize the bits and bytes  Tom Kurtz inventor of BASIC  
185 MicroOptimizations MicroOptimizations are about squeezing every ounce of performance out of the machine as possible When code is written with this as the priority its very difficult to write code that is readable simple or idiomatic  
186 DataOrientation Data oriented design is a core philosophy and concept with the language I must embrace data oriented design with a prototype first approach   Data dominates If youve chosen the right data structures and organized things well the algorithms will almost always be selfevident Data structures not algorithms are central to programming  Rob Pike  Design Philosophy   If I dont understand the data I dont understand the problem    All problems are unique and specific to the data Im working with    Data transformations are at the heart of solving problems Each function method and workflow must focus on implementing the specific data transformations required to solve the problems    If my data is changing my problems are changing When my problems are changing the data transformations need to change with it    Uncertainty about the data is not a license to guess but a directive to STOP and learn more    Solving problems I dont have creates more problems I now do    If performance matters I must have mechanical sympathy for how the hardware and operating system work    Minimize simplify and REDUCE the amount of code required to solve each problem Do less work by not wasting effort    Code that can be reasoned about and does not hide execution costs can be better understood debugged and performance tuned    Coupling data together and writing code that produces predictable access patterns to the data will be the most performant    Changing data layouts can yield more significant performance improvements than changing just the algorithms    Efficiency is obtained through algorithms but performance is obtained through data structures and layouts  Reading   DataOriented Design and C  Mike Acton    Efficiency with Algorithms Performance with Data Structures Carruth  20   Chandler  
187 Interface And Composition Here are design philosophies and guidelines I need to follow when it comes to interfaces and composition Design Philosophy   Interfaces give programs structure    Interfaces encourage design by composition    Interfaces enable and enforce clean divisions between components   The  standardization  of  interfaces  can  set  clear  and  consistent  expectations   Decoupling means reducing the dependencies between components and the types they use     This leads to correctness quality and performance  Interfaces allow me to group concrete types by what they do   Dont group types by a common DNA but by a common behavior    Everyone can work together when we focus on what we do and not who we are    Interfaces help my code decouple itself from change   I must do my best to understand what could change and use interfaces to decouple    Interfaces with more than one method have more than one reason to change    Uncertainty about change is not a license to guess but a directive to STOP and learn more    I must distinguish between code that   Defends against fraud vs protects against accidents  Validation Use an interface when   Users of the API need to provide an implementation detail    APIs have multiple implementations they need to maintain internally    Parts of the API that can change have been identified and require decoupling  Dont use an interface   For the sake of using an interface    To generalize an algorithm    When users can declare their own interfaces    If its not clear how the interface makes the code better  Reading   Methods interfaces and Embedding  William Kennedy    Composition with Go  William Kennedy    Reducing type hierarchies  William Kennedy    Application Focused API Design  William Kennedy    Avoid interface pollution  William Kennedy    Interface Values Are Valueless  William Kennedy 21    Interface Semantics  William Kennedy  
188 Writing Concurrent Software Concurrency means undefined out of order execution Taking a set of instructions that would otherwise be executed in sequence and finding a way to execute them out of order and still produce the same result For the problem in front of me it has to be obvious that out of order execution would add value When I say it adds value I mean that it adds enough of a performance gain for the complexity cost Depending on my problem out of order execution may not be possible or even make sense Its also important to understand that concurrency is not the same as parallelism Parallelism means executing two or more instructions at the same time This is a different concept from concurrency Parallelism is only possible when I have at least 2 cores or hardware threads available to me and I have at least 2 Goroutines each executing instructions independently on each corehardware thread Both me and the runtime have a responsibility of managing the concurrency of the application Im responsible for managing these three things when writing concurrent software Design Philosophy The application must startup and shutdown with integrity Know how and when every Goroutine I create terminates All Goroutines I create should terminate before main returns Applications should be capable of shutting down on demand even under load in a controlled way I want to stop accepting new requests and finish the requests I have load shedding Identify and monitor critical points of back
pressure that can exist inside my application Channels mutexes and atomic functions can create back pressure when Goroutines are required to wait A little back pressure is good it means there is a good balance of concerns A lot of back pressure is bad it means things are imbalanced Back pressure that is imbalanced will cause Failures inside the software and across the entire platform My application to collapse implode or freeze Measuring back pressure is a way to measure the health of the application Rate limit to prevent overwhelming back pressure inside my application 22 Every system has a breaking point I must know what it is for my application Applications should reject new requests as early as possible once they are overloaded Dont take in more work than I can reasonably work on at a time Push back when Im at critical mass Create my own external back pressure Use an external system for rate limiting when it is reasonable and practical Use timeouts to release the back pressure inside my application No request or task is allowed to take forever Identify how long users are willing to wait Higherlevel calls should tell lowerlevel calls how long they have to run At the top level the user should decide how long they are willing to wait Use the Context package Functions that users wait for should take a Context These functions should select on ctxDone when they would otherwise block indefinitely Set a timeout on a Context only
when I have good reason to expect that a functions execution has a real time limit Allow the upstream caller to decide when the Context should be canceled Cancel a Context whenever the user abandons or explicitly aborts a call Architect applications to Identify problems when they are happening Stop the bleeding Return the system back to a normal state Reading Scheduling In Go Chapter 14
189 Signaling and Channels Channels allow Goroutines to communicate with each other through the use of signaling semantics Channels accomplish this signaling through the use of sendingreceiving data or by identifying state changes on individual channels Dont architect software with the idea of channels being queues focus on signaling and the semantics that simplify the orchestration required Depending on the problem Im solving I may require different channel semantics Depending on the semantics I need different architectural choices must be taken  23  Language Mechanics   Use channels to orchestrate and coordinate Goroutines   Focus on the signaling semantics and not the sharing of data    Signaling with data or without data    Question their use for synchronizing access to shared state   There are cases where channels can be simpler for this but initially question      Unbuffered channels   Receive happens before the Send    Benefit 100 guarantee the signal being sent has been received    Cost Unknown latency on when the signal will be received  Buffered channels   Send happens before the Receive    Benefit Reduce blocking latency between signaling    Cost No guarantee when the signal being sent has been received       The larger the buffer the less guarantee  Closing channels   Close happens before the Receive like Buffered    Signaling without data    Perfect for signaling cancellations and deadlines  NIL channels   Send and Receive block    Turn off signaling    Perfect for rate limiting or shortterm stoppages  Design Philosophy   If any given Send on a channel CAN cause the sending Goroutine to block   Be careful with Buffered channels larger than 1      Must know what happens when the sending Goroutine blocks  If any given Send on a channel WONT cause the sending Goroutine to block   I have the exact number of buffers for each send     Fan Out pattern  I have the buffer measured for max capacity     Buffers larger than 1 must have reasonmeasurements  Drop pattern  Less is more with buffers   Dont think about performance when thinking about buffers    Buffers can help to reduce blocking latency between signaling   Reducing blocking latency towards zero does not necessarily mean better throughput    If a buffer of one is giving me good enough throughput then keep it  24    Question buffers that are larger than one and measure for size    Find the smallest buffer that provides good enough throughput  
Chapter 2 Language Mechanics In this chapter I will learn about the basic language mechanics idioms and guidelines around types and memory management I will learn the data semantics behind the builtin types  
21 Builtin Types Types provide integrity and readability by asking 2 questions   What is the amount of memory to allocate eg 1 2 4 8 bytes    What does that memory represent eg int uint bool  Types can be specific to a precision such as int32 or int64   uint8 represents an unsigned integer with 1 byte of allocation    int32 represents a signed integer with 4 bytes of allocation  When I declare a type using a nonprecision based type unit int the size of the value is based on the architecture being used to build the program   32 bit arch int represents a signed int at 4 bytes of memory allocation    64 bit arch int represents a signed int at 8 bytes of memory allocation  
22 Word Size The word size represents the amount of memory allocation required to store integers and pointers for a given architecture For example   32 bit arch word size is 4 bytes of memory allocation    64 bit arch word size is 8 bytes of memory allocation  This is important because Go has internal data structures maps channels slices interfaces and functions that store integers and pointers The size of these data structures will be based on the architecture being used to build the program In Go the amount of memory allocated for a value of type int a pointer or a word will always be the same on the same architecture  
23 Zero Value Concept Every single value I construct in Go is initialized at least to its zero value state unless I specify the initialization value at construction The zero value is the setting of every bit in every byte to zero This is done for data integrity and its not free It takes time to push electrons through the machine to reset those bits but I should always take integrity over 25  performance Listing 21 Type Boolean Integer Float Complex String Pointer  Zero Value false 0 0 0i  empty nil  
24 Declare and Initialize The keyword var can be used to construct values to their zero value state for all types Listing 22 var a int var b string var c float64 var d bool fmtPrintfvar a int t T vn a a fmtPrintfvar b string t T vn b b fmtPrintfvar c float64 t T vn c c fmtPrintfvar d bool t T vnn d d Output var a int var b string var c float64 var d bool  int 0 string  float64 0 bool false  Strings use the UTF8 character set but are really just a collection of bytes A string is a twoword internal data structure in Go   The first word represents a pointer to a backing array of bytes    The second word represents the length or the number of bytes in the backing array    If the string is set to its zero value state then the first word is nil and the second word is 0  Using the short variable declaration operator I can declare construct and initialize a value all at the same time  26  Listing 23 aa  10 bb  hello cc  314159 dd  true   int 10  string hello  float64 314159  bool true  fmtPrintfaa  10 t T vn aa aa fmtPrintfbb  hello t T vn bb bb fmtPrintfcc  314159 t T vn cc cc fmtPrintfdd  true t T vnn dd dd Output aa  10 bb  hello cc  314159 dd  true  int 10 string hello float64 314159 bool true  
25 Conversion vs Casting Go doesnt have casting but conversion Instead of telling the compiler to map a set of bytes to a different representation the bytes need to be copied to a new memory location for the new representation Listing 24 aaa  int3210 fmtPrintfaaa  int3210 T vn aaa aaa Output aaa  int3210 int32 10  Go does have a package in the standard library called unsafe if I need to perform an actual casting operation I should really avoid that and be honest with myself why I am considering using it Performing a conversion provides the highest level of integrity for these types of operations  
26 Struct and Construction Mechanics This declaration represents a concrete userdefined type as a composite of different fields and types Listing 25 type example struct  flag bool counter int16 pi float32   Declare a variable of type example and initialize it to its zero value state  27  Listing 26 var e1 example fmtPrintfvn e1 Output flagfalse counter0 pi0  Declare a variable of type example not set to its zero value state by using literal construction syntax Listing 27 e2  example flag true counter 10 pi 3141592  fmtPrintlnFlag e2flag fmtPrintlnCounter e2counter fmtPrintlnPi e2pi Output Flag true Counter 10 Pi 3141592  Declare a variable of an unnamed literal type set to its nonzero value state using literal construction syntax This is a onetime thing Listing 28 e3  struct  flag bool counter int16 pi float32  flag true counter 10 pi 3141592  fmtPrintlnFlag e3flag fmtPrintlnCounter e3counter fmtPrintlnPi e3pi Output Flag true Counter 10 Pi 3141592  The idea of literal construction is just that to construct something literally and not to its zero value state Because of this I should use var for zero value and the short variable declaration operator with the   syntax for nonzero value construction  28  
27 Padding and Alignment How much memory is allocated for a value of type example Listing 29 type example struct flag bool counter int16 pi float32 A bool is 1 byte int16 is 2 bytes and float32 is 4 bytes Add that all together and I get 7 bytes However the actual answer is 8 bytes Why because there is a padding byte sitting between the flag and counter fields for the reason of alignment Figure 21 The idea of alignment is to allow the hardware to read memory more efficiently by placing memory on specific alignment boundaries The compiler takes care of the alignment boundary mechanics so I dont have to Depending on the size of a particular field and its placement in the struct Go determines the padding I need Listing 210 type example2 struct flag bool counter int16 flag2 bool pi float32 In this example Ive added a new field called flag2 between the counter and pi fields This causes more padding inside the struct 29 Listing 211 type example2 struct flag bool 0xc000100020 Starting Address byte 0xc000100021 1 byte padding counter int16 0xc000100022 2 byte alignment flag2 bool 0xc000100024 1 byte alignment byte 0xc000100025 1 byte padding byte 0xc000100026 1 byte padding byte 0xc000100027 1 byte padding pi float32 0xc000100028 4 byte alignment This is how the alignment and padding play out if a value of type example2 starts at address 0xc000100020 The flag field represents the starting address and is only 1 byte in size
Since the counter field requires 2 bytes of allocation it must be placed in memory on a 2byte alignment meaning it needs to fall on an address that is a multiple of 2 This requires the counter field to start at address 0xc000100022 This creates a 1byte gap between the flag and counter fields Figure 22 The flag2 field is a bool and can fall at the next address 0xc000100024 The final field is pi and requires 4 bytes of allocation so it needs to fall on a 4byte alignment The next address for a 4 byte value is at 0xc000100028 That means 3 more padding bytes are needed to maintain a proper alignment This results in a value of type example2 requiring 12 bytes of total memory allocation The largest field in a struct represents the alignment boundary for the entire struct In this case the largest field is 4 bytes so the starting address for this struct value must be a multiple of 4 I can see the address 0xc000100020 is a multiple of 4 If I need to minimize the amount of padding bytes I must lay out the fields from highest allocation to lowest allocation This will push any necessary padding bytes down to the bottom of the struct and reduce the total number of padding bytes necessary Listing 212 type example struct pi float32 0xc000100020 Starting Address counter int16 0xc000100024 2 byte alignment flag bool 0xc000100026 1 byte alignment flag2 bool 0xc000100027 1 byte alignment
After the reordering of the fields the struct value only requires 8 bytes of allocation 30 and not 12 bytes Since all the fields allow the struct value to fall on a 4byte alignment no extra padding bytes are necessary Figure 23
28 Assigning Values If I have two different named types that are identical in structure I cant assign a value of one to the other For example if the types example1 and example2 are declared using the same exact declaration and we initialize two variables Listing 213 var ex1 example1 var ex2 example2  I cant assign these two variables to each other since they are of different named types The fact that they are identical in structure is irrelevant Listing 214 ex1  ex2   Not allowed compiler error  To perform this assignment I would have to use conversion syntax and since they are identical in structure the compiler will allow this Listing 215 ex1  example1ex2   Allowed NO compiler error  However if ex2 was changed to be declared as an unnamed type using the same exact declaration as ex1 no conversion syntax would be required Listing 216 var ex2 struct  flag bool counter int16 pi float32  ex1  ex2   Allowed NO need for conversion syntax  31  The compiler will allow this assignment without the need for conversion  
29 Pointers Pointers serve the purpose of sharing values across program boundaries There are several types of program boundaries The most common one is between function calls There is also a boundary between Goroutines which I have notes for later When a Go program starts up the Go runtime creates a Goroutine Goroutines are lightweight application level threads with many of the same semantics as operating system threads Their job is to manage the physical execution of a distinct set of instructions  Every Go program has at least 1 Goroutine that I call the main  Goroutine Each Goroutine is given its own block of memory called a stack Each stack starts out as a 2048 byte 2k allocation Its very small but stacks can grow in size over time Figure 24  Every time a function is called a block of stack space is taken to help the Goroutine execute the instructions associated with that function Each individual block of memory is called a frame The size of a frame for a given function is calculated at compile time No value can be constructed on the stack unless the compiler knows the size of that value at compile time If the compiler doesnt know the size of a value at compile time the value has to be constructed on the heap Stacks are self cleaning and zero value helps with the initialization of the stack Every time I make a function call and a frame of memory is blocked out the memory for that frame is initialized which is how the stack is self cleaning On a 32  function return the memory for the frame is left alone since its unknown if that memory will be needed again It would be inefficient to initialize memory on returns  
210 Pass By Value All data is moved around the program by value This means as data is being passed across program boundaries each function or Goroutine is given its own copy of the data There are two types of data I work with the value itself int string user or the values address Addresses are data that need to be copied and stored across program boundaries The following code attempts to explain this more Listing 217 func main   Declare variable of type int with a value of 10 count  10  To get the address of a value use the  operator printlncounttValue Of count tAddr Of count   Pass a copy of the value of count whats in the box  to the increment1 function increment1count  Print out the value of and address of count  The value of count will not change after the function call printlncounttValue Of count tAddr Of count   Pass a copy of the address of count where is the box  to the increment2 function This is still considered a pass by  value and not a pass by reference because addresses are values increment2count  Print out the value of and address of count  The value of count has changed after the function call printlncounttValue Of count tAddr Of count    increment1 declares the function to accept its own copy of  and integer value func increment1inc int      Increment the local copy of the callers int value inc printlninc1tValue Of inc tAddr Of inc    increment2 declares the function to accept its own copy of  an address that points to an integer value  Pointer variables are literal types and are declared using  func increment2inc int   Increment the callers int value through the pointer inc printlninc2tValue Of inc tAddr Of inc tPoints To inc   33   Output count Value Of 10  Addr Of 0xc000050738  inc1 Value Of 11  Addr Of 0xc000050730  count Value Of 10  Addr Of 0xc000050738  inc2 Value Of 0xc000050738  Addr Of 0xc000050748  Points To 11  count Value Of 11  Addr Of 0xc000050738   There are lots of little details related to the stacks and pointers so to learn more read the post in chapter 14 titled Stacks and Pointer Mechanics  
211 Escape Analysis The algorithm the compiler uses to determine if a value should be constructed on the stack or heap is called escape analysis The name of the algorithm makes it sound like values are constructed on the stack first and then escape or move to the heap when necessary This is NOT the case The construction of a value only happens once and the escape analysis algorithm decides where that will be stack or heap Only construction on the heap is called an allocation in Go Understanding escape analysis is about understanding value ownership The idea is when a value is constructed within the scope of a function then that function owns the value From there ask the question does the value being constructed still have to exist when the owning function returns If the answer is no the value can be constructed on the stack If the answer is yes the value must be constructed on the heap Note The ownership rule is a good base rule for identifying code that causes allocations However I must appreciate that escape analysis has flaws that can result in nonobvious allocations Also the algorithm takes opportunities to leverage compiler optimizations to save on allocations Listing 218 user represents a user in the system type user struct name string email string func stayOnStack user u user name Bill email billemailcom return u The stayOnStack function is using value semantics to return a user value back to the caller In other words
the caller gets their own copy of the user value being 34 constructed When the stayOnStack function is called and returns the user value it constructs no longer needs to exist since the caller is getting their own copy Therefore the construction of the user value inside of stayOnStack can happen on the stack No allocation Listing 219 type user struct name string email string func escapeToHeap user u user name Bill email billemailcom return u The escapeToHeap function is using pointer semantics to return a user value back to the caller In other words the caller gets shared access an address to the user value being constructed When the escapeToHeap function is called and returns the user value it constructs does still need to exist since the caller is getting shared access to the value Therefore the construction of the user value inside of escapeToHeap cant happen on the stack it must happen on the heap Yes allocation Think about what would happen if the user value in the last example was constructed on the stack when using pointer semantics on the return 35 Figure 25 The caller would get a copy of a stack address from the frame below and integrity would be lost Once control goes back to the calling function the memory on the stack where the user value exists is reusable again The moment the calling function makes another function call a new frame is sliced and the memory will be overridden destroying the shared
value This is why I think about the stack being self cleaning Zero value initialization helps every stack frame that I need to be cleaned without the use of GC The stack is self cleaning since a frame is taken and initialized for the execution of each function call The stack is cleaned during function calls and not on returns because the compiler doesnt know if that memory on the stack will ever be needed again Escape analysis decides if a value is constructed on the stack the default or the heap the escape With the stayOnStack function Im passing a copy of the value back to the caller so its safe to keep the value on the stack With the escapeToHeap function Im passing a copy of the values address back to the caller sharing up the stack so its not safe to keep the value on the stack There are lots of little details related to the escape analysis so to learn more read the post in chapter 14 titled Escape Analysis Mechanics Note As of version 117 Go changed the ABI application binary interface to implement a new way of passing function input and output arguments using registers instead of memory on the stack This is enabled for Linux MacOS and 36 Windows on the 64bit x86 architectures This means that some function arguments wont be copied on the stack but some may depending on the viability of using registers This doesnt change any of the semantics
described in this chapter
212 Stack Growth The size of each frame for every function is calculated at compile time This means if the compiler doesnt know the size of a value at compile time the value must be constructed on the heap An example of this is using the builtin function make to construct a slice whose size is based on a variable Listing 220 b  makebyte size  Backing array allocates on the heap  Go uses a contiguous stack implementation to determine how stacks grow and shrink One alternative Go could have used is a segmented stack implementation which is used by some operating systems Every function call comes with a little preamble that asks Is there enough stack space for this new frame If yes then no problem and the frame is taken and initialized If not then a new larger stack must be constructed and the memory on the existing stack must be copied over to the new one This requires changes to pointers that reference memory on the stack The benefits of contiguous memory and linear traversals with modern hardware is the tradeoff for the cost of the copy Because of the use of contiguous stacks no Goroutine can have a pointer to some other Goroutines stack There would be too much overhead for the runtime to keep track of every pointer to every stack and readjust those pointers to the new location  
213 Garbage Collection Once a value is constructed on the heap the Garbage Collector GC has to get involved The most important part of the GC is the pacing algorithm It determines the frequencypace that the GC has to run in order to maintain the smallest heap possible in conjunction with the best application throughput There are lots of little details related to the GC so to learn more read the post in chapter 14 titled Garbage Collection Semantics  
214 Constants One of the more unique features of Go is how the language implements constants The rules for constants in the language specification are unique to Go They provide the flexibility Go needs to make the code we write readable and intuitive while still maintaining type safety Constants can be typed or untyped When a constant is untyped its considered to be of a kind Constants of a kind can be implicitly converted by the compiler This 37  all happens at compile time and not at runtime Listing 221 const ui  12345 const uf  3141592   kind integer  kind floatingpoint  Untyped numeric constants have a precision of 256 bits as stated by the specification They are based on a kind Listing 222 const ti int  12345 const tf float64  3141592   type int  type float64  Typed constants still use the constant type system but their precision is restricted Listing 223 const myUint8 uint8  1000  Compiler Error constant 1000 overflows uint8  This doesnt work because the number 1000 is too large to store in an uint8 Listing 224 var answer  3  0333   float64  KindFloat3  KindFloat0333  Constant arithmetic supports the use of different kinds of constants Kind Promotion is used to handle these different scenarios All of this happens implicitly The answer variable in this example will be of type float64 and represent 0999 at a precision of 64 bits Listing 225 const third  1  30   KindFloat  KindFloat1  KindFloat30  The third constant will be of kind float and represent 13 at a precision of 256 bits Listing 226 const zero  1  3   KindInt  KindInt1  KindInt3  The zero constant will be of kind integer and set to 0 since integer division has no remainder Listing 227 const one int8  1 const two  2  one   int82  int81  This is an example of constant arithmetic between typed and untyped constants In this case a constant of a type promotes over a constant of a kind The two constant  38  will be of type int8 and set to 2 Listing 228 const maxInt  9223372036854775807  This is the max integer value for a 64 bit integer Listing 227 const bigger  9223372036854775808543522345  The bigger constant is a much larger value than a 64 bit integer but it can be stored in a constant of kind int since constants of kind int are not limited to 64 bits of precision Listing 229 const bigger int64  9223372036854775808543522345 Compiler Error constant 9223372036854775808543522345 overflows int64  However if bigger was a constant of type int64 this would not compile  
215 IOTA IOTA provides support for setting successive integer constants Its possible the name comes from the integer function  from the programming language APL In APL the  function represented with the ninth letter of the Greek alphabet iota is used to create a zerobased array of consecutive ascending integers of a specified length Listing 230 const  A1  iota  0  Start at 0 B1  iota  1  Increment by 1 C1  iota  2  Increment by 1  fmtPrintlnA1 B1 C1 Output 0 1 2  The iota keyword works within a constant block and starts with the value of 0 Then for each successive constant declared in the block iota increments by 1  39  Listing 231 const  A2  iota  0  Start at 0 B2  1  Increment by 1 C2  2  Increment by 1  fmtPrintlnA2 B2 C2 Output 0 1 2  I dont need to repeat the use of the iota keyword The successive nature of the integer constants are assumed once applied Listing 232 const  A3  iota  1  1  0  1 B3  2  1  1 C3  3  2  1  fmtPrintlnA3 B3 C3 Output 1 2 3  If I didnt want to apply a mathematical pattern I can perform some math and the math is reapplied with an increasing value of iota Listing 233 const  Ldate 1  iota Ltime Lmicroseconds Llongfile Lshortfile LUTC    1  Shift 1 to the left 0  2  Shift 1 to the left 1  4  Shift 1 to the left 2  8  Shift 1 to the left 3  16  Shift 1 to the left 4  32  Shift 1 to the left 5  0000 0001 0000 0010 0000 0100 0000 1000 0001 0000 0010 0000  fmtPrintlnLdate Ltime Lmicroseconds Llongfile Lshortfile LUTC Output 1 2 4 8 16 32  I can use this feature like the Log package does for setting flags In this case bit operations are being applied with increasing values of iota to calculate flag values  40  
Chapter 3 Data Structures In this chapter I will learn about Gos data structures and the mechanical sympathy behind them  
31 CPU Caches There are lots of mechanical differences between processors and their design In this section I will talk at a high level about processors and the semantics that are relatively the same between them all This semantic understanding will provide me a good mental model for how the processor works and the sympathy I can provide Each core inside the processor has its own local cache of memory L1 and L2 and a common cache of memory L3 used to storeaccess data and instructions The hardware threads in each core can access their local L1 and L2 caches Data from L3 or main memory needs to be copied into the L1 or L2 cache for access Figure 31 The latency cost of accessing data that exists in the different caches changes from least to most L1 L2 L3 main memory As Scott Meyers said If performance matters then the total amount of memory I have is the total amount of cache Main memory is so slow to access practically speaking it might as well not 41 even be there Performance today is about how efficiently data flows through the hardware If every piece of data the hardware needs at any given time exists only in main memory my programs will run slower as compared to the data already being present in the L1 or L2 caches Listing 31 3GHz3 clock cyclesns 4 instructions per cycle 12 instructions per ns 1 ns 1 ns 12 instructions 1 s 1000
ns 12000 instructions 1 ms 1000000 ns 12000000 instructions 1 s 1000000000 ns 12000000000 instructions one thousand million billion Industry Defined Latencies L1 cache reference 05 ns 6 ins L2 cache reference 7 ns 84 ins Main memory reference 100 ns 1200 ins How do I write code that guarantees the data that is needed to execute an instruction is always present in the L1 or L2 caches I need to write code that is mechanically sympathetic with the processors prefetcher The prefetcher attempts to predict what data is needed before instructions request the data so its already present in either the L1 or L2 cache There are different granularities of memory access depending on where the access is happening My code can readwrite a byte of memory as the smallest unit of memory access However from the caching systems point of view the granularity is 64 bytes This 64 byte block of memory is called a cache line The Prefetcher works best when the instructions being executed create predictable access patterns to memory One way to create a predictable access pattern to memory is to construct a contiguous block of memory and then iterate over that memory performing a linear traversal with a predictable stride The array is the most important data structure to the hardware because it supports predictable access patterns However the slice is the most important data structure in Go Slices in Go use an array underneath Once I construct an array every element is equally
distant from the next or previous element As I iterate over an array I begin to walk cache line by connected cache line in a predictable stride The Prefetcher will pick up on this predictable data access pattern and begin to efficiently pull the data into the processor thus reducing data access latency costs 42 Imagine I have a big square matrix of memory and a linked list of nodes that match the number of elements in the matrix If I perform a traversal across the linked list and then traverse the matrix in both directions Column and Row how will the performance of the different traversals compare Listing 32 func RowTraverse int var ctr int for row 0 row rows row for col 0 col cols col if matrixrowcol 0xFF ctr return ctr Row traverse will have the best performance because it walks through memory cache line by connected cache line which creates a predictable access pattern Cache lines can be prefetched and copied into the L1 or L2 cache before the data is needed Listing 33 func ColumnTraverse int var ctr int for col 0 col cols col for row 0 row rows row if matrixrowcol 0xFF ctr return ctr Column Traverse is the worst by an order of magnitude because this access pattern crosses over OS page boundaries on each memory access This causes no predictability for cache line prefetching and becomes essentially random access memory Listing 34 func LinkedListTraverse int var ctr int d list for
d nil if dv 0xFF ctr d dp return ctr 43 The linked list is twice as slow as the row traversal mainly because there are cache line misses but fewer TLB Translation Lookaside Buffer misses A bulk of the nodes connected in the list exist inside the same OS pages Listing 35 BenchmarkLinkListTraverse16 BenchmarkColumnTraverse16 BenchmarkRowTraverse16 128 30 310 28738407 nsop 126878630 nsop 11060883 nsop I can see this is true from the benchmark results I will learn about benchmarking later
32 Translation Lookaside Buffer TLB Each running program is given a full memory map of virtual memory by the OS and that running program thinks they have all of the physical memory on the machine However physical memory needs to be shared with all the running programs The operating system shares physical memory by breaking the physical memory into pages and mapping pages to virtual memory for any given running program Each OS can decide the size of a page but 4k 8k 16k are reasonable and common sizes The TLB is a small cache inside the processor that helps to reduce latency on translating a virtual address to a physical address within the scope of an OS page and offset inside the page A miss against the TLB cache can cause large latencies because now the hardware has to wait for the OS to scan its page table to locate the right page for the virtual address in question If the program is running on a virtual machine like in the cloud then the virtual machine paging table needs to be scanned first Remember when I said The linked list is twice as slow as the row traversal mainly because there are cache line misses but fewer TLB misses explained next A bulk of the nodes connected in the list exist inside the same OS pages The LinkedList is orders of magnitude faster than the column traversal because of TLB access Even though there are cache line misses with the linked list traversal since a majority of the memory for a group of nodes will land inside the same page TLB latencies are not affecting performance This is why for programs that use a large amount of memory like DNA based applications I may want to use a distribution of Linux that is configured with page sizes in the order of a megabyte or two of memory All that said dataoriented design matters Writing an efficient algorithm has to take into account how the data is accessed Remember performance today is about 44  how efficiently I can get data into the processor  
33 Declaring and Initializing Values Declare an array of five strings initialized to its zero value state Listing 36 var strings 5string  A string is an immutable two word data structure representing a pointer to a backing array of bytes and the total number of bytes in the backing array Since this array is set to its zero value state every element is set to its zero value state This means that each string has the first word set to nil and the second word set to 0 Figure 32  
34 String Assignments What happens when a string is assigned to another string Listing 37 strings0  Apple  When a string is assigned to another string the two word value is copied resulting in two different string values both sharing the same backing array  45  Figure 33  The cost of copying a string is the same regardless of the size of a string a two word copy  
35 Iterating Over Collections Go provides two different semantics for iterating over a collection I can iterate using value semantics or pointer semantics Listing 38  Value Semantic Iteration for i fruit  range strings  printlni fruit   Pointer Semantic Iteration for i  range strings  printlni stringsi   When using value semantic iteration two things happen First the collection Im iterating over is copied and I iterate over the copy In the case of an array the copy could be expensive since the entire array is copied In the case of a slice there is no real cost since only the internal slice value is copied and not the backing array Second I get a copy of each element being iterated on When using pointer semantic iteration I iterate over the original collection and I access each element associated with the collection directly  
36 Value Semantic Iteration Given the following code and output  46  Listing 39 strings  5stringApple Orange Banana Grape Plum for i fruit  range strings  printlni fruit  Output 0 Apple 1 Orange 2 Banana 3 Grape 4 Plum  The strings variable is an array of 5 strings The loop iterates over each string in the collection and displays the index position and the string value Since this is value semantic iteration the for range is iterating over its own shallow copy of the array and on each iteration the fruit variable is a copy of each string the two word data structure Notice how the fruit variable is passed to the print function using value semantics The print function is getting its own copy of the string value as well By the time the string is passed to the print function there are 4 copies of the string value array shallow copy fruit variable and the print functions copy All 4 copies are sharing the same backing array of bytes Figure 34  47  Making copies of the string value is important because it prevents the string value from ever escaping to the heap This eliminates nonproductive allocation on the heap  
37 Pointer Semantic Iteration Given the following code and output Listing 310 strings  5stringApple Orange Banana Grape Plum for i  range strings  printlni stringsi  Output 0 Apple 1 Orange 2 Banana 3 Grape 4 Plum  Once again the strings variable is an array of 5 strings The loop iterates over each string in the collection and displays the index position and the string value Since this is pointer semantic iteration the for range is iterating over the strings array directly and on each iteration the string value for each index position is accessed directly for the print call  
38 Data Semantic Guideline For BuiltIn Types As a guideline if the data Im working with is a numeric string or bool then use value semantics to move the data around my program This includes declaring fields on a struct type Listing 311 func Foox int y string z bool int string bool type Foo struct  X int Y string Z bool   One reason I might take an exception and use pointer semantics is if I need the semantics of NULL absence of value Then using pointers of these types is an option but document this if its not obvious The nice thing about using value semantics for these types is that Im guaranteed that each function is operating on its own copy This means reads and writes to this data are isolated to that function This helps with integrity and identifying bugs related to data corruption  48  
39 Different Type Arrays Its interesting to see what the compiler provides as an error when assigning arrays of the same types that are of different lengths Listing 312 var five 5int four  4int10 20 30 40 five  four Compiler Error cannot use four type 4int as type 5int in assignment  Here I declare an array of 4 and 5 integers initialized to its zero value state Then try to assign them to each other and the compiler says cannot use four type 4int as type 5int in assignment Its important to be clear about what the compiler is saying Its saying that an array of 4 integers and an array of 5 integers represent data of different types The size of an array is part of its type information In Go the size of an array has to be known at compile time  
310 Contiguous Memory Construction I want to prove that an array provides a contiguous layout of memory Listing 313 five  5stringAnnie Betty Charley Doug Bill for i v  range five  fmtPrintfValuestAddressp v v fivei  Output ValueAnnie ValueBetty ValueCharley ValueDoug ValueBill  Address0xc000010250 Address0xc000010250 Address0xc000010250 Address0xc000010250 Address0xc000010250  IndexAddrpn  IndexAddr0xc000052180 IndexAddr0xc000052190 IndexAddr0xc0000521a0 IndexAddr0xc0000521b0 IndexAddr0xc0000521c0  Here I declare an array of 5 strings initialized with values Then use value semantic iteration to display information about each string The output shows each individual string value the address of the v variable and the address of each element in the array I can see how the array is a contiguous block of memory and how a string is a two word or 16 byte data structure on my 64 bit architecture The address for each element is distanced on a 16 byte stride  49  The fact that the v variable has the same address on each iteration strengthens the understanding that v is a local variable of type string which contains a copy of each string value during iteration  
311 Constructing Slices The slice is Gos most important data structure and its represented as a three word data structure Figure 35  Constructing a slice can be done in several ways Listing 314  Slice of string set to its zero value state var slice string  Slice of string set to its empty state slice  string  Slice of string set with a length and capacity of 5 slice  makestring 5  Slice of string set with a length of 5 and capacity of 8 slice  makestring 5 8  Slice of string set with values with a length and capacity of 5 slice  stringA B C D E  I can see the builtin function make allows me to preallocate both length and capacity for the backing array If the compiler knows the size at compile time the backing array could be constructed on the stack  
312 Slice Length vs Capacity The length of a slice represents the number of elements that can be read and written to The capacity represents the total number of elements that exist in the backing array from that pointer position  50  Because of syntactic sugar slices look and feel like an array Listing 315 slice  makestring 5 slice0  Apple slice1  Orange slice2  Banana slice3  Grape slice4  Plum  I can tell the difference between slice and array construction since an array has a known size at compile time and slices necessarily dont If I try to access an element beyond the slices length I will get a runtime error Listing 316 slice  makestring 5 slice5  Raspberry Compiler Error Error panic runtime error index out of range slice5  Runtime error  In this example the length of the slice is 5 and Im attempting to access the 6th element which does not exist  
313 Data Semantic Guideline For Slices As a guideline if the data Im working with is a slice then use value semantics to move the data around my program This includes declaring fields on a type Listing 317 func Foodata byte byte type Foo struct  X int Y string Z bool   This goes for all of Gos internal data structures slices maps channels interfaces and functions One reason to switch to pointer semantics is if I need to share the slice for a decoding or unmarshaling operation Using pointers for these types of operations are ok but document this if its not obvious  
314 Contiguous Memory Layout The idea behind the slice is to have an array which is the most efficient data structure as it relates to the hardware However I still need the ability to be 51  dynamic and efficient with the amount of data I need at runtime and future growth Listing 318 func main  slice  makestring 5 8 slice0  Apple slice1  Orange slice2  Banana slice3  Grape slice4  Plum inspectSliceslice  func inspectSliceslice string  fmtPrintfLengthd Capacitydn lenslice capslice for i  range slice  fmtPrintfd p sn i slicei slicei   Output Length5 Capacity8 0 0xc00007e000 Apple 1 0xc00007e010 Orange 2 0xc00007e020 Banana 3 0xc00007e030 Grape 4 0xc00007e040 Plum  The inspectSlice function shows how a slice does have a contiguous backing array with a predictable stride It also shows how a slice has a length and capacity which may be different Notice how the print function only iterates over the length of a slice  
315 Appending With Slices The language provides a builtin function called append to add values to an existing slice Listing 319 var data string for record  1 record  102400 record  data  appenddata fmtSprintfRec d record   The append function works with a slice even when the slice is initialized to its zero value state The API design of append is whats interesting because it uses value semantic mutation Append gets its own copy of a slice value it mutates its own copy then it returns a copy back to the caller Why is the API designed this way This is because the idiom is to use value semantics to move a slice value around a program This must still be respected even with a mutation operation Plus value semantic mutation is the safest way to 52  perform mutation since the mutation is being performed on the functions own copy of the data in isolation Append always maintains a contiguous block of memory for the slices backing array even after growth This is important for the hardware Figure 36  Every time the append function is called the function checks if the length and capacity of the slice is the same or not If its the same it means there is no more room in the backing array for the new value In this case append creates a new backing array doubling or growing by 25 and then copies the values from the old array into the new one Then the new value can be appended Figure 37  If its not the same it means that there is an extra element of capacity existing for the append An element is taken from capacity and added to the length of the slice This makes an append operation very efficient When the backing array has 1024 elements of capacity or less new backing arrays are constructed by doubling the size of the existing array Once the backing array grows past 1024 elements growth happens at 25 53  
316 Slicing Slices Slices provide the ability to avoid extra copies and heap allocations of the backing array when needing to isolate certain elements of the backing array for different operations The slicing syntax represents the list notation ab which means include elements from index a through b but not including b Listing 320 slice1  stringA B C D E slice2  slice124  The variable slice2 is a new slice value that is now sharing the same backing array that slice1 is using However slice2 only allows me to access the elements at index 2 and 3 C and D of the original slices backing array The length of slice2 is 2 and not 5 like in slice1 and the capacity is 3 since there are now 3 elements from that pointer position Figure 38  A better way to think about slicing is to focus on the length using this notation aalen index a through a plus the length This will reduce errors in calculating new slices Using this inspect function  54  Listing 321 func inspectSliceslice string  fmtPrintfLengthd Capacitydn lenslice capslice for i s  range slice  fmtPrintfd p sn i slicei s     I can see this in action Listing 322 slice1  stringA B C D E slice2  slice124 inspectSliceslice1 inspectSliceslice2 Output Length5 Capacity5 0 0xc00007e000 A 1 0xc00007e010 B 2 0xc00007e020 C 3 0xc00007e030 D 4 0xc00007e040 E Length2 Capacity3 0 0xc00007e020 C 1 0xc00007e030 D   SAME AS INDEX 2 IN SLICE 1  SAME AS INDEX 3 IN SLICE 1  Notice how the two different slices are sharing the same backing array I can see this by comparing addresses The nice thing here is there are no allocations The compiler knows the size of the backing array for slice1 at compile time Passing a copy of the slice value down into the inspectSlice function keeps everything on the stack  
317 Mutations To The Backing Array When I use slice2 to change the value of the string at index 0 any slice value that is sharing the same backing array where the address for that index is part of that slices length will see the change  55  Listing 323 slice1  stringA B C D E slice2  slice124 slice20  CHANGED inspectSliceslice1 inspectSliceslice2 Output Length5 Capacity5 0 0xc00007e000 A 1 0xc00007e010 B 2 0xc00007e020 CHANGED 3 0xc00007e030 D 4 0xc00007e040 E Length2 Capacity3 0 0xc00007e020 CHANGED 1 0xc00007e030 D  I always have to be aware when I am modifying a value at an index position if the backing array is being shared with another slice Figure 39  What if I use the builtin function append instead  56  Listing 324 slice1  stringA B C D E slice2  slice124 slice2  appendslice2 CHANGED inspectSliceslice1 inspectSliceslice2 Output Length5 Capacity5 0 0xc00007e000 A 1 0xc00007e010 B 2 0xc00007e020 C 3 0xc00007e030 D 4 0xc00007e040 CHANGED Length3 Capacity3 0 0xc00007e020 C 1 0xc00007e030 D 2 0xc00007e040 CHANGED  The append function creates the same side effect but its hidden In this case bringing in more length from capacity for slice2 has caused the value at address 0xc00007e040 to be changed Unfortunately slice1 had this address already as part of its length Figure 310  One way to avert the side effect is to use a three index slice when constructing slice2 so the length and capacity is the same at 2  57  Listing 325 slice1  stringA B C D E slice2  slice1244 inspectSliceslice1 inspectSliceslice2 Output Length5 Capacity5 0 0xc00007e000 A 1 0xc00007e010 B 2 0xc00007e020 C 3 0xc00007e030 D 4 0xc00007e040 E Length2 Capacity2 0 0xc00007e020 C 1 0xc00007e030 D  The syntax for a three index slice is abc when b and c should be the same since ab sets the length and ac sets the capacity Now the length and capacity of slice2 is the same Now I use the builtin function append again like before Listing 326 slice1  stringA B C D E slice2  slice1244 slice2  appendslice2 CHANGED inspectSliceslice1 inspectSliceslice2 Output Length5 Capacity5 0 0xc00007e000 A 1 0xc00007e010 B 2 0xc00007e020 C 3 0xc00007e030 D 4 0xc00007e040 E  Length3 Capacity4 0 0xc000016080 C 1 0xc000016090 D 2 0xc0000160a0 CHANGED Notice after the call to append slice2 has a new backing array  58  Figure 311  This can be seen by comparing the addresses of each slice In this case the mutation against slice2 didnt cause a side effect against slice1  
318 Copying Slices Manually There is a builtin function named copy that will allow for the shallow copying of slices Since a string has a backing array of bytes that are immutable it can be used as a source but never a destination Listing 327 slice1  stringA B C D E slice3  makestring lenslice1 copyslice3 slice1 inspectSliceslice1 inspectSliceslice3 Output Length5 Capacity5 0 0xc00005c050 A 1 0xc00005c060 B 2 0xc00005c070 C 3 0xc00005c080 D 4 0xc00005c090 E Length5 Capacity5 0 0xc00005c0a0 A 1 0xc00005c0b0 B 2 0xc00005c0c0 C 3 0xc00005c0d0 D 4 0xc00005c0e0 E  As long as the destination slice has the proper type and length the builtin function copy can perform a shallow copy  59  
319 Slices Use Pointer Semantic Mutation Its important to remember that even though I use value semantics to move a slice around the program when reading and writing a slice I am using pointer semantics Sharing individual elements of a slice with different parts of my program can cause unwanted side effects Listing 328  Construct a slice of 1 user set a pointer to that user  use the pointer to update likes users  makeuser 1 ptrUsr0  users0 ptrUsr0likes for i  range users  fmtPrintfUser d Likes dn i usersilikes  Output User 0 Likes 1  A slice is used to maintain a collection of users Then a pointer is set to the first user and used to update likes The output shows that using the pointer is working Figure 312  Then a new user is appended to the collection and the pointer is used again to add a like to the first user  60  Listing 329  Append a new user to the collection Use the pointer again  to update likes users  appendusers user ptrUsr0likes for i  range users  fmtPrintfUser d Likes dn i usersilikes  Output User 0 Likes 1 User 1 Likes 0  However since the append function replaced the backing array with a new one the pointer is updating the old backing array and the likes are lost The output shows the likes for the first user did not increase Figure 313  I have to be careful to know if a slice is going to be used in an append operation during the course of a running program How I share the slice needs to be considered Sharing individual indexes may not be the best idea Sharing an entire slice value may not work either when appending is in operation Probably using a slice as a field in a struct and sharing the struct value is a better way to go  
320 Linear Traversal Efficiency The beauty of a slice is its ability to allow for performing linear traversals that are mechanically sympathetic while sharing data using value semantics to minimize heap allocations  61  Listing 330 x  byte0x0A 0x15 0x0e 0x28 0x05 0x96 0x0b 0xd0 0x0 a  x0 b  binaryLittleEndianUint16x13 c  binaryLittleEndianUint16x35 d  binaryLittleEndianUint32x59 printlna b c d  The code is performing a linear traversal by creating slice values that read different sections of the byte array from beginning to end Figure 314  All the data in this code stays on the stack No extra copies of the data inside the byte slice are copied  
321 UTF8 Gos compiler expects all code to be encoded in the UTF8 character set Make sure any file with source code is saved with this encoding or literal strings may be wrong when the program runs UTF8 is a character set where I have bytes code points and then characters One to four bytes of data can represent a code point int32 and one to many code points can represent a character Listing 331 s   means world  The string above represents 18 bytes 14 code points and 14 characters Each Chinese character I see requires 3 bytes to represent the code pointcharacter I see  62  Listing 332 var buf utf8UTFMaxbyte  There is a utf8 package in the standard library that declares a constant named UTFMax This constant represents the max number of bytes a code point could require which is 4 Listing 333 for i r  range s   When iterating over a string the iteration moves code point by code point Go has an alias type named rune alias of int32 that represents a code point Hence the use of the variable r as the value being copied On the first iteration i will equal 0 On the next iteration i will equal 3 Then in the next iteration i will equal 6 All subsequent iterations will increment i by 1 Listing 334 rl  utf8RuneLenr  The RuneLen function returns the number of bytes required to store the rune value For the first two iterations rl will equal 3 Listing 335 si  i  rl copybuf sisi  The si variable represents the index position for the slice operation to slice the bytes associated with the rune Then the builtin function copy is used to copy the bytes for the rune into the array Notice how an array can be sliced This proves that every array in Go is just a slice waiting to happen Listing 336    fmtPrintf2d q codepoint 6x encoded bytes vn i r r bufrl  The print statement displays each character code point and the set of bytes  63  Listing 337 Output 0  codepoint 0x4e16 encoded bytes byte0xe4 0xb8 0x96 3  codepoint 0x754c encoded bytes byte0xe7 0x95 0x8c 6   codepoint 0x20 encoded bytes byte0x20 7 m codepoint 0x6d encoded bytes byte0x6d 8 e codepoint 0x65 encoded bytes byte0x65 9 a codepoint 0x61 encoded bytes byte0x61 10 n codepoint 0x6e encoded bytes byte0x6e 11 s codepoint 0x73 encoded bytes byte0x73 12   codepoint 0x20 encoded bytes byte0x20 13 w codepoint 0x77 encoded bytes byte0x77 14 o codepoint 0x6f encoded bytes byte0x6f 15 r codepoint 0x72 encoded bytes byte0x72 16 l codepoint 0x6c encoded bytes byte0x6c 17 d codepoint 0x64 encoded bytes byte0x64  
322 Declaring And Constructing Maps A map is a data structure that provides support for storing and accessing data based on a key It uses a hash map and bucket system that maintains a contiguous block of memory underneath Listing 338 type user struct  name string username string   Construct a map set to its zero value  that can store user values based on a key of type string  Trying to use this map will result in a runtime error panic var users mapstringuser  Construct a map initialized using make  that can store user values based on a key of type string users  makemapstringuser  Construct a map initialized using empty literal construction  that can store user values based on a key of type string users  mapstringuser  There are several ways to construct a map for use A map set to its zero value is not usable and will result in my program panicking The use of the builtin function make and literal construction constructs a map ready for use  64  Listing 339 func main  users  makemapstringuser usersRoy  userRob Roy usersFord  userHenry Ford usersMouse  userMickey Mouse usersJackson  userMichael Jackson for key value  range users  fmtPrintlnkey value   Output Roy Rob Roy Ford Henry Ford Mouse Mickey Mouse Jackson Michael Jackson  If the builtin function make is used to construct a map then the assignment operator can be used to add and update values in the map The order of how keysvalues are returned when ranging over a map is undefined by the spec and up to the compiler to implement Listing 340 func main  users  mapstringuser Roy Rob Roy Ford Henry Ford Mouse Mickey Mouse Jackson Michael Jackson     for key value  range users  fmtPrintlnkey value   Output Ford Henry Ford Jackson Michael Jackson Roy Rob Roy Mouse Mickey Mouse  In this case the output was returned in a different order from how they are listed in the construction The current algorithm for 116 will return the results in a random order once the number of values reaches a certain limit Once again this is a compiler implementation that is allowed to change I cant depend on it  
323 Lookups and Deleting Map Keys Once data is stored inside of a map to extract any data a key lookup is required  65  Listing 341 user1 exists1  usersBill user2 exists2  usersFord fmtPrintlnBill exists1 user1 fmtPrintlnFord exists2 user2 Output Bill false   Ford true Henry Ford  To perform a key lookup square brackets are used with the map variable Two values can be returned from a map lookup the value and a boolean that represents if the value was found or not If I dont need to know this I can leave the exists variable out When a key is not found in the map the operation returns a value of the map type set to its zero value state I can see this with the Bill key lookup Dont use zero value to determine if a key exists in the map or not since zero value may be valid and what was actually stored for the key Listing 342 deleteusers Roy  There is a builtin function named delete that allows for the deletion of data from the map based on a key  
324 Key Map Restrictions Not all types can be used as a key Listing 343 type slice user Users  makemapsliceuser Compiler Error invalid map key type users  A slice is a good example of a type that cant be used as a key Only values that can be run through the hash function are eligible A good way to recognize types that can be a key is if the type can be used in a comparison operation I cant compare two slice values  66  
Chapter 4 Decoupling In this chapter I will learn about the mechanics and semantics for decoupling code from change I will learn how to extend behavior to data what polymorphism really is about and how it is applied in the language  
41 Methods A method provides data the ability to exhibit behavior A function is called a method when that function has a receiver declared The receiver is the parameter that is declared between the keyword func and the function name There are two types of receivers value receivers for implementing value semantics and pointer receivers for implementing pointer semantics Listing 41 type user struct  name string email string  func u user notify  fmtPrintfSending User Email To ssn uname uemail  func u user changeEmailemail string  uemail  email fmtPrintfChanged User Email To sn email   The notify function is implemented with a value receiver This means the method operates under value semantics and will operate on its own copy of the value used to make the call The changeEmail function is implemented with a pointer receiver This means the method operates under pointer semantics and will operate on shared access to the value used to make the call Outside of a few exceptions a method set for a type should not contain a mix of value and pointer receivers Data semantic consistency is critically important and this includes declaring methods  
42 Method Calls When making a method call the compiler doesnt care if the value used to make the call matches the receivers data semantics exactly The compiler just wants a value or pointer of the same type  67  Listing 42 bill  userBill billemailcom billnotify billchangeEmailbillhotmailcom  I can see that a value of type user is constructed and assigned to the bill variable In the case of the notify call the bill variable matches the receiver type which is using a value receiver In the case of the changeEmail call the bill variable doesnt match the receiver type which is using a pointer receiver However the compiler accepts the method call and shares the bill variable with the method Go will adjust to make the call This works the same when the variable used to make the call is a pointer variable Listing 43 bill  userBill billemailcom billnotify billchangeEmailbillhotmailcom  In this case the bill variable is a pointer variable to a value of type user Once again Go adjusts to make the method call when calling the notify method If Go didnt adjust then this is what I would have to do to make those same method calls Listing 44 bill  userBill billemailcom billchangeEmailbillhotmailcom bill  userBill billemailcom billnotify  Im glad I dont have to do that to make method calls in Go  
43 Data Semantic Guideline For Internal Types As a guideline if the data Im working with is an internal type slice map channel function interface then use value semantics to move the data around my program This includes declaring fields on a type However when Im reading and writing I need to remember Im using pointer semantics Listing 45 type IP byte type IPMask byte  These types are declared in the net package that is part of the standard library They are declared with an underlying type which is a slice of bytes Because of this 68  these types follow the guidelines for internal types Listing 46 func ip IP Maskmask IPMask IP  if lenmask  IPv6len  lenip  IPv4len  allFFmask12  mask  mask12  if lenmask  IPv4len  lenip  IPv6len  bytesEqualip12 v4InV6Prefix  ip  ip12  n  lenip if n  lenmask  return nil  out  makeIP n for i  0 i  n i  outi  ipi  maski  return out   With the Mask method value semantics are in play for both the receiver parameter and return argument This method accepts its own copy of a Mask value it mutates that value and then it returns a copy of the mutation This method is using value semantic mutation This is not an accident or random A function can decide what data input and output it needs What it cant decide is the data semantics for how the data flows in or out The data drives that decision and the function must comply This is why Mask implements a value semantic mutation api It must respect how a slice is designed to be moved around the program Listing 47 func ipEmptyStringip IP string  if lenip  0  return   return ipString   The ipEmptyString function is also using value semantics for the input and output This function accepts its own copy of an IP value and returns a string value No use of pointer semantics because the data dictates the data semantics and not the function One exception to using value semantics is when I need to share a slice or map with a function that performs unmarshaling or decoding  
44 Data Semantic Guideline For Struct Types As a guideline if the data Im working with is a struct type then I have to think 69 about what the data represents to make a decision Though it would be great to choose value semantics for everything when Im not sure evaluate if the data is safe to be copied If its safe to be copied start with value semantics If its not safe for copying or thats not even clear start with pointer semantics After some time working with a new data type the data semantic could become selfevident and refactoring the data semantic is what I want to do Dont become paralyzed if its not selfevident from the start Listing 48 type Time struct sec int64 nsec int32 loc Location Here is the Time struct from the time package If I was being asked to implement the API for this data structure what should I choose value or pointer semantics Sometimes I can ask these question Does a change in the data completely create a new data point Is the data specific to a context and are mutations isolated to that context Should there only ever be one instance of this data If Im looking at an existing code base and I want to know what data semantic was chosen look for a factory function The return type of a factory function should dictate the data semantics Listing 49 func Now Time sec nsec now return Timesec unixToInternal nsec
Local This is the factory function for constructing Time values Look at the return its using value semantics This tells me that I should be using value semantics for Time values which means every function gets its own copy of a Time value and fields in a struct should be declared as values of type Time 70 Listing 410 func t Time Addd Duration Time tsec int64d 1e9 nsec int32tnsec int32d1e9 if nsec 1e9 tsec nsec 1e9 else if nsec 0 tsecnsec 1e9 tnsec nsec return t Add is a method that needs to perform a mutation operation If I look closely I will see the function is using value semantic mutation The Add method gets its own copy of the Time value used to make the call it mutates its own copy then it returns a copy back to the caller Once again this is the safest way to perform a mutation operation Listing 411 func divt Time d Duration qmod2 int r Duration Here is another example where the div function accepts a value of type Time and Duration int64 then returns values of type int and Duration Value semantics for the Time type and for all the builtin types Duration has an underlying type of int64 Listing 412 func t Time UnmarshalBinarydata byte error func t Time GobDecodedata byte error func t Time UnmarshalJSONdata byte error func t Time UnmarshalTextdata byte error These four methods from the Time package seem to break the rules for data semantic
consistency They are using pointer semantics why Because they are implementing an interface where the method signature is locked in Since the implementation requires a mutation pointer semantics are the only choice Here is a guideline If value semantics are at play I can switch to pointer semantics for some functions as long as I dont let the data in the remaining call chain switch back to value semantics Once I switch to pointer semantics all future calls from that point need to stick to pointer semantics I can never ever never go from pointer to value Its never safe to make a copy of a value that a pointer points to 71 Listing 413 func Openname string file File err error return OpenFilename ORDONLY 0 The Open function from the os package shows that when using a value of type File pointer semantics are at play File values need to be shared and should never be copied Listing 414 func f File Chdir error if f nil return ErrInvalid if e syscallFchdirffd e nil return PathErrorchdir fname e return nil The method Chdir is using a pointer receiver even though this method does not mutate the File value This is because File values need to be shared and cant be copied Listing 415 func epipecheckfile File e error if e syscallEPIPE if atomicAddInt32filenepipe 1 10 sigpipe else atomicStoreInt32filenepipe 0 The epipecheck function as well accepts File values using pointer semantics
45 Methods Are Just Functions Methods are really just functions that provide syntactic sugar to provide the ability for data to exhibit behavior  72  Listing 416 type data struct  name string age int  func d data displayName  fmtPrintlnMy Name Is dname  func d data setAgeage int  dage  age fmtPrintlndname Is Age dage   A type and two methods are declared The displayName method is using value semantics and setAge is using pointer semantics Note Do not implement setters and getters in Go These are not apis with purpose and in these cases its better to make those fields exported Listing 417 d  data name Bill  ddisplayName dsetAge21  A value of type data is constructed and method calls are made Listing 418 datadisplayNamed datasetAged 21  Since methods are really just functions with syntactic sugar the methods can be executed like functions I can see that the receiver is really a parameter its the first parameter When I call a method the compiler converts that to a function call underneath Note Do not execute methods like this but I may see this syntax in tooling messages  
46 Know The Behavior of the Code If I know the data semantics at play then I know the behavior of the code If I know the behavior of the code then I know the cost of the code Once I know the cost Im engineering Given this type and method set  73  Listing 419 type data struct  name string age int  func d data displayName  fmtPrintlnMy Name Is dname  func d data setAgeage int  dage  age fmtPrintlndname Is Age dage   I can write the following code Listing 420 func main  d  data name Bill     f1  ddisplayName f1 dname  Joan f1  Output My Name Is Bill My Name Is Bill  I start with constructing a value of type Data assigning it to the variable d Then I take the method displayName bound to d and assign that to a variable named f1 This is not a method call but an assignment which creates a level of indirection Functions are values in Go and belong to the set of internal types After the assignment I can call the method indirectly through the use of the f1 variable This displays the name Bill Then I change the data so the name is now Joan and call the method once again through the f1 variable I dont see the change Bill is the output once again So Why  74  Figure 41  It has to do with the data semantics at play The displayName method is using a value receiver so value semantics are at play Listing 421 func d data displayName  fmtPrintlnMy Name Is dname   This means that the f1 variable maintains and operates against its own copy of d So calling the method through the f1 variable will always use the copy and that copy is protected against change This is what I want with value semantics Now I will do the same thing but with the setAge method Listing 422 func main  d  data name Bill     f2  dsetAge f245 dname  Sammy f245  Output Bill Is Age 45 Sammy Is Age 45  This time the setAge method is assigned to the variable f2 Once again the method is executed indirectly through the f2 variable passing 45 for Bills age Then Bills name is changed to Sammy and the f2 variable is used again to make the call This time I see the name has changed  75  Figure 42  The setAge function is using a pointer receiver so setAge doesnt operate on its own copy of the d variable but is operating directly on the d variable Therefore f2 is operating on shared access and I see the change Listing 423 func d data setAgeage int  dage  age fmtPrintlndname Is Age dage   Without knowing the data semantics at play I wont know the behavior of the code These data semantics are real and affect the behavior  
47 Interfaces Interfaces give programs structure and encourage design by composition They enable and enforce clean divisions between components The standardization of interfaces can set clear and consistent expectations Decoupling means reducing the dependencies between components and the types they use This leads to correctness quality and maintainability Interfaces allow me to group concrete data together by what the data can do Its about focusing on what data can do and not what the data is Interfaces also help my code decouple itself from change by asking for concrete data based on what it can do Its not limited to one type of data I must do my best to understand what data changes are coming and use interfaces to decouple my program from that change Interfaces should describe behavior and not state They should be verbs and not nouns Generalized interfaces that focus on behavior are best Interfaces with more than one method have more than one reason to change Interfaces that are based on nouns tend to be less reusable are more susceptible to change and defeat the purpose of the interface  76  Uncertainty about change is not a license to guess but a directive to STOP and learn more I must distinguish between code that defends against fraud vs protects against accidents Use an interface when     Users of the API need to provide an implementation detail APIs have multiple implementations they need to maintain internally Parts of the API that can change have been identified and require decoupling  Dont use an interface      For the sake of using an interface To generalize an algorithm When users can declare their own interfaces If its not clear how the interface makes the code better  
48 Interfaces Are Valueless The first important thing to understand is that an interface type declares a valueless type Listing 424 type reader interface  readb byte int error   Type reader is not a struct type but an interface type Its declaration is not based on state but behavior Interface types declare a methodset of behavior that concrete data must exhibit in order to satisfy the interface There is nothing concrete about interface types therefore they are valueless Listing 425 var r reader  Because they are valueless the construction of a variable like r is odd because in our programming model r does not exist its valueless There is nothing about r itself that I can manipulate or transform This is a critical concept to understand Im never working with interface values only concrete values An interface has a compiler representation internal type but from our programming model interfaces are valueless  
49 Implementing Interfaces Go is a language that is about convention over configuration When it comes to a concrete type implementing an interface there is no exception  77  Listing 426 type reader interface  readb byte int error  type file struct  name string  func file readb byte int error  s  rsschanneltitleGoing Gotitlechannelrss copyb s return lens nil   The code declares a type named file and then declares a method named read Because of these two declarations I can say the following The concrete type file now implements the reader interface using value semantics Every word I just said is important In Go all I have to do is declare the full methodset of behavior defined by an interface to implement that interface In this case that is what Ive done since the reader interface only declares a single act of behavior named read Listing 427 type reader interface  readb byte int error  type pipe struct  name string  func pipe readb byte int error  s  name Bill title developer copyb s return lens nil   This code declares a type named pipe and then declares a method name read Because of these two declarations I can say the following The concrete type pipe now implements the reader interface using value semantics Now I have two concrete types implementing the reader interface Two concrete types each with their unique implementation One type is reading file systems and the other networks  78  
410 Polymorphism Polymorphism means that a piece of code changes its behavior depending on the concrete data its operating on This was said by Tom Kurtz who is the inventor of BASIC This is the definition we will use moving forward Listing 428  retrieve can read any device and process the data func retriever reader error  data  makebyte 100 len err  rreaddata if err  nil  return err  fmtPrintlnstringdatalen return nil   Take a look at the type of data this function accepts It wants a value of type reader Thats impossible since reader is an interface and interfaces are valueless types It cant be asking for a reader value they dont exist If the function is not asking for a reader value then what is the function asking for It is asking for the only thing it can ask for concrete data The function retrieve is a polymorphic function because its asking for concrete data not based on what the data is concrete type but based on what the data can do interface type Listing 429 f  filedatajson p  pipecfgservice retrievef retrievep  I can construct two concrete values one of type file and one of type pipe Then I can pass a copy of each value to the polymorphic function This is because each of these values implement the full method set of behavior defined by the reader interface When the concrete file value is passed into retrieve the value is stored inside a two word internal type representing the interface value  79  Figure 43  The second word of the interface value points to the value being stored In this case its a copy of the file value since value semantics are at play The first word points to a special data structure that is called the iTable The iTable serves 2 purposes   It describes the type of value being stored In my case its a file value    It provides function pointers to the concrete implementations of the method set for the type of value being stored  When the read call is made against the interface value an iTable lookup is performed to find the concrete implementation of the read method associated with the type Then the method call is made against the value being stored in the second word I can say retrieve is a polymorphic function because the concrete value pipe can be passed into retrieve and now the call to read against the interface value changes its behavior This time that call to read is reading a network instead of reading a file  
411 Method Set Rules Implementing an interface using pointer semantics applies some constraints on interface compliance 80 Listing 430 type notifier interface notify type user struct name string email string func u user notify fmtPrintfSending User Email To ssn uname uemail func sendNotificationn notifier nnotify func main u userBill billemailcom sendNotificationu The notifier interface is implemented by the user type using pointer semantics When value semantics are used to make the polymorphic call the following compiler message is produced cannot use u type user as type notifier in argument to sendNotification user does not implement notifier notify method has pointer receiver This is because there is a special set of rules in the specification about method sets These rules define what methods are attached to values and pointers of a type They are in place to maintain the highest level of integrity in my program These are the rules defined in the specification For any value of type T only those methods implemented with a value receiver for that type belong to the method set of that value For any address of type T all methods implemented for that type belong to the method set of that value In other words when working with an address pointer all methods implemented are attached and available to be called When working with a value only those methods implemented with value receivers are attached and available to be called In the previous lesson about methods I was able to call a method against
a concrete piece of data regardless of the data semantics declared by the receiver This is because the compiler can adjust to make the call In this case a value is being stored inside an interface and the methods must exist No adjustments can be made 81 The question now becomes Why cant methods implemented with pointer receivers be attached to values of type T What is the integrity issue here One reason is because I cant guarantee that every value of type T is addressable If a value doesnt have an address it cant be shared Listing 431 type duration int func d duration notify fmtPrintlnSending Notification in d func main duration42notify Compiler Error cannot call pointer method on duration42 cannot take the address of duration42 In this example the value of 42 is a constant of kind int Even though the value is converted into a value of type duration its not being stored inside a variable This means the value is never on the stack or heap There isnt an address Constants only live at compile time The second reason is the bigger reason The compiler is telling me that I am not allowed to use value semantics if I have chosen to use pointer semantics In other words I am being forced to share the value with the interface since its not safe to make a copy of a value that a pointer points to If I chose to implement the method with pointer semantics I
am stating that a value of this type isnt safe to be copied Listing 432 func main u userBill billemailcom sendNotificationu To fix the compiler message I must use pointer semantics on the call to the polymorphic function and share u The answer is not to change the method to use value semantics
412 Slice of Interface When I declare a slice of an interface type Im capable of grouping different concrete values together based on what they can do This is why Go doesnt need the concept of subtyping Its not about a common DNA its about a common behavior  82  Listing 433 type printer interface  print  type canon struct  name string  func c canon print  fmtPrintfPrinter Name sn cname  type epson struct  name string  func e epson print  fmtPrintfPrinter Name sn ename  func main  c  canonPIXMA TR4520 e  epsonWorkForce Pro WF3720 printers  printer c e  cname  PROGRAF PRO1000 ename  Home XP4100    for  p  range printers  pprint   Output Printer Name PIXMA TR4520 Printer Name Home XP4100  The code shows how a slice of the interface type printer allows me to create a collection of different concrete printer types Iterating over the collection and leveraging polymorphism since the call to pprint changes its behavior depending on the concrete value the code is operating against The example also shows how the choice of data semantics changes the behavior of the program When storing the data using value semantics the change to the original value is not seen This is because a copy is stored inside the interface When pointer semantics are used any changes to the original value are seen  
413 Embedding This first example does not show embedding just the declaration of two struct types working together as a field from one type to the other 83 Listing 434 type user struct name string email string type admin struct person user level string NOT Embedding This is embedding Listing 435 type user struct name string email string type admin struct user level string Value Semantic Embedding The person field is removed and just the type name is left I can also embed a type using pointer semantics Listing 436 type user struct name string email string type admin struct user level string Pointer Semantic Embedding In this case a pointer of the type is embedded In either case accessing the embedded value is done through the use of the types name The best way to think about embedding is to view the user type as an inner type and admin as an outer type Its this innerouter type relationship that is magical because with embedding everything related to the inner type both fields and methods can be promoted up to the outer type 84 Listing 437 type user struct name string email string func u user notify fmtPrintfSending user email To ssn uname uemail type admin struct user level string Pointer Semantic Embedding func main ad admin user user name john smith email johnyahoocom level super adusernotify adnotify Outer type promotion Output Sending user email To john smithjohnyahoocom Sending user email To john smithjohnyahoocom Once I add a method
named notify for the user type and then a small main function I can see the output is the same whether I call the notify method through the inner pointer value directly or through the outer type value The notify method declared for the user type is accessible directly by the admin type value Though this looks like inheritance I must be careful This is not about reusing state but about promoting behavior Listing 438 type notifier interface notify func sendNotificationn notifier nnotify Now I add an interface and a polymorphic function that accepts any concrete value that implements the full method set of behavior defined by the notifier interface Which is just a method named notify Because of embedding and promotion values of type admin now implement the 85 notifier interface Listing 439 func main ad admin user user name john smith email johnyahoocom level super sendNotificationad Output Sending user email To john smithjohnyahoocom I can send the address of the admin value into the polymorphic function since embedding promotes the notify behavior up to the admin type Listing 440 type admin struct user Pointer Semantic Embedding level string func a admin notify fmtPrintfSending admin Email To ssn aname aemail When the outer type implements a method already implemented by the inner type the promotion doesnt take place Listing 441 func main ad admin user user name john smith email johnyahoocom level super sendNotificationad Output Sending admin email To john smithjohnyahoocom I can see the outer types method is
now being executed 86
414 Exporting Exporting provides the ability to declare if an identifier is accessible to code outside of the package its declared in A package is the basic unit of compiled code in Go It represents a physical compiled unit of code usually as a compiled library on the host operating system Exporting determines access to identifiers across package boundaries Listing 442 package counters type AlertCounter int  In this case since a capital letter is being used to name the type AlterCounter the type is exported and can be referenced directly by code outside of the counters package Listing 443 package counters type alertCounter int  Now that I changed the types name to start with a lowercase letter the type is unexported This means only code inside the counters package can reference this type directly Listing 444 package counters type alertCounter int func Newvalue int alertCounter  return alertCountervalue   Even though the code above is legal syntax and will compile there is no value in it Returning a value of an unexported type is confusing since the caller who will probably exist in a different package cant reference the type name directly  87  Listing 445 package main import  fmt   githubcomardanlabsexportingexample3counters  func main  counter  countersNew10 fmtPrintfCounter dn counter   In this case the main function in package main calls the countersNew function successfully and the compiler can declare and construct a variable of the unexported type This doesnt mean I should do this nor does it mean Im getting any real protections for this This should be avoided and if New will return a value it should be of an exported type Listing 446 package users type User struct  Name string ID int password string   When it comes to fields in a struct the first letter declares if the field is accessible to code outside of the package its declared in In this case Name and ID are accessible but password is not Its an idiom to separate exported and unexported fields in this manner if this is reasonable or practical to do Normally all fields would be one or the other Listing 447 package users type user struct  Name string ID int  type Manager struct  Title string user   In this scenario even though the user type is unexported it has two exported fields This means that when the user type is embedded in the exported Manager type the user fields promote and are accessible Its common to have types that 88  are unexported with exported fields because the reflection package can only operate on exported fields Marshallers wont work otherwise The example creates a bad situation where code outside of package users can construct a Manager but since the embedded type user is unexported the fields for those type can be initialized This creates partial construction problems that will lead to bugs I need to be consistent with exporting and unexporting  89  
Chapter 5 Software Design In this chapter I will learn about the mechanics and semantics behind error handling and an important design pattern in Go called composition Along the way I will learn how to group data of different types and learn about precision based semantics  
51 Grouping Different Types of Data Its important to remember that in Go the concepts of subtyping or subclassing really dont exist and these design patterns should be avoided The following is an antipattern I shouldnt follow or implement Listing 51 type Animal struct Name string IsMammal bool The Animal type is being declared as a base type that tries to define data that is common to all animals I also attempt to provide some common behavior to an animal as well Listing 52 func a Animal Speak fmtPrintlnUGH My name is aName it is aIsMammal I am a mammal Most animals have the ability to speak in one way or the other However trying to apply this common behavior to just an animal doesnt make any sense At this point I have no idea what sound this animal makes so I wrote UGH Listing 53 type Dog struct Animal PackFactor int Now the real problems begin Im attempting to use embedding to make a Dog everything an Animal is plus more On the surface this will seem to work but there will be problems With that being said a Dog does have a specific way they speak 90 Listing 54 func d Dog Speak fmtPrintlnWoof My name is dName it is dIsMammal I am a mammal with a pack factor of dPackFactor In the implementation of the Speak method I can change out UGH for Woof This is specific to how a dog speaks Listing 55 type Cat struct
Animal ClimbFactor int If Im going to have a Dog that represents an Animal then I have to have a Cat Using embedding a Cat is everything an Animal is plus more Listing 56 func c Cat Speak fmtPrintlnMeow My name is cName it is cIsMammal I am a mammal with a climb factor of cClimbFactor In the implementation of the Speak method I can change out UGH for Meow This is specific to how a cat speaks Everything seems fine and it looks like embedding is providing the same functionality as inheritance does in other languages Then I try to go ahead and group dogs and cats by the fact they have a common DNA of being an Animal 91 Listing 57 animals Animal Dog Animal Animal Name Fido IsMammal true PackFactor 5 Cat Animal Animal Name Milo IsMammal true ClimbFactor 4 for animal range animals animalSpeak When I try to do this the compiler complains that a Dog and Cat are not an Animal and this is true Embedding isnt the same as inheritance and this is the pattern I need to stay away from A Dog is a Dog a Cat a Cat and an Animal an Animal I cant pass Dogs and Cats around as if they are Animals because they are not This kind of mechanic is also not very flexible It requires configuration by the developer and unless I have access to the code I cant make configuration changes over time If this is
not how we can construct a collection of Dogs and Cats how can we do this in Go Its not about grouping through common DNA its about grouping through common behavior Behavior is the key Listing 58 type Speaker interface Speak If I use an interface then I can define the common method set of behavior that I want to group different types of data against 92 Listing 59 speakers Speaker Dog Animal Animal Name Fido IsMammal true PackFactor 5 Cat Animal Animal Name Milo IsMammal true ClimbFactor 4 for speaker range speakers speakerSpeak In the new code I can now group Dogs and Cats together based on their common set of behavior which is the fact that Dogs and Cats can speak In fact the Animal type is really type pollution because declaring a type just to share a set of common states is a smell and should be avoided Listing 510 type Dog struct Name string IsMammal bool PackFactor int type Cat struct Name string IsMammal bool ClimbFactor int In this particular case I would rather see the Animal type removed and the fields copied and pasted into the Dog and Cat types Later I will have notes about better patterns that eliminate these scenarios from happening Here are the code smells from the original code The Animal type provides an abstraction layer of reusable state The program never needs to create or solely use a value of Animal type The implementation of the Speak method for the
Animal type is generalized The Speak method for the Animal type is never going to be called 93 Guidelines around declaring types Declare types that represent something new or unique Dont create aliases just for readability Validate that a value of any type is created or used on its own Embed types not because I need the state but because we need the behavior If I am not thinking about behavior Im locking myself into the design that I cant grow in the future without cascading code changes Question types that are aliases or abstractions for an existing type Question types whose sole purpose is to share a common set of states
52 Dont Design With Interfaces Unfortunately too many developers attempt to solve problems in the abstract first They focus on interfaces right away and this leads to interface pollution As a developer I exist in one of two modes a programmer and then an engineer When I am programming I am focused on getting a piece of code to work Trying to solve the problem and break down walls Prove that my initial ideas work That is all I care about This programming should be done in the concrete and is never production ready Once I have a prototype of code that solves the problem I need to switch to engineering mode I need to focus on how to write the code at a microlevel for data semantics and readability then at a macrolevel for mental models and maintainability I also need to focus on errors and failure states This work is done in a cycle of refactoring Refactoring for readability efficiency abstraction and for testability Abstracting is only one of a few refactors that need to be performed This works best when I start with a piece of concrete code and then DISCOVER the interfaces that are needed Dont apply abstractions unless they are absolutely necessary Every problem I solve with code is a data problem requiring me to write data transformations If I dont understand the data I dont understand the problem If I dont understand the problem I cant write any code Starting with a concrete solution that is based on the concrete data structures is critical As Rob Pike said Data dominates If youve chosen the right data structures and organized things well the algorithms will almost always be selfevident  Rob Pike When is abstraction necessary When I see a place in the code where the data could change and I want to minimize the cascading code effects that would result I might use abstraction to help make code testable but I should try to avoid this if 94  possible The best testable functions are functions that take raw data in and send raw data out It shouldnt matter where the data is coming from or going In the end start with a concrete solution to every problem Even if the bulk of that is just programming Then discover the interfaces that are absolutely required for the code today Dont design with interfaces discover them  Rob Pike  
53 Composition The best way to take advantage of embedding is through the compositional design pattern The idea is to compose larger types from smaller types and focus on the composition of behavior Listing 511 type Xenia struct  Host string Timeout timeDuration  func Xenia Pulld Data error  switch randIntn10  case 1 9 return ioEOF case 5 return errorsNewError reading data from Xenia default dLine  Data fmtPrintlnIn dLine return nil    The Xenia type represents a system that I need to pull data from The implementation is not important What is important is that the method Pull can succeed fail or not have any data to pull Listing 512 type Pillar struct  Host string Timeout timeDuration  func Pillar Stored Data error  fmtPrintlnOut dLine return nil   The Pillar type represents a system that I need to store data into What is important again is that the method Store can succeed or fail These two types represent a primitive layer of code that provides the base behavior  95  required to solve the business problem of pulling data out of Xenia and storing that data into Pillar Listing 513 func Pullx Xenia data Data int error  for i  range data  if err  xPulldatai err  nil  return i err   return lendata nil  func Storep Pillar data Data int error  for i  range data  if err  pStoredatai err  nil  return i err   return lendata nil   The next layer of code is represented by these two functions Pull and Store They build on the primitive layer of code by accepting a collection of data values to pull or store in the respective systems These functions focus on the concrete types of Xenia and Pillar since those are the systems the program needs to work with at this time Listing 514 func Copysys System batch int error  data  makeData batch for  i err  PullsysXenia data if i  0  if  err  StoresysPillar datai err  nil  return err      if err  nil  return err     The Copy function builds on top of the Pull and Store functions to move all the data that is pending for each run If I notice the first parameter to Copy its a type called System  96  Listing 515 type System struct  Xenia Pillar   The initial idea of the System type is to compose a system that knows how to Pull and Store In this case composing the ability to Pull and Store from Xenia and Pillar Listing 516 func main  sys  System Xenia Xenia Host localhost8000 Timeout timeSecond  Pillar Pillar Host localhost9000 Timeout timeSecond   if err  Copysys 3 err  ioEOF  fmtPrintlnerr    Finally the main function can be written to construct a Xenia and Pillar within the composition of a System Then the System can be passed to the Copy function and data can begin to flow between the two systems With all this code I now have my first draft of a concrete solution to a concrete problem  
54 Decoupling With Interfaces The next step is to understand what could change in the program In this case what can change is the systems themselves Today its Xenia and Pillar tomorrow it could be Alice and Bob With this knowledge I want to decouple the existing concrete solution from this change To do that I want to change the concrete functions to be polymorphic functions 97 Listing 517 func Pullp Puller data Data int error for i range data if err pPulldatai err nil return i err return lendata nil func Stores Storer data Data int error for i range data if err sStoredatai err nil return i err return lendata nil Currently the Pull function accepts a Xenia value and the Store function accepts a Pillar value In the end it wasnt Xenia and Pillar that was important whats important is a concrete value that knows how to Pull and Store I can change these concrete functions to be polymorphic by asking for data based on what it can do instead of what it is Listing 518 type Puller interface Pulld Data error type Storer interface Stored Data error These two interfaces describe what concrete data must do and its these types that are replaced in the declaration of the Pull and Store functions Now these functions are polymorphic When Alice and Bob are declared and implemented as a Puller and a Storer they can be passed into the functions I am not done yet The Copy function
needs to be polymorphic as well 98 Listing 519 func Copyps PullStorer batch int error data makeData batch for i err Pullps data if i 0 if err Storeps datai err nil return err if err nil return err The Copy function is no longer asking for a System value but any concrete value that knows how to both Pull and Store Listing 520 type PullStorer interface Puller Storer The PullStorer interface is declared through the use of composition Its composed of the Puller and Storer interfaces Work towards composing larger interfaces from smaller ones Notice how the PullStorer variable is now being passed into the Pull and Store functions How is this possible when the type information is different Listing 521 func Pullp Puller data Data int error i err Pullps data func Stores Storer data Data int error if err Storeps datai err nil I always need to remember I am never passing an interface value around my program since they dont exist and are valueless I can only pass concrete data So the concrete data stored inside of the interface ps variable is whats being passed to Pull and Store Isnt it true the concrete value stored inside of ps must know how to Pull and Store 99 Figure 51 Since a System is composed from a Xenia and Pillar System implements the PullStorer interface With these changes I can now create new concrete types that implement the PullStorer interface Listing 522 type System1 struct Xenia Pillar
type System2 struct Alice Bob type System3 struct Xenia Bob type System4 struct Alice Pillar When I think about this more declaring different System types for all the possible combinations is not realistic This will work but the maintenance nightmare requires a better solution
55 Interface Composition What if I decided to compose my concrete system type from two interface types  100  Listing 523 type System struct  Puller Storer   This is an interesting solution This would allow the application to inject the concrete Puller or Storer into the system at application startup Listing 524 func main  sys  System Puller Xenia Host localhost8000 Timeout timeSecond  Storer Pillar Host localhost9000 Timeout timeSecond      if err  Copysys 3 err  ioEOF  fmtPrintlnerr   This one system type implements the PullStorer interface for all possible combinations of concrete types Figure 52  With this change the application is fully decoupled from changes to a new system that may come online over time  
56 Precision Review The next question to ask is are the polymorphic functions as precise as they otherwise could be This is a part of the engineering process that cant be skipped The answer is no two changes can be made 101  Listing 525 func Copysys System batch int error   The Copy function doesnt need to be polymorphic anymore since there will only be a single System type The PullStorer interface type can be removed from the program Remember I moved the polymorphism inside the type when I used composition with the interface types Listing 526 func Copyp Puller s Storer batch int error   This is another change that can be made to the Copy function This change makes the function more precise and polymorphic again Now the function is asking for exactly what it needs based on what the concrete data can do Figure 53  With that change the System struct type can be removed from the program as well Listing 525 func main  x  Xenia Host localhost8000 Timeout timeSecond  p  Pillar Host localhost9000 Timeout timeSecond  if err  Copyx p 3 err  ioEOF  fmtPrintlnerr    By removing the PullStorer and System types the program simplifies The main function can focus on constructing the concrete Puller and Storer values necessary for that moving data The type system and APIs are more precise  102  This idea of precision comes from Edsger W Dijkstra The purpose of abstraction is not to be vague but to create a new semantic level in which one can be absolutely precise  Edsger W Dijkstra  
57 Implicit Interface Conversions As I saw in the last example An interface value of one type can be passed for a different interface type if the concrete value stored inside the interface implements both behaviors This could be considered an implicit interface conversion but its better to think about how concrete data is being moved through interfaces in a decoupled state Listing 526 type Mover interface  Move  type Locker interface  Lock Unlock  type MoveLocker interface  Mover Locker   Given these three interfaces where MoveLocker is the composition of Mover and Locker Listing 527 type bike struct func bike Move  fmtPrintlnMoving the bike  func bike Lock  fmtPrintlnLocking the bike  func bike Unlock  fmtPrintlnUnlocking the bike   And given this concrete type bike that implements all three interfaces What can I do  103  Listing 528 var ml MoveLocker var m Mover  I can construct a value of type MoveLocker and Mover to its zero value state These are interface values that are truly valueless Listing 529 ml  bike  Then I can construct a value of type bike to its zero value state and assign a copy to the MoveLocker variable ml This is possible because a bike implements all three behaviors and the compiler can see that the implementation exists Listing 530 m  ml  I can then assign the MoveLocker variable ml to the Mover variable m This is possible because Im not assigning the interface value ml but the concrete value stored inside of ml which is a bike value The compiler knows that any concrete value stored inside of ml must also implement the Mover interface This assignment however is not valid Listing 531 ml  m cannot use m type Mover as type MoveLocker in assignment Mover does not implement MoveLocker missing Lock method  I cant assign the Mover variable m back to the MoverLocker variable ml because the compiler can only guarantee that the concrete value stored inside of m knows how to Move It doesnt know at compile time if the concrete value also knows how to Lock and Unlock  
58 Type assertions With all that being said there is a way at runtime to test if the assignment is legal and then make it happen That is by using a type assertion Listing 532 b  mbike ml  b  A type assertion allows me at runtime to ask a question is there a value of the given type stored inside the interface I see that with the mbike syntax In this 104  case I am asking if there is a bike value stored inside of m at the moment the code is executed If there is then the variable b is given a copy of the bike value stored Then the copy can be copied inside of the ml interface variable If there isnt a bike value stored inside of the interface value then the program panics I want this if there absolutely should have been a bike value stored What if there is a chance there isnt and that is valid Then I need the second form of the type assertion Listing 533 b ok  mbike  In this form if ok is true there is a bike value stored inside of the interface If ok is false then there isnt and the program does not panic The variable b however is still of type bike but it is set to its zero value state Listing 534 func main  randSeedtimeNowUnixNano mvs  fmtStringer Car Cloud  for i  0 i  10 i  rn  randIntn2 if v is  mvsrnCloud is  fmtPrintlnGot Lucky v continue  fmtPrintlnGot Unlucky     Assuming the program does declare two types named Car and Cloud that each implement the fmtStringer interface I can construct a collection that allows me to store a value of both Car and Cloud Then 10 times I randomly choose a number from 0 to 1 and perform a type assertion to see if the value at that random index contains a Cloud value Since its possible its not of type Cloud the second form of the type assertion is critical here  
59 Interface Pollution I can spot interface pollution from a mile away It mostly comes from the fact that people are designing software with interfaces instead of discovering them I should design a concrete solution to the problem first Then I can discover where the program needs to be polymorphic if at all 105 These are things Ive heard from other developers Im using interfaces because we have to use interfaces No We dont have to use interfaces We use interfaces when its practical and reasonable to do so There is a cost of using interfaces a level of indirection and allocation when we store concrete values inside of them Unless the cost of the allocation is worth what Im gaining by decoupling I shouldnt be using interfaces I need to be able to test my code so I need to use interfaces No I must design my API for the user first not my test If the API is not testable I should question if its usable There are different layers of APIs as well The lower level unexported APIs can and should focus on testability The higher level exported APIs need to focus on usability Functions that accept raw data in and return raw data out are the most testable Separate the data transformation from where the data comes from and where it is going This is a refactoring exercise I need to perform during the engineering coding cycle Below is an example that creates interface pollution by
improperly using an interface when one is not needed Listing 535 type Server interface Start error Stop error Wait error The Server interface defines a contract for TCP servers The problem here is I dont need a contract I need an implementation There will only be one implementation as well especially since I am the one implementing it I do not need someone else to implement this for me Plus this interface is based on a noun and not a verb Concrete types are nouns since they represent the concrete problem Interfaces describe the behavior and Server is not behavior Here are some ways to identify interface pollution A package declares an interface that matches the entire API of its own concrete type The interfaces are exported but the concrete types implementing the interface are unexported 106 The factory function for the concrete type returns the interface value with the unexported concrete type value inside The interface can be removed and nothing changes for the user of the API The interface is not decoupling the API from change Guidelines around interface pollution Use an interface When users of the API need to provide an implementation detail When APIs have multiple implementations that need to be maintained When parts of the APIs that can change have been identified and require decoupling Question an interface When its only purpose is for writing testable APIs write usable APIs first When its not providing support for the API to decouple from change When its
not clear how the interface makes the code better
510 Interface Ownership One thing that is different about Go from other languages is the idea of convention over configuration This really shows itself with how Go handles interface compliance Because the compiler can perform static code analysis to determine if a concrete value implements an interface the developer declaring the concrete type doesnt need to provide interfaces as well Listing 536 package pubsub type PubSub struct  host string  func Newhost string PubSub  return PubSub host host   func ps PubSub Publishkey string v interface error   PRETEND THERE IS A SPECIFIC IMPLEMENTATION return nil  func ps PubSub Subscribekey string error   PRETEND THERE IS A SPECIFIC IMPLEMENTATION return nil   Ive just implemented a new API that provides a concrete implementation for publish and subscribe There are no interfaces being provided because this API does  107  not need one This is a single concrete implementation What if the application developer wanting to use this new API needs an interface because they have the need to mock this implementation during tests In Go that developer can declare the interface and the compiler can identify the compliance Listing 537 package main type publisher interface  Publishkey string v interface error Subscribekey string error  type mock struct func m mock Publishkey string v interface error   ADD MY MOCK FOR THE PUBLISH CALL return nil  func m mock Subscribekey string error   ADD MY MOCK FOR THE SUBSCRIBE CALL return nil   This code in the main package is declaring an interface This interface represents the API that the application is using from the pubsub package The developer has implemented their own pubsub implementation for testing The key here is that this application developer doesnt use any concrete implementation directly but decouples themselves through their own interface Listing 538 func main  pubs  publisher pubsubNewlocalhost mock     for  p  range pubs  pPublishkey value pSubscribekey   To provide an example the main function constructs a collection that is initialized with the pubsub implementation and the mock implementation The publisher interface allows this Then a for range loop is implemented to show how the application code is abstracted from any concrete implementation  
511 Error Handling Integrity matters and its a big part of the engineering process At the heart of 108 integrity is error handling When it comes to Go error handling is not an exception to be handled later or somewhere else in the code Its a part of the main path and needs to be a main focus Developers have the responsibility to return enough context about any error so a user can make an informed decision about how to proceed Handling an error is about three things logging the error not propagating the error any further and determining if the Goroutineprogram needs to be terminated In Go errors are just values so they can be anything I need them to be They can maintain any state or behavior Listing 539 httpgolangorgpkgbuiltinerror type error interface Error string This is the error interface and its an interface that is built into the language This is why it appears to be an unexported identifier Any concrete value that implements this interface can be used as an error value One important aspect of Go is that error handling is done in a decoupled state through this interface A key reason for this is because error handling is an aspect of my application that is more susceptible to change and improvement This interface is the type Go applications must use as the return type for error handling Listing 540 httpgolangorgsrcpkgerrorserrorsgo type errorString struct s string httpgolangorgsrcpkgerrorserrorsgo func e errorString Error string return es This
is the most commonly used error value in Go programs Its declared in the errors package from the standard library Notice how the type is unexported and it has one unexported field which is a string I can also see how pointer semantics are used to implement the error interface This means only addresses to values of this type can be shared and stored inside the interface The method just returned the error string Its important to remember the implementation of the Error method serves the purpose of implementing the interface and for logging If any user needs to parse 109 the string returned from this method I have failed to provide the user the right amount of context to make an informed decision Listing 541 httpgolangorgsrcpkgerrorserrorsgo func Newtext string error return errorStringtext The New function is how an error using the concrete type errorString is constructed Notice how the function returns the error using the error interface Also notice how pointer semantics are being used Listing 542 func main if err webCall err nil fmtPrintlnerr return fmtPrintlnLife is good func webCall error return Newbad request Context is everything with errors Each error must provide enough context to allow the caller to make an informed decision about the state of the goroutineapplication In this example the webCall function returns an error with the message Bad Request In the main function a call is made to webCall and then a check is made to see if an error has occurred with
the call Listing 543 if err webCall err nil fmtPrintlnerr return The key to the check is err nil What this condition is asking is is there a concrete value stored inside the err interface value When the interface value is storing a concrete value there is an error In this case the context is literally just the fact that a concrete value exists its not important what the concrete value is What if its important to know what error value exists inside the err interface variable Then error variables are a good option 110 Listing 544 var ErrBadRequest errorsNewBad Request ErrPageMoved errorsNewPage Moved Error variables provide a mechanic to identify what specific error is being returned They have an idiom of starting with the prefix Err and are based on the concrete type errorString from the errors package Listing 545 func webCallb bool error if b return ErrBadRequest return ErrPageMoved In this new version of webCall the function returns one or the other error variable This allows the caller to determine which error took place Listing 546 func main if err webCalltrue err nil switch err case ErrBadRequest fmtPrintlnBad Request Occurred return case ErrPageMoved fmtPrintlnThe Page moved return default fmtPrintlnerr return fmtPrintlnLife is good In the application after the call to webCall is made a check can be performed to see if there is a concrete value stored inside the err interface variable If there is then a switch statement is used to determine which error it was by
comparing err to the different error variables In this case the context of the error is based on which error variable was returned What if an error variable is not enough context What if some special state needs to be checked like with networking errors In these cases a custom concrete error type is the answer 111 Listing 547 type UnmarshalTypeError struct Value string Type reflectType func e UnmarshalTypeError Error string return json cannot unmarshal eValue into Go value of type eTypeString This is a custom concrete error type implemented in the json package Notice the name has a suffix of Error in the naming of the type Also notice the use of pointer semantics for the implementation of the error interface Once again the implementation is for logging and should display information about all the fields being captured Listing 548 type InvalidUnmarshalError struct Type reflectType func e InvalidUnmarshalError Error string if eType nil return json Unmarshalnil if eTypeKind reflectPtr return json Unmarshalnonpointer eTypeString return json Unmarshalnil eTypeString This is a second custom concrete error type found in the json package The implementation of the Error method is a bit more complex but once again just for logging and using pointer semantics Listing 549 func Unmarshaldata byte v interface error rv reflectValueOfv if rvKind reflectPtr rvIsNil return InvalidUnmarshalErrorreflectTypeOfv return UnmarshalTypeErrorstring reflectTypeOfv Here is a portion of the Unmarshal function Notice how it constructs the concrete error values in the return passing them back to the caller through the error interface
Pointer semantic construction is being used because pointer semantics were used in the declaration of the Error method The context of the error here is more about the type of error stored inside the error interface There needs to be a way to determine that 112 Listing 550 func main var u user err Unmarshalbytenamebill u if err nil switch e errtype case UnmarshalTypeError fmtPrintfUnmarshalTypeError Values Typevn eValue eType case InvalidUnmarshalError fmtPrintfInvalidUnmarshalError Typevn eType default fmtPrintlnerr return fmtPrintlnName uName A generic type assertion within the scope of the switch statement is how I can write code to test what type of value is being stored inside the err interface value Type is the context here and now I can test and take action with access to all the states of the error However this poses one problem Im no longer decoupled from the concrete error value This means if the concrete error value is changed my code can break The beautiful part of using an interface for error handling is being decoupled from breaking changes If the concrete error value has a method set then I can use an interface for the type check As an example the net package has many concrete error types that implement different methods One common method is called Temporary This method allows the user to test if the networking error is critical or just something that can recover on its own 113 Listing 551 type temporary interface Temporary bool func c client BehaviorAsContext for
line err creaderReadStringn if err nil switch e errtype case temporary if eTemporary logPrintlnTemporary Client leaving chat return default if err ioEOF logPrintlnEOF Client leaving chat return logPrintlnreadroutine err fmtPrintlnline In this code the call to ReadString could fail with an error from the net package In this case an interface is declared that represents the common behavior a given concrete error value could implement Then with a generic type assertion I test if that behavior exists and I can call into it The best part I stay in a decoupled state with my error handling
512 Always Use The Error Interface One mistake Go developers can make is when they use the concrete error type and not the error interface for the return type for handling errors If I were to do this bad things could happen  114  Listing 552 type customError struct func c customError Error string  return Find the bug  func fail byte customError  return nil nil  func main  var err error if  err  fail err  nil  logFatalWhy did this fail  logPrintlnNo Error  Output Why did this fail  Why does this code think there is an error when the fail function returns nil for the error Its because the fail function is using the concrete error type and not the error interface In this case there is a nil pointer of type customError stored inside the err variable That is not the same as a nil interface value of type error  
513 Handling Errors Handling errors is more of a macro level engineering conversation In my world error handling means the error stops with the function handling the error the error is logged with full context and the error is checked for its severity Based on the severity and ability to recover a decision to recover move on or shutdown is made One problem is that not all functions can handle an error One reason could be because not all functions are allowed to log What happens when an error is being passed back up the call stack and cant be handled by the function receiving it An error needs to be wrapped in context so the function that eventually handles it can properly do so There are two options for wrapping extra context around an error I can use Dave Cheneys errors package or I can use standard library support that can be found in the errors and fmt packages Whatever I decide its important to annotate errors for enough context to help identify and fix problems Both at runtime and after  115  Using Dave Cheneys package Listing 553 package main import  fmt   githubcompkgerrors  type AppError struct  State int  func c AppError Error string  return fmtSprintfApp Error State d cState  func main  if err  firstCall10 err  nil  switch v  errorsCauseerrtype  case AppError fmtPrintlnCustom App Error vState default fmtPrintlnDefault Error    fmtPrintfvn err   func firstCalli int error  if err  secondCalli err  nil  return errorsWrapferr secondCalld i  return nil  func secondCalli int error  return AppError99  Output Custom App Error 99 secondCall10 App Error State 99  Whats nice about this package is the errorsWrap and errorsCause APIs They make the code a bit more readable  116  Using the standard library Listing 554 package main import  errors fmt  type AppError struct  State int  func c AppError Error string  return fmtSprintfApp Error State d cState  func Causeerr error error  root  err for  if err  errorsUnwraproot err  nil  return root  root  err   func main  if err  firstCall10 err  nil  var ap AppError if errorsAserr ap  fmtPrintlnAs says it is an AppError  switch v  Causeerrtype  case AppError fmtPrintlnCustom App Error vState default fmtPrintlnDefault Error    fmtPrintfvn err   func firstCalli int error  if err  secondCalli err  nil  return fmtErrorfsecondCalld  w i err  return nil  func secondCalli int error  return AppError99  Output  As says it is an AppError  Custom App Error 99 secondCall10 App Error State 99  117  To use the standard library in a similar way the Cause function needed to be written In this example I can see the use of the errorsAs function  118  
Chapter 6 Concurrency In this chapter I will learn about the mechanics and semantics behind writing multithreaded code in Go I will learn what concurrency is and how to apply it safely by understanding the concepts of synchronization and orchestration  
61 Scheduler Semantics When a Go program starts up the Go runtime asks the machine virtual or physical how many operating system threads can run in parallel This is based on the number of cores that are available to the program For each thread that can be run in parallel the runtime creates an operating system thread M and attaches that to a data structure that represents a logical processor P inside the program This P and M represent the compute power or execution context for running the Go program Also an initial Goroutine G is created to manage the execution of instructions on a selected MP Just like an M manages the execution of instructions on the hardware a G manages the execution of instructions on the M This creates a new layer of abstraction above the operating system but it moves execution control to the application level Figure 61 Since the Go scheduler sits on top of the operating system scheduler its important to have some semantic understanding of the operating system scheduler and the constraints it applies to the Go scheduler and applications The operating system scheduler has the job of creating the illusions that multiple pieces of work are being executed at the same time Even when this is physically 119 impossible This requires some tradeoffs in the design of the scheduler Before I go any further its important to define some words Work A set of instructions to be executed for a running application This
is accomplished by threads and an application can have 1 to many threads Thread A path of execution that is scheduled and performed Threads are responsible for the execution of instructions on the hardware Thread States A thread can be in one of three states Running Runnable or Waiting Running means the thread is executing its assigned instructions on the hardware by having a G placed on the M Runnable means the thread wants time on the hardware to execute its assigned instructions and is sitting in a run queue Waiting means the thread is waiting for something before it can resume its work Waiting threads are not a concern of the scheduler Concurrency This means undefined out of order execution In other words given a set of instructions that would be executed in the order provided they are executed in a different undefined order but all executed The key is the result of executing the full set of instructions in any undefined order produces the same result I will say work can be done concurrently when the order the work is executed in doesnt matter as long as all the work is completed Parallelism This means doing a lot of things at once For this to be an option I need the ability to physically execute two or more operating system threads at the same time on the hardware CPU Bound Work This is work that does not cause the thread to naturally move into a waiting state Calculating
fibonacci numbers would be considered CPUBound work IO Bound Work This is work that does cause the thread to naturally move into a waiting state Fetching data from different URLs would be considered IOBound work Synchronization When two or more Goroutines will need to access the same memory location potentially at the same time they need to be synchronized and take turns If this synchronization doesnt take place and at least one Goroutine is performing a write I can end up with a data race Data races are a cause of data corruption bugs that can be difficult to find Orchestration When two or more Goroutines need to signal each other with or without data orchestration is the mechanic required If orchestration does not take place guarantees about concurrent work being performed and completed will be 120 missed This can cause all sorts of data corruption bugs There are lots of little details related to the scheduling semantics so to learn more read the three posts in chapter 14 titled Scheduling In Go
62 Concurrency Basics Starting with a basic concurrency problem that requires orchestration Listing 61 func init runtimeGOMAXPROCS1 The call to GOMAXPROCS is being used to run the Go program as a single threaded Go program This program will be single threaded and have a single PM to execute all Goroutines The function is capitalized because it s also an environment variable Though this function call will overwrite the variable Listing 62 g runtimeGOMAXPROCS0 This function is an important function when I set CPU quotas to a container configuration When passing 0 the number of threads the Go program will be using is reported I must make sure that number matches the number of operating system threads I have available in my containerized environment If the numbers are not the same the Go program wont run as well as it otherwise could I might want to use the environment variable or this call to match things up Listing 63 func main var wg syncWaitGroup wgAdd2 go func lowercase wgDone go func uppercase wgDone fmtPrintlnWaiting To Finish wgWait fmtPrintlnnTerminating Program This program has to solve an orchestration problem The main Goroutine cant allow 121 the main function to return until there is a guarantee the two Goroutines being created finish their work first A WaitGroup is a perfect tool for orchestration problems that dont require data to be passed between Goroutines The signaling here is performed through an API that allows a Goroutine to wait for other Goroutines to signal theyre done
In this code a WaitGroup is constructed to its zero value state and then immediately the Add method is called to set the WaitGroup to 2 which will match the number of Goroutines to be created When I know how many Goroutines upfront that will be created I should call Add once with that number When I dont know like in a streaming service then calling Add1 is acceptable At the end of main is the call to Wait Wait holds the main Goroutine from causing the function to return When the main function returns the Go program is shut down with extreme prejudice This is why managing the orchestration with the proper guarantees is important The Wait call will block until the WaitGroup is set back to 0 In the middle of the program I have the creation of the two Goroutines Listing 64 func main go func lowercase wgDone go func uppercase wgDone Literal functions are declared and executed with the use of the keyword go At this point I am telling the Go scheduler to execute these functions concurrently To execute them in an undefined order Inside the implementation of each Goroutine is the call to Done That call is what decrements the WaitGroup by 1 Once both calls to Done are made the WaitGroup will change from 2 to 0 and then the main Goroutine will be allowed to be unblocked from the call to Wait terminating the program 122 Listing 65 func main var wg
syncWaitGroup wgAdd2 go func lowercase wgDone An important part of this orchestration pattern is keeping the Add and Done calls in the same line of sight Try not to pass the WaitGroup as a function parameter where the calls get lost This will help to reduce bugs Listing 66 Output Start Goroutines Waiting To Finish A B C D E F G H I J K L M N O P Q R S T U V W X Y Z A B C D E F G H I J K L M N O P Q R S T U V W X Y Z A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z a b c d e f g h i j k l m n o p q r s t u v w x y z a b c d e f g h i j k l m n o p q r s t u v w x y z Terminating Program When I build and run this program I see how this program runs concurrently The second Goroutine created was scheduled first It got to finish its work and then the other Goroutine ran Both ran to completion
before the program terminated The next time I run this program there is no guarantee I see the same output The only guarantee in this program is that the program wont terminate until the two Goroutines are done Even if I run this program 100 times and see the same output there is no guarantee it will happen again It may be highly probable but not guaranteed Especially not guaranteed across different versions operating systems and architectures Listing 67 func main fmtPrintlnWaiting To Finish wgWait CHANGED fmtPrintlnnTerminating Program If I comment the call to Wait what will happen when I run this program Once 123 again there is no guarantee at all anymore with what will happen but there are different possibilities The program could behave as before since calls to Println are system calls that do allow the scheduler to make a context switch The program could execute just one of the two Goroutines or possibly terminate immediately Listing 68 func main var wg syncWaitGroup wgAdd2 go func lowercase wgDone CHANGED What happens if I forget to call Done in one of the Goroutines In this case the program would deadlock since the WaitGroup cant get back down to 0 The Wait call will block forever Listing 69 Output Start Goroutines Waiting To Finish A B C D E F G H I J K L M N O P Q R S T U V W X Y Z A B C D E F G H I
J K L M N O P Q R S T U V W X Y Z A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z a b c d e f g h i j k l m n o p q r s t u v w x y z a b c d e f g h i j k l m n o p q r s t u v w x y z fatal error all goroutines are asleep deadlock goroutine 1 semacquire syncruntimeSemacquire0xc00001a0a8 usrlocalgosrcruntimesemago56 0x45 syncWaitGroupWait0xc00001a0a0 usrlocalgosrcsyncwaitgroupgo130 0x65 mainmain Usersbillcodegosrcgithubcomardanlabsgotrainingtopicsgo concurrencygoroutinesexample1example1go42 0x145 exit status 2 I can see how the Go Runtime identifies the program is deadlocked on line 42 where the call to Wait is happening I shouldnt get too excited about deadlock detection since every single Goroutine needs to be blocked with no way out This shows why keeping the Add and Done call together is so important 124 Listing 610 func main var wg syncWaitGroup wgAdd1 CHANGED Number Too Small go func lowercase wgDone go func uppercase wgDone What happens if I dont give the WaitGroup the correct number of Goroutines to wait on If the number is too large I will have another deadlock If the number
is too small there are no guarantees that the work is done before the program moves on The output of the program is undefined
63 Preemptive Scheduler Even though the scheduler runs within the scope of the application its important to see how the schedule is preemptive This means I cant predict when a context switch will take place and this will change every time I run the program Listing 611 func main  var wg syncWaitGroup wgAdd2 go func  printHashesA wgDone  go func  printHashesB wgDone  fmtPrintlnWaiting To Finish wgWait   fmtPrintlnnTerminating Program  Using the same orchestration pattern as before this program has each Goroutine doing a lot more work Work that the scheduler wont give a Goroutine enough time to finish completely in one time slice  125  Listing 612 func printHashesprefix string  for i  1 i  50000 i  num  strconvItoai sum  sha1Sumbytenum fmtPrintfs 05d xn prefix i sum  fmtPrintlnCompleted prefix   This function is performing a lot of IO bound work that has the potential of being context switched Listing 613  example2  cut c1  grep AB  uniq B A B A B A B A B A 9 Context Switches  example2  cut c1  grep AB  uniq B A B A 3 Context Switches  As I can see every time I run the program there are a different number of context switches This is a great thing because a scheduler shouldnt be predictable Concurrency needs to remain undefined and I must remember that when I use concurrency to solve my performance problems Listing 614 func init  runtimeGOMAXPROCS2   What happens if I go back to the original program but change GOMAXPROCS so the program runs as a two threaded Go program  126  Listing 615 Output Start Goroutines Waiting To Finish A B C D E F G H I J K L M N O P Q R S T U V W X Y Z A B C D E F G H I J K L M N a b c d e f g h i j k l m n o O P Q R S T U V W X Y Z A B C D E F G H I J K L M N O P Q R S T U V W X Y Z p q r s t u v w x y z a b c d e f g h i j k l m n o p q r s t u v w x y z a b c d e f g h i j k l m n o p q r s t u v w x y z Terminating Program  What I see is that the concurrency of the program is now more fine grained The output to the letter is undefined and out of order  
64 Data Races A data race is when two or more Goroutines are trying to access the same memory location at the same time where at least one Goroutine is performing a write When this happens it is impossible to predict the result These types of bugs are difficult to find because they cause issues that always appear random These 8 minutes from Scott Meyers is great to listen to here CPU Caches and Why You Care 30093830 httpsyoutubeWDIkqP4JbkEt1809  
65 Data Race Example This is a great example of a data race and how they can be hidden for years and eventually show up at odd times and cause data corruption Listing 616 var counter int func main  const grs  2 var wg syncWaitGroup wgAddgrs for g  0 g  grs g  go func  for i  0 i  2 i  value  counter value counter  value  wgDone   wgWait fmtPrintlnCounter counter   127  This program creates two Goroutines that each access the same integer variable incrementing the variable twice The Goroutine performs a read modify and write operation against the shared state manually Listing 617 var counter int func main     go func  for i  0 i  2 i  value  counter value counter  value  wgDone       I can see the access to the shared state inside the for loop When I build and run this program I get the right answer of 4 each and every time Listing 618  example1 Final Counter 4  example1 Final Counter 4  example1 Final Counter 4  How is this working  128  Listing 619 G1 Shared State 0 G2 Read 0 Modify 1 Write 1 Shared State 1 Context Switch Read 1 Modify 2 Shared State 2  Write 2  Context  Switch Read 2 Modify 3 Write 3 Terminate  Shared State 3  Read 3 Modify 4 Shared State 4  Write 4  Terminate   The read modify and write operations are happening uninterrupted Just because I am getting the right answer doesnt mean there isnt a problem What happens if I add a log statement in the middle of the read modify and write operation Listing 620 var counter int func main     go func  for i  0 i  2 i  value  counter value logPrintlnlogging counter  value  wgDone     Add Logging Here      If I run this program I no longer get the same result of 4 now I get the answer of 2  129  Listing 621  example1 Final Counter 2  example1 Final Counter 2  example1 Final Counter 2  What is happening I am running into a data race bug that did exist before but wasnt happening The call to log is now causing the scheduler to make a context switch between the two Goroutines at a bad time Listing 622 G1 Shared State 0 G2 Read 0 Modify 1 Context Switch Read 0 Modify 1 Switch Write 1 Read 1 Modify 2 Context Switch  Context Shared State 1  Shared State 1  1  Write Read  1 Modify 2 Switch Write 2 Terminate  Context Shared State 2 Shared State 2  Write  2 Terminate   After the modify operation a context switch is taking place The three operations are no longer uninterrupted and Goroutine 2 ends up with its local value being wrong by the time it completes the write operation I am very lucky this is happening every time and I can see it But normally a data race like this happens randomly and is impossible to know about until its too late Luckily Go has a race detector to help find data races  
66 Race Detection There are several ways to engage the race detector I can use it with the run build 130  and test command If I use it with the build command I have to remember to run the program They say an instrumented binary can slow my program down by 20 Listing 623  go build race  example1  The race flag is how to instrument the build with the race detector I will probably use it more with go test but for this example I am instrumenting the binary and then running it Listing 624 20210201 173052 logging 20210201 173052 logging 20210201 173052 logging  WARNING DATA RACE Write at 0x000001278d88 by goroutine 8 mainmainfunc1 dataraceexample1example1go41 0xa6 Previous read at 0x000001278d88 by goroutine 7 mainmainfunc1 dataraceexample1example1go38 0x4a Goroutine 8 running created at mainmain dataraceexample1example1go36 0xaf Goroutine 7 finished created at mainmain dataraceexample1example1go36 0xaf  20210201 173052 logging Final Counter 2 Found 1 data races  I can see a race was detected when running the program This would happen with or without the log statement inserted When a race is detected the program panics and provides this trace The trace shows where there was unsynchronized access to the same shared state where at least one access was a write In this trace a Goroutine performed a write at address 0x000001278d88 on line 41 and there was an unsynchronized read at the same address by another Goroutine on line 38 Both Goroutines were created on line 36  131  Listing 625 36 go func  37 for i  0 i  2 i  38 value  counter 39 value 40 logPrintlnlogging 41 counter  value 42  43 wgDone 44   I can clearly see the unsynchronized read and write As a side note the plus plus operation on line 39 would also be a data race if the code was accessing the counter variable The plus plus operation is a read modify and write operation underneath and the operating system could easily context switch in the middle of that So how can I fix the code to make sure that I remove the data race There are two tools I can use atomic instructions and mutexes  
67 Atomics Atomics provide synchronization at the hardware level Because of this its limited to words and halfwords of data So theyre great for counters or fast switching mechanics The WaitGroup APIs use atomics What changes do I need to make to apply atomics to the code Listing 626 var counter int32   CHANGED  func main  const grs  2 var wg syncWaitGroup wgAddgrs for g  0 g  grs g  go func  for i  0 i  2 i  atomicAddInt32counter 1  wgDone     CHANGED  wgWait fmtPrintlnCounter counter   I only need to do a couple things First change the counter variable to be a precision based integer I can see that at the top of the code listing The atomic functions only work with precision based integers Second remove the manually read modify and write code for one call to atomicAddInt32 That one call handles  132  it all All of the functions associated with the atomic package take the address to the shared state to be synchronized Synchronization only happens at the address level So different Goroutines calling the same function but at a different address wont be synchronized The API for atomics looks like this Listing 627 func AddInt32addr int32 delta int32 new int32 func AddInt64addr int64 delta int64 new int64 func AddUint32addr uint32 delta uint32 new uint32 func AddUint64addr uint64 delta uint64 new uint64 func AddUintptraddr uintptr delta uintptr new uintptr func CompareAndSwapInt32addr int32 old new int32 swapped bool func CompareAndSwapInt64addr int64 old new int64 swapped bool func CompareAndSwapPointeraddr unsafePointer old new unsafePointer swapped bool func CompareAndSwapUint32addr uint32 old new uint32 swapped bool func CompareAndSwapUint64addr uint64 old new uint64 swapped bool func CompareAndSwapUintptraddr uintptr old new uintptr swapped bool func LoadInt32addr int32 val int32 func LoadInt64addr int64 val int64 func LoadPointeraddr unsafePointer val unsafePointer func LoadUint32addr uint32 val uint32 func LoadUint64addr uint64 val uint64 func LoadUintptraddr uintptr val uintptr func StoreInt32addr int32 val int32 func StoreInt64addr int64 val int64 func StorePointeraddr unsafePointer val unsafePointer func StoreUint32addr uint32 val uint32 func StoreUint64addr uint64 val uint64 func StoreUintptraddr uintptr val uintptr func SwapInt32addr int32 new int32 old int32 func SwapInt64addr int64 new int64 old int64 func SwapPointeraddr unsafePointer new unsafePointer old unsafePointer func SwapUint32addr uint32 new uint32 old uint32 func SwapUint64addr uint64 new uint64 old uint64 func SwapUintptraddr uintptr new uintptr old uintptr type Value func v Value Load x interface func v Value Storex interface  I can see that the first parameter is always the address to a precision based integer or pointer There is also a type named Value that provides a synchronous value with a small API  
68 Mutexes What if I wanted to keep the three lines of code I had Then atomics arent going to 133  work What I need then is a mutex A mutex lets me box a group of code so only one Goroutine at a time can execute that code Listing 628 var counter int func main  const grs  2 var wg syncWaitGroup wgAddgrs var mu syncMutex for g  0 g  grs g  go func  for i  0 i  2 i  muLock  value  counter value counter  value  muUnlock  wgDone       CHANGED   CHANGED   CHANGED  wgWait fmtPrintlnCounter counter  There are several changes to this code from the original I added the construction of the mu variable to be a mutex set to its zero value Then inside the for loop I added calls to Lock and Unlock with an artificial code block Inside the code block I have the code that needs to be synchronized The code block is used for readability With this code in place the scheduler will only allow one Goroutine to enter the code block at a time Its important to understand that a mutex is not a queue The first Goroutine that calls Lock isnt necessarily the first Goroutine who gets the Lock There is a fairness based algorithm but this is done on purpose so people dont use mutexes as queues Its important to remember the Lock creates back pressure so the longer it takes to get from the Lock to the Unlock the more chance of Goroutines waiting for their turn If I forget to call Unlock then all Goroutines waiting will deadlock This is why its critical that the call to Lock and Unlock happen in the same function Make sure Im doing the bare minimum synchronization I need in the code block but at least the minimum This is very bad code where someone is trying to get in and out of the Lock so quickly they actually lose the synchronization and the race detector cant even 134  discover the problem Listing 629 var counter int func main  const grs  2 var wg syncWaitGroup wgAddgrs var mu syncMutex for g  0 g  grs g  go func  for i  0 i  2 i  var value int muLock  value  counter  muUnlock   Bad Use Of Mutex  value muLock  counter  value  muUnlock   Bad Use Of Mutex   wgDone       wgWait fmtPrintlnCounter counter  As a general guideline if I see a call to Lock from the same mutex twice in the same function stop the code review There is probably a mistake or over complication In this case the calls to read and write are being synchronized however two Goroutines can end up at the value line of code with the same value The data race still exists and the race detector is helpless in finding it  
69 ReadWrite Mutexes There is a second type of mutex called a readwrite mutex It allows me to separate the locks around reads and writes This is important since reading data doesnt pose a threat unless a Goroutine is attempting to write at the same time So this type of mutex allows multiple Goroutines to read the same memory at the same time As soon as a write lock is requested the reads are no longer issued the write takes place the reads can start again  135  Listing 630 package main import  fmt mathrand sync time  var data string var rwMutex syncRWMutex func main  var wg syncWaitGroup wgAdd1 go func  for i  0 i  10 i  writeri  wgDone  for i  0 i  8 i  go funcid int  for  readerid  i  wgWait fmtPrintlnProgram Complete  func writeri int  rwMutexLock  timeSleeptimeDurationrandIntn100  timeMillisecond fmtPrintln  Performing Write data  appenddata fmtSprintfString d i  rwMutexUnlock  func readerid int  rwMutexRLock  timeSleeptimeDurationrandIntn10  timeMillisecond fmtPrintfd  Performing Read  Lengthdn id lendata  rwMutexRUnlock   I can see the use of a readwrite mutex where there are 8 Goroutines reading the length of a slice within a 10 millisecond delay of each other and 1 Goroutine waking up within 100 milliseconds to append a value write to the slice The key is the implementation of the writer and reader functions Notice how I use Lock for the writer and RLock for the reader One of the biggest mistakes I can 136  make with this is mixing up the Unlock calls with the wrong version Having a Lock with a RUnlock will never end well Listing 631 7  Performing Read  Length0 5  Performing Read  Length0 0  Performing Read  Length0 3  Performing Read  Length0 7  Performing Read  Length0 2  Performing Read  Length0 1  Performing Read  Length0   Performing Write 0  Performing Read  Length1 5  Performing Read  Length1 3  Performing Read  Length1 6  Performing Read  Length1 7  Performing Read  Length1 4  Performing Read  Length1 1  Performing Read  Length1 2  Performing Read  Length1   Performing Write 7  Performing Read  Length2 1  Performing Read  Length2 3  Performing Read  Length2  The output shows how multiple Goroutines are reading at the same time but all the reading stops when the write takes place  
610 Channel Semantics Its important to think of a channel not as a data structure but as a mechanic for signaling This goes in line with the idea that I send and receive from a channel not read and write If the problem in front of me cant be solved with signaling if the word signaling is not coming out of my mouth I need to question the use of channels There are three things that I need to focus on when thinking about signaling The first one is does the Goroutine that is sending the signal need a guarantee that the signal has been received I might think that the answer to this question is always yes but remember there is a cost to every decision and there is a cost to having a guarantee at the signaling level The cost of having the guarantee at the signaling level is unknown latency The sender wont know how long they need to wait for the receiver to accept the signal Having to wait for the receiver creates blocking latency In this case unknown amounts of blocking latency The sender has to wait for an unknown amount of time until the receiver becomes available to receive the signal Waiting for the receiver means mechanically the receive operation happens before the send With channels the receive happens nanoseconds before but its before This means the receiver takes the signal and then walks away allowing the sender 137 to now move on with
a guarantee What if the process cant wait for an unknown amount of time What if that kind of latency wont work Then the guarantee cant be at the signaling level it needs to be outside of it The mechanics behind this working is that the send now happens before the receive The sender can perform the signal without needing the receiver to be available So the sender gets to walk away and not wait Eventually I hope the receiver shows up and takes the signal This is reducing latency cost on the send but its creating uncertainty about signals being received and therefore knowing if there are problems upstream with receivers This can create the process to accept work that never gets started or finished It could eventually cause massive back pressure and systems to crash The second thing to focus on is do I need to send data with the signal If the signal requires the transmission of data then the signaling is a 1 to 1 between Goroutines If a new Goroutine needs to receive the signal as well a second signal must be sent If data doesnt need to be transmitted with the signal then the signal can be a 1 to 1 or 1 to many between Goroutines Signaling without data is primarily used for cancellation or shutdowns Its done by closing the channel The third thing to focus on is channel state A channel can be in 1 of 3 states A channel can
be in a nil state by constructing the channel to its zero value state Sends and receives against channels in this state will block This is good for situations where I want to implement short term stoppages of work A channel can be in an open state by using the builtin function make Sends and receives against channels in this state will work under the following conditions Unbuffered Channels Guarantees at the signaling level with the receive happening before send Sending and receiving Goroutines need to come together in the same space and time for a signal to be processed Buffered Channels Guarantees outside of the signaling level with the send happening before the receive If the buffer is not full sends can complete else they block If the buffer is not empty receives can complete else they block A channel can be in a closed state by using the builtin function close I dont need to close a channel to release memory this is for changing the state Sending on a 138 closed channel will cause a panic however receiving on a closed channel will return immediately With all this information I can focus on channel patterns The focus on signaling is important The idea is if I need a guarantee at the signaling level or not based on latency concerns If I need to transmit data with the signal or not based on handling cancellations or not I want to convert the syntax to these semantics
611 Channel Patterns There are 7 channel patterns that are important to understand since they provide the building blocks to signaling  
6111 Wait For Result The wait for result pattern is a foundational pattern used by larger patterns like fan outin In this pattern a Goroutine is created to perform some known work and signals their result back to the Goroutine that created them This allows for the actual work to be placed on a Goroutine that can be terminated or walked away from Listing 632 func waitForResult  ch  makechan string go func  timeSleeptimeDurationrandIntn500  timeMillisecond ch  data fmtPrintlnchild  sent signal  d  ch fmtPrintlnparent  recvd signal  d timeSleeptimeSecond fmtPrintln   The beginning of this function uses the builtin function make In this case an unbuffered channel is being constructed to its open state Its better to look at this as a channel that is being constructed to signal string data with guarantees at the signaling level Which means the sending Goroutine wants a guarantee that the signal being sent has been received Once the channel is constructed a child Goroutine is created to perform work and the parent Goroutine waits to receive a signal with string data Because there are guarantees at the signaling level the amount of time the parent Goroutine will need to wait is unknown Its the unknown latency cost of this type of channel The child Goroutine goes ahead and begins to perform its work immediately To  139  simulate the unknown latency problem a sleep with a random number of milliseconds is employed to define the work Once the work is done the child Goroutine performs a send with string data The parent Goroutine is already blocked waiting in a receive Since the receive happens nanoseconds before the send which creates the guarantee I would think the print call for the receive signal would always appear before the print for the send But there is no guarantee in what order I will see the print calls execute I need to remember both Goroutines are running on their own operating system thread in parallel the receive is only happening nanoseconds before after the channel operation all things are equal again  
6112 Fan OutIn The fan outin pattern uses the wait for result pattern just described Listing 633 func fanOut  children  2000 ch  makechan string children for c  0 c  children c  go funcchild int  timeSleeptimeDurationrandIntn200  timeMillisecond ch  data fmtPrintlnchild  sent signal  child c  for children  0  d  ch childrenfmtPrintlnd fmtPrintlnparent  recvd signal  children     timeSleeptimeSecond fmtPrintln  The idea of this pattern is to create a Goroutine for each individual piece of work that is pending and can be done concurrently In this code sample I am going to create 2000 child Goroutines to perform 2000 individual pieces of work I am going to use a buffered channel since there is only one receiver and its not important to have a guarantee at the signaling level That will only create extra latency Instead the idea is to move the guarantee to know when all the signals have been received This will reduce the cost of latency from the channels That will be done with a counter that is decremented for each received signal until it reaches zero A buffered channel of 2000 is constructed one for each child Goroutine being created Then in a loop 2000 child Goroutines are created and they are off to do 140  their work A random sleep is used to simulate the work and the unknown amount of time it takes to get the work done The key is that the order of the work is undefined out of order execution which also changes each time the program runs If this is not acceptable I cant use concurrency Once all the Goroutines are created the parent Goroutine waits in a receive loop Eventually as data is signaled into the buffered channel the parent Goroutine will pick up the data and eventually all the work is received I must remember a fan out is dangerous in a running service since the number of child Goroutines I create for the fan are a multiplier If I have a service handling 50k requests on 50 thousand Goroutines and I decide to use a fan out pattern of 10 child Goroutines for some of the requests in a worse case scenario I would be talking 500k Goroutines existing at the same time Depending on the resources those child Goroutines needed I might not have them available at that scale and the back pressure could bring the service down  
6113 Wait For Task The wait for task pattern is a foundational pattern used by larger patterns like pooling Listing 634 func waitForTask  ch  makechan string go func  d  ch fmtPrintlnchild  recvd signal  d  timeSleeptimeDurationrandIntn500  timeMillisecond ch  data fmtPrintlnparent  sent signal timeSleeptimeSecond fmtPrintln   At the beginning the function creates an unbuffered channel so there is a guarantee at the signaling level This is critically important for pooling so I can add mechanics later if needed to allow for timeouts and cancellation Once the channel is created a child Goroutine is created immediately waiting for a signal with data to perform work The parent Goroutine begins to prepare that work and finally signals the work to the child Goroutine Since the guarantee is at the signaling level the child Goroutine doesnt know how long it needs to wait  
6114 Pooling The pooling pattern uses the wait for task pattern just described The pooling pattern allows me to manage resource usage across a well defined number of 141 Goroutines As explained previously in Go pooling is not needed for efficiency in CPU processing like at the operating system Its more important for efficiency in resource usage Listing 635 func pooling ch makechan string g runtimeGOMAXPROCS0 for c 0 c g c go funcchild int for d range ch fmtPrintfchild d recvd signal sn child d fmtPrintfchild d recvd shutdown signaln child c const work 100 for w 0 w work w ch data fmtPrintlnparent sent signal w closech fmtPrintlnparent sent shutdown signal timeSleeptimeSecond fmtPrintln In this pattern a group of child Goroutines are created to service the same channel There is efficiency in this because the size of the pool dictates the amount of concurrent work happening at the same time If I have a pool of 16 Goroutines that could represent 16 files being opened at any given time or the amount of memory needed for 16 Goroutines to perform their work The code starts with the creation of an unbuffered channel Its critically important that an unbuffered channel is used because without the guarantee at the signaling level I cant perform timeouts and cancellation on the send if needed at a later time The next part of the code decides the number of child Goroutines the pool will contain Listing 636 g runtimeGOMAXPROCS0 The call to runtimeGOMAXPROCS
is important in that it queries the runtime when passing 0 as a parameter to the number of threads that exist for running Goroutines The number should always equal the number of coreshardwarethreads that are available to the program It represents the amount of CPU capacity available to the program When the size of the pool isnt obvious start with this number as a baseline It wont be uncommon for this 142 number to provide a reasonable performance benchmark The for loop creates the pool of child Goroutines where each child Goroutine sits in a blocking receive call using the forrange mechanics for a channel Listing 637 for c 0 c g c go funcchild int for d range ch fmtPrintfchild d recvd signal sn child d fmtPrintfchild d recvd shutdown signaln child c The for range helps to minimize the amount of code I would otherwise need to receive a signal and then shutdown once the channel is closed Without the forrange mechanics I would have to write this code Listing 638 for c 0 c g c go func child int for d wd ch CHANGED if wd CHANGED break CHANGED fmtPrintfchild d recvd signal sn child d fmtPrintfchild d recvd shutdown signaln child c The forrange eliminates 4 extra lines of code and streamlines the mechanics Its important to note it must not matter which of the child Goroutines in the pool are chosen to receive a signal Depending on the amount of work being signaled it
could be the same child Goroutines over and over while others are never selected Then the call to close is executed which will cause the for loops to terminate and stop the program If the channel being used was a buffered channel data would flush out of the buffer first before the child Goroutines would receive the close signal
6115 Drop The drop pattern is an important pattern for services that may experience heavy loads at times and can drop requests when the service reaches a capacity of pending requests As an example a DNS service would need to employ this pattern  143  Listing 639 func drop  const cap  100 ch  makechan string cap go func  for p  range ch  fmtPrintlnchild  recvd signal  p   const work  2000 for w  0 w  work w  select  case ch  data fmtPrintlnparent  sent signal  w default fmtPrintlnparent  dropped data  w   closech fmtPrintlnparent  sent shutdown signal timeSleeptimeSecond fmtPrintln   The code starts with the creation of a buffered channel This is a case where its reasonable to have a large buffer Identifying the capacity value buffer size will require work in the lab I want a number that allows the service to maintain reasonable levels of resource usage and performance when the buffer is full Next a child Goroutine using the pooling pattern is created This child Goroutine is waiting for a signal to receive data to work on In this example having only one child Goroutine will cause back pressure quickly on the sending side One child Goroutine will not be able to process all the work in time before the buffer gets full Representing the service is at capacity Inside the for loop I see the use of a select statement The select statement is a blocking call that allows the parent Goroutine to handle multiple channel operations at the same time Each case represents a channel operation a send or a receive However this select is using the default keyword as well which turns the select into a nonblocking call The key to implementing this pattern is the use of default If the channel buffer is full that will cause the case statement to block since the send cant complete When every case in a select is blocked and there is a default the default is then executed This is where the drop code is placed In the drop code I can now decide what to do with the request I can return a 500 to the caller I could store the request somewhere else The key is I have options 144  
6116 Cancellation The cancellation pattern is used to tell a function performing some IO how long I am willing to wait for the operation to complete Sometimes I can cancel the operation and sometimes all I can do is just walk away Listing 640 func cancellation  duration  150  timeMillisecond ctx cancel  contextWithTimeoutcontextBackground duration defer cancel ch  makechan string 1 go func  timeSleeptimeDurationrandIntn200  timeMillisecond ch  data  select  case d  ch fmtPrintlnwork complete d case ctxDone fmtPrintlnwork cancelled     timeSleeptimeSecond fmtPrintln  The code starts with defining a timeDuration variable named duration set to 150 milliseconds Then a Context value is created to support a timeout of the 150 seconds using the WithTimeout function That function takes a Context value in and returns a new one with the changes In this case I use the Background function which returns an empty parent Context Its important to call the cancel function that is returned as the second argument from WithTimeout using a defer If that cancel function is not called at least once there will be a memory leak After a buffered channel of 1 is created a child Goroutine is created to perform some IO bound work In this case a random Sleep call is made to simulate blocking work that cant be directly cancelled That work can take up to 200 milliseconds to finish There is a 50 millisecond difference between the timeout and the amount of time the work could take With the child Goroutine created and performing the work the parent Goroutine blocks in a select statement waiting on two signals The first case represents the child Goroutine finishing the work on time and the result being received That is what I want The second case represents a timeout from the Context This means the work didnt finish within the 150 millisecond time limit 145  If the parent Goroutine receives the timeout signal it walks away In this situation it cant inform the child Goroutine that it wont be around to receive its signal This is why its so important for the work channel to be a buffer of 1 The child Goroutine needs to be able to send its signal whether or not the parent Goroutine is around to receive it If a nonbuffered channel is used the child Goroutine will block forever and become a memory leak  
6117 Fan OutIn Semaphore The fan outin semaphore pattern provides a mechanic to control the number of Goroutines executing work at any given time while still creating a unique Goroutine for each piece of work Listing 641 func fanOutSem  children  2000 ch  makechan string children g  runtimeGOMAXPROCS0 sem  makechan bool g for c  0 c  children c  go funcchild int  sem  true  t  timeDurationrandIntn200  timeMillisecond timeSleept ch  data fmtPrintlnchild  sent signal  child  sem c  for children  0  d  ch childrenfmtPrintlnd fmtPrintlnparent  recvd signal  children  timeSleeptimeSecond fmtPrintln   At the start of the function a channel with a buffer size of 2000 is set This is the same thing we need in the original fan outin pattern One buffer for each child Goroutine that will be created Then like the pooling pattern the use of the GOMAXPROCS function is used to determine how many of the 2000 child Goroutines will be allowed to execute their work at any given time With g configured a second buffered channel is constructed next with a buffer sized to the number of child Goroutines that can execute their work at the same time This channel is the semaphore that will control the number of child Goroutines  146  performing work Then a for loop is used to create all 2000 child Goroutines and each child Goroutine finds itself in a send operation sem  true against the semaphore channel Here is where the rubber hits the road Only a GOMAXPROCS number of child Goroutines can perform this send without blocking The other 2000  GOMAXPROCS child Goroutines will block until the running child Goroutines get to the receive operation sem  This code uses a code block to show the code that is being executed  between the semaphore locking I like this for better readability At the end of the function the parent Goroutine waits to receive work from all 2000 child Goroutines For each piece of work received the children variable is decremented until it gets down to zero Just like the original fan outin pattern  
6118 Bounded Work Pooling The bounded work pooling pattern uses a pool of Goroutines to perform a fixed amount of known work Listing 642 func boundedWorkPooling  work  stringpaper paper paper paper 2000 paper g  runtimeGOMAXPROCS0 var wg syncWaitGroup wgAddg ch  makechan string g for c  0 c  g c  go funcchild int  defer wgDone for wrk  range ch  fmtPrintfchild d  recvd signal  sn child wrk  fmtPrintfchild d  recvd shutdown signaln child c  for  wrk  range work  ch  wrk  closech wgWait timeSleeptimeSecond fmtPrintln   Right from the start the function defines 2000 arbitrary pieces of work to perform Then the GOMAXPROCS function is used to define the number of child Goroutines to use in the pool and a WaitGroup is constructed to make sure the parent Goroutine can be told to wait until all 2000 pieces of work are completed  147  Just like I saw with the pooling pattern a pool of child Goroutines is created in the loop and they all wait on a receive call using the for range mechanics One change is the call to Done using a defer when each of the child Goroutines in the pool eventually terminate This will happen when all the work is completed and this is how the pool will report back to the parent Goroutine they are aware they are not needed any longer After the creation of the pool of child Goroutines A loop is executed by the parent Goroutine to start signaling work into the pool Once the last piece of work is signaled the channel is closed Each of the child Goroutines will receive the closed signal once the signals in the buffer are emptied  
6119 Retry Timeout The retry timeout pattern is great when I have to ping something like a database which might fail but I dont want to fail immediately I want to retry for a specified amount of time before I fail Listing 643 func retryTimeoutctx contextContext retryInterval timeDuration check funcctx contextContext error  for  fmtPrintlnperform user check call if err  checkctx err  nil  fmtPrintlnwork finished successfully return  fmtPrintlncheck if timeout has expired if ctxErr  nil  fmtPrintlntime expired 1  ctxErr return  fmtPrintfwait s before trying againn retryInterval t  timeNewTimerretryInterval    select  case ctxDone fmtPrintlntimed expired 2  ctxErr tStop return case tC fmtPrintlnretry again     The function takes a context for the amount of time the function should attempt to perform work unsuccessfully It also takes a retry interval that specifies how long to wait between attempts and finally a function to execute This function is coded by the caller for the specific work like pinging the database that needs to be performed and could fail 148  The core of the function runs in an endless loop The first step in the loop is to run the check function passing in the context so the callers function can also respect the context If that doesnt fail the function returns that life is good If it fails the code goes on to the next step Next the context is checked to see if the amount of time given has expired If it has the function returns the timeout error else it continues to the next step which is to create a timer value The time value is set to the retry interval The timer could be created above the for loop and reused which would be good if this function was going to be running a lot To simplify the code a new timer is created every time The last step is to block on a select statement waiting to receive one of two signals The first signal is that the context expires The second signal is the retry interval expires In the case of the second signal the loop is restarted and the process runs again  
61110 Channel Cancellation With channel cancellation I can take an existing channel being used already for cancellation purposes legacy code and convert its use with a context where a context is needed for a future function call Listing 644 func channelCancellationstop chan struct  ctx cancel  contextWithCancelcontextBackground defer cancel go func  select  case stop cancel case ctxDone   funcctx contextContext error  req err  httpNewRequestWithContext ctx httpMethodGet httpswwwardanlabscomblogindexxml nil  if err  nil  return err      err  httpDefaultClientDoreq if err  nil  return err  return nil ctx  This function accepts a channel typed with the empty struct to signal cancellation 149  This is code that could be found in Go programs prior to the inclusion of context A function this function needs to call works with the new context package A context is created using the Background function for the parent context in the WithCancel call This returns a new context value that can be cancelled with the returned cancel function The key is the creation of the Goroutine that blocks in a select statement waiting on two signals The first signal is the legacy channel that may be closed by the originator The second is the context itself which is important if future functions decide to cancel the context directly On receiving a stop signal the cancel function is then executed cancelling the context for all functions that were passed the context As an example a literal function is declared and executed that performs a web request that supports a context for cancellable IO  150  
Chapter 7 Testing In this chapter I will learn how to write tests in Go and the integrated support provided by the language  
71 Basic Unit Test One of the best things about Go is that the language defines what a unit of code is A unit of code is called a package and its represented as a folder in my source tree The compiler builds a static library from each folder and then links them all together to form the final application binary When I talk about a unit test I am talking about testing code from a single package Usually exported function by exported function This doesnt restrict me from hitting external systems like a database or a set of web services to perform the tests This is different from an integration test which will run tests across multiple packages There are no rules or idioms when writing a unit test Whats important is that a company or at least a team develops a set of consistent idioms and rules for writing a test This way anyone can review a test result and maintain the code in a consistent way One thing I need to decide upon is how verbose I want the tests to be Do I want a test that only provides output when something fails or do I want an indication as well that things are passing The standard library and the Go frontend tooling has everything I need to write a test It all starts with creating a file with the testgo naming convention inside the package I want to test and then adding test functions by
using the word Test with a capital T for each function Listing 71 sampletestgo package sampletest import testing func TestDownloadt testingT func TestUploadt testingT These are examples of test functions I could declare in the sampletestgo testing file Its important that the first letter following the word Test in the function name starts with a capital letter If I dont the testing tool wont see the function as a test 151 function The other important piece is that the test function takes a testingT pointer as the only argument I should notice the package name also has the test naming convention which puts the test functions in a different package and requires the test to import the package being tested and only work with the exported API This is a good practice to force the test to use the package API like any other user If I need to test unexported functions I cant use this convention Listing 72 type T func c T Cleanupf func func t T Deadline deadline timeTime ok bool func c T Errorargs interface func c T Errorfformat string args interface func c T Fail func c T FailNow func c T Failed bool func c T Fatalargs interface func c T Fatalfformat string args interface func c T Helper func c T Logargs interface func c T Logfformat string args interface func c T Name string func t T Parallel func t T Runname string f funct T bool func c T Skipargs interface
func c T SkipNow func c T Skipfformat string args interface func c T Skipped bool func c T TempDir string The testingT value provides the API to share with the testing tool if the test passed or failed Two families of functions exist tFatal and tError which are the most common APIs I will use and both indicate if a test failed The big difference is calling tFatal will cause the test function to return and calling tError will allow the test function to continue and report potentially more failures within the test function The tLog function provides the option for verbose output for those who want to share more when a test fails or when the v option is used when running a test 152 Listing 73 package sampletest import testing http func TestDownloadt testingT url httpswwwardanlabscomblogindexxml statusCode 200 resp err httpGeturl if err nil tFatalfunable to issue GET on URL s s url err defer respBodyClose if respStatusCode statusCode tLogexp statusCode tLoggot respStatusCode tFatalstatus codes dont match In this test I plan to test the ability to download the RSS feed for the Ardan Labs blog First I declare some variables with the URL to download and the expected status code I then perform the httpGet call for the URL and check the error Its critical that any function called that returns an error has the error value checked If there is an error I call tFatal to report the problem and the test fails If there
is no error I proceed to the next step which is checking the status code Before checking the status code I prepare the close call for the response body Always write tests like production code If I dont get back the status code I expect I provide verbose information about what I expected and what I got This really helps with debugging failed tests Then I call tFatal to report the test has failed This is a test that only outputs information if the test fails
72 Table Unit Test Table tests can be powerful when I have code that can be run through a series of different inputs and expected outputs They are perfect for negative path testing  153  Listing 74 package sampletest import  testing  http   func TestDownloadt testingT  tt  struct  url string statusCode int  httpswwwardanlabscomblogindexxml httpStatusOK httprsscnncomrsscnntopstorierss httpStatusNotFound  for  test  range tt  resp err  httpGettesturl if err  nil  tFatalfunable to issue GET on URL s s testurl err  defer respBodyClose    if respStatusCode  teststatusCode  tLogexp teststatusCode tLoggot respStatusCode tFatalstatus codes dont match     In this test I am checking different URLs and potential status codes The table is a slice of a literal struct that has two fields url to test and the expected status code Then two entries are added to the table the first one is the positive path test the second is a negative path test for a bad URL The test code is similar to the first test except the input is coming from iterating over the table This allows me to add more entries over time without the need to write more code The table can be anything from a slice to a map or any data structure I can iterate over and provide input and expected output  
73 Web Call Mocking I am not a fan of mocking when it can be avoided The use of Docker provides lots of opportunities to hit real systems over mocked behavior I need to remember a mock is only as good as the behavior being simulated If the behavior changes in an upgrade of a system the mock is no longer representative of the correct behavior Mocks can be useful for negative path testing when its difficult to cause real systems to fail on demand When it comes to making web calls to external systems the standard library already provides support for mocking web calls This is great for testing API behavior without the cost of a real web call Especially if the testing environment is 154 on a private network with no access to the outside world Listing 75 package sampletest import testing http httptest var feed xml version10 encodingUTF8 rss channel titleGoing Go Programmingtitle descriptionGolang httpsgithubcomgoinggodescription linkhttpwwwgoinggonetlink item pubDateSun 15 Mar 2015 150400 0000pubDate titleObject Oriented Programming Mechanicstitle descriptionGo is an object oriented languagedescription linkhttpwwwgoinggonet201503objectorientedlink item channel rss func mockServer httptestServer f funcw httpResponseWriter r httpRequest wWriteHeader200 wHeaderSetContentType applicationxml fmtFprintlnw feed return httptestNewServerhttpHandlerFuncf Before I can write the test function I need a way to mock a web server The mockServer function is providing just that It uses the NewServer function from the httptest package This function takes an http handler function that can be executed when a request is sent to the mock server and returns the
localhost port the mock server is listening on In this case the handler function sets the status code to 200 the content type to applicationxmls and writes the feed string back to the caller as part of the response The feed string is mocking a small portion of the RSS document that is returned from the Ardan Labs blog call from the first test 155 Listing 76 func TestDownloadt testingT statusCode 200 server mockServer defer serverClose resp err httpGetserverURL if err nil tFatalfunable to issue GET on the URL s s serverURL err defer respBodyClose if respStatusCode statusCode tLogexp statusCode tLoggot respStatusCode tFatalstatus codes dont match In this test the call to mockServer is performed to start the mock server and the server value is captured The server value contains the URL to access the mock server and that is used in the call to httpGet Since the mock server always returns a proper RSS feed document this test should always pass I can extend this test function by declaring concrete types that match the RSS feed document These types can be used to validate the actual data from the response Listing 77 type Item struct XMLName xmlName xmlitem Title string xmltitle Description string xmldescription Link string xmllink Channel defines the fields associated with the channel tag in the buoy RSS document type Channel struct XMLName xmlName xmlchannel Title string xmltitle Description string xmldescription Link string xmllink PubDate string xmlpubDate Items Item xmlitem Document defines the fields associated with the
buoy RSS document type Document struct XMLName xmlName xmlrss Channel Channel xmlchannel URI string With these types declared I can extend the test function 156 Listing 78 func TestDownloadt testingT statusCode 200 server mockServer defer serverClose resp err httpGetserverURL if err nil tFatalfunable to issue GET on the URL s s serverURL err defer respBodyClose if respStatusCode statusCode tLogexp statusCode tLoggot respStatusCode tFatalstatus codes dont match var d Document if err xmlNewDecoderrespBodyDecoded err nil tFatalunable to decode the response err if lendChannelItems 1 tFatalnot seeing 1 item in the feed len lendChannelItems Now the test is extended to decode the response body into the concrete types and then it validates there is one news item in the feed Obviously I can do more now that the feed is decoded and I control the feed document
74 Internal Web Endpoints If Im building a web service I will want to test the endpoints without the need to build and run the service The goal is to load the router with the endpoints and execute those endpoints without the need of all the networking stuff The standard library provides support for this To see this I need to define a handler function and bind the function to a route using the default mux in the http package Its important to understand this will work with any mux 157 Listing 79 package handlers import encodingjson nethttp func Routes httpHandleFuncsendjson sendJSON func sendJSONrw httpResponseWriter r httpRequest u struct Name string Email string Name Bill Email billardanlabscom rwHeaderSetContentType applicationjson rwWriteHeader200 jsonNewEncoderrwEncodeu This code declares a new package named handlers and a function named sendJSON that represents a handler function that needs to be tested In this case the handler function returns a JSON representation of the literal struct with a status of 200 The Routes function binds the sendJSON handler to the sendjson route in the default server mux of the http package This means any calls to httpdomaincomsendjson would be routed to the sendJSON handler function by the mux 158 Listing 710 package handlerstest import encodingjson nethttp nethttphttptest testing githubcomardanlabsgotrainingapphandlers func init handlersRoutes func TestSendJSONt testingT url sendjson statusCode 200 r httptestNewRequestGET url nil w httptestNewRecorder httpDefaultServeMuxServeHTTPw r The first thing the test does is use the init function to make sure all the routes are loaded in the
mux This is done by calling the handlersRoutes function A big mistake that is made with these types of tests is forgetting to load the routes With the routes loaded the next step is to construct a request and construct a value that implements the httpResponseWriter interfaces The httptest package provides support for both The httptestNewRequest constructs the request as a GET call for the sendjson route The httptestNewRecorder constructs the concrete value that implements the httpResponseWriter interfaces and can be directly checked to validate if the web call passed or failed The key to running the test is calling the ServeHTTP function from the mux Since I am using the httpDefaultServer mux in this code that is where I am making the call to ServeHTTP Any mux I may choose to use will implement this method Once I call this method I am asking the mux to process the request through the route and apply the response in the concrete recorder value 159 Listing 711 func TestSendJSONt testingT if wCode 200 tLogexp statusCode tLoggot wStatusCode tFatalstatus codes dont match var u struct Name string Email string if err jsonNewDecoderwBodyDecodeu err nil tFatalunable to decode the response err exp Bill if uName exp tLogexp exp tLoggot uName tFataluser name does not match exp billardanlabscom if uEmail exp tLogexp exp tLoggot uEmail tFataluser name does not match Now using the recorder value I can check if the route behaved as expected First I check that the status code is 200 Then
I attempt to unmarshal the JSON response back to a concrete type If that is successful then I check the values in the Name and Email fields If all that is correct the test passes This test was executed without the need of a network or running the web service
75 Basic SubTests There are times when I want to group a set of tests together under a single test function I might have a table test where I want to control which data is tested on the command line Its also possible that I may want to run a series of tests in parallel If I fall into any of these use cases the subtesting support in Go is what I need  160  Listing 712 package sampletest import  nethttp testing  func TestDownloadt testingT  tt  struct  name string url string statusCode int   ok httpswwwardanlabscomblogindexxml httpStatusOK   notfound httprsscnncomrsscnntopstorierss httpStatusNotFound         This test is going to test the downloading of URLs like I saw with the table test from before The difference here is the table has a new field called name which will represent the name of the subtest I am going to create for each entry in the table Its important to give each subtest a name that is unique within a few starting characters so I can better filter the tests I want to run with the least number of characters to type Listing 713 for  test  range tt  test  test  LOOK HERE tf  funct testingT   LOOK HERE resp err  httpGettesturl if err  nil  tFatalfunable to issue GETURL s s testurl err  defer respBodyClose if respStatusCode  teststatusCode  tLogexp teststatusCode tLoggot respStatusCode tFatalstatus codes dont match   tRuntestname tf    LOOK HERE    There are a couple of changes to the code inside the for range loop from the original 161  table test example Look for the LOOK HERE comments The very first line of code inside the loop is creating a new variable named test that has local scope to the loop This is being done to prevent any closure bugs since the next line of code creates a literal test function I must make sure each literal test function has its own copy of the test data it will operate on Then a literal test function is declared and assigned to the tf variable This function is passed to the call to tRun at the bottom of the loop The call to tRun is how the subtest is registered by name Once all the test functions are registered they can be executed all together or specified individually on the command line Listing 714  go test v  go test run TestDownloadok v  go test run TestDownloadnotfound v  This is how I can run all the subtests or specify any individual subtest There is one other cool feature with subtests I can run all the tests in parallel by using the tParallel method Listing 715 for  test  range tt  test  test tf  funct testingT  tParallel     LOOK HERE      tRuntestname tf    By adding the call to tParallel as the first line of code in the literal test function the testing tool will run all the registered test functions in parallel speeding up test execution time  162  
Chapter 8 Benchmarking In this chapter I will learn how to write benchmarks in Go and use the integrated support provided by the language I will also learn how to realize that benchmarks lie and I need to be careful when interpreting the results  
81 Basic Benchmark Benchmarks lie At the same time until I run a benchmark Im only guessing If theres one thing about Go its that I never have to guess Thats how good the tooling is The standard library and the Go frontend tooling has everything I need to write a benchmark It all starts with creating a file with the testgo naming convention inside the package I want to benchmark and then adding benchmark functions by using the word Benchmark with a capital B for each function Listing 81 sampletestgo package sample import testing func BenchmarkDownloadb testingB func BenchmarkUploadb testingB These are examples of benchmark functions I could declare in the sampletestgo testing file Its important that the first letter following the word Benchmark in the function name starts with a capital letter If I dont the testing tool wont see the function as a benchmark function The other important piece is that the benchmark function takes a testingB pointer as the only argument Here is a super interesting benchmark to run 163 Listing 82 package basic import fmt testing var gs string func BenchmarkSprintb testingB var s string for i 0 i bN i s fmtSprinthello gs s func BenchmarkSprintfb testingB var s string for i 0 i bN i s fmtSprintfhello gs s The premise of this benchmark is to know which version of Sprint is faster the regular or format version When people are asked to guess they tend to say the regular version since there
is no formatting necessary These people are always wrong Listing 83 go test bench benchtime 3s benchmem goos darwin goarch amd64 pkg githubcomardanlabsgotrainingtopicsgotestingbenchmarksbasic cpu IntelR CoreTM i99980HK CPU 240GHz BenchmarkSprint16 BenchmarkSprintf16 56956252 80984947 5548 nsop 4246 nsop 5 Bop 5 Bop 1 allocsop 1 allocsop I can see from the results that the format version is faster by 13 nanoseconds per operation The difference is insignificant but nonetheless there is a difference I want to break down both the writing and running of these benchmarks 164 Listing 84 var gs string func BenchmarkSprintb testingB var s string for i 0 i bN i s fmtSprinthello gs s At the core of every benchmark is the for loop from 0 to bN Inside this loop is where the code to be benchmarked is placed To understand the loop I need to understand a setting called benchtime The benchtime setting represents the total amount of time to spin the loop before providing a result The default benchtime is 1 second This is where things get interesting because I cant spin a loop based on time only on a number of iterations The number of iterations required to match the benchtime needs to be identified Identifying the correct bN to match the benchtime is accomplished through some trial and error At the very beginning of running the benchmark the tooling will set the value of bN to 1 and run the loop Then it will multiply the value of bN by 100 until
it gets close to the benchtime Then the algorithm can fix on a working bN Listing 85 var gs string var a int func BenchmarkSprintb testingB var s string a appenda bN for i 0 i bN i s fmtSprinthello if lena 4 fmtPrintlna gs s I added the ability to capture the value of bN on every call to the benchmark function Knowing that in this case it will take 5 calls to find the right bN that is when I output the contents of the slice 165 Listing 86 BenchmarkSprint16 1 100 10000 1000000 54495168 I can see the tooling identifying a value of bN after 5 tries finding the right number after trying a million iterations Another important aspect of benchmarking is the compiler The compiler will build this code into a test binary to run the benchmark Its important that the code inside the loop is accurate to how it runs in production Any slight variation could change the compilers behavior on how the code is built or how the code behaves at runtime Back to the original code for the Sprint benchmark Listing 87 var gs string func BenchmarkSprintb testingB var s string for i 0 i bN i s fmtSprinthello gs s This benchmark is measuring the performance of calling Sprint Notice how the Sprint function returns a new string after the call Its important to capture that return value as part of running the benchmark because doing so represents the real behavior of
using this function in production code If I dont capture the return value from the call to Sprint its possible that the compiler could throw the function call out of the compiled binary The function is useless without capturing the return since the behavior of the call alone wouldnt change the behavior of the program If the compiler chooses to throw the call to Sprint away the benchmark would just spin an empty loop providing an inaccurate result When I run the benchmark I am using the following call on the command line Listing 88 go test bench benchtime 3s benchmem Breaking this down I am asking go test to run all the benchmark functions it finds in the current directory bench increasing the time of spinning the loop to three seconds benchtime 3s and to show memory allocations benchmem 166 Listing 89 BenchmarkSprint16 BenchmarkSprintf16 56956252 80984947 5548 nsop 4246 nsop 5 Bop 5 Bop 1 allocsop 1 allocsop BenchmarkSprint16 Name of the benchmark function and the number of threads which was 16 56956252 The number of iterations of the loop that were executed which was 56956252 5548 nsop The amount of time the code inside the loop took to execute which was 5548 nanoseconds 5 Bop The amount of memory the code inside the loop allocated which was 5 bytes 1 allocsop The number of values the code inside the loop allocated which was 1 value In the end the use of Sprintf was faster than Sprint for the
string hello though the both allocated the same number of values and total amount of memory on the heap
82 Basic SubBenchmarks There are times when I may want to group a set of benchmarks together under a single benchmark function I might have a data table that I want to use to control which data is being benchmarked on the command line If I fall into any of these use cases the subbenchmarking support in Go is what I need Listing 810 package basic import  fmt testing  var gs string func BenchmarkSprintb testingB  bRunnone benchSprint bRunformat benchSprintf   A quick way to show this is to rename the existing benchmark functions to benchSprint and benchSprintf then write a single benchmark function that uses the bRun function Now on the command line I can control what runs Listing 811  167   go test bench   go test bench BenchmarkSprintnone  go test bench BenchmarkSprintformat  This is how I can run all the subbenchmarks or specify any individual subbenchmark  
83 Validate Benchmarks When I started this chapter I tried to make it clear that benchmarks lie and I must validate the results Especially when the results are not what I expected To show this I have written a merge sort algorithm so I can run the algorithm under different conditions First with a single Goroutine then using a different Goroutine for every split of the collection that needs to be sorted finally only using the same number of Goroutines that can be run in parallel Listing 812 func mergel r int int Declare the sorted return list with the proper capacity ret makeint 0 lenllenr Compare the number of items required for switch case lenl 0 We appended everything in the left list so now append everything contained in the right and return return appendret r case lenr 0 We appended everything in the right list so now append everything contained in the left and return return appendret l case l0 r0 First value in the left list is smaller than the first value in the right so append the left value ret appendret l0 Slice that first value away l l1 default First value in the right list is smaller than the first value in the left so append the right value ret appendret r0 Slice that first value away r r1 Here is the merge function that performs the work All this slicing is going to create a large number of allocations on the heap Merge sort
is not a great algorithm to use in Go 168 Listing 813 func singlen int int Once we have a list of one we can begin to merge values if lenn 1 return n Split the list in half i lenn 2 Sort the left side l singleni Sort the right side r singleni Place things in order and merge ordered lists return mergel r The single function uses a single Goroutine to perform the sort It splits the initial list in half then uses recursion to continue to split the list until the merge function can be executed to sort everything Listing 814 func unlimitedn int int Once we have a list of one we can begin to merge values if lenn 1 return n Split the list in half i lenn 2 Maintain the ordered left and right side lists var l r int The unlimited function starts out the same as the single function Then it begins to throw new Goroutines at each split of the list 169 Listing 815 func unlimitedn int int For each split we will have 2 goroutines var wg syncWaitGroup wgAdd2 Sort the left side concurrently go func l unlimitedni wgDone Sort the right side concurrently go func r unlimitedni wgDone Wait for the splitting to end wgWait Place things in order and merge ordered lists return mergel r This could result in tens of thousands of Goroutines depending on the size of the list to be sorted Listing 816 func numCPUn
int int Once we have a list of one we can begin to merge values if lenn 1 return n Split the list in half i lenn 2 Maintain the ordered left and right side lists var l r int The numCPU function starts out like the unlimited function Then it must calculate how many concurrent splits can occur for the number of Goroutines that can run in parallel 170 Listing 817 func numCPUn int int Calculate how many levels deep we can create goroutines On an 8 core machine we can keep creating goroutines until level 4 Lvl 0 1 Lists 1 Goroutine Lvl 1 2 Lists 2 Goroutines Lvl 2 4 Lists 4 Goroutines Lvl 3 8 Lists 8 Goroutines Lvl 4 16 Lists 16 Goroutines On an 8 core machine this will produce the value of 3 maxLevel intmathLog2float64runtimeGOMAXPROCS0 Once the number of levels is calculated the sorting work can begin Listing 818 func numCPUn int lvl int int We dont need more goroutines then we have logical processors if lvl maxLevel lvl For each split we will have 2 goroutines var wg syncWaitGroup wgAdd2 Sort the left side concurrently go func l numCPUni lvl wgDone Sort the right side concurrently go func r numCPUni lvl wgDone Wait for the splitting to end wgWait Place things in order and merge ordered lists return mergel r Sort the left and right side on this goroutine l numCPUni lvl r numCPUni lvl Place things in order and merge ordered
lists return mergel r With this function complete I can write the benchmark functions and see what version of merge sort is faster 171 Listing 819 package main import math runtime sync testing var n int func init for i 0 i 1000000 i n appendn i func BenchmarkSingleb testingB for i 0 i bN i singlen func BenchmarkUnlimitedb testingB for i 0 i bN i unlimitedn func BenchmarkNumCPUb testingB for i 0 i bN i numCPUn 0 Everything I know suggests that numCPU should be the fastest since it is efficiently using the cpu capacity of the machine Listing 820 go test bench benchtime 3s goos darwin goarch amd64 pkg githubcomardanlabsgotrainingtopicsgotestingbenchmarksvalidate cpu IntelR CoreTM i99980HK CPU 240GHz BenchmarkSingle16 BenchmarkUnlimited16 BenchmarkNumCPU16 52 13 78 66837183 nsop 251840589 nsop 46336693 nsop The results of the benchmark show that my expectation was correct numCPU is faster and in this case by 362 But is this actually correct What happens if I just run the numCPU benchmark in isolation 172 Listing 821 go test bench NumCPU benchtime 3s goos darwin goarch amd64 pkg githubcomardanlabsgotrainingtopicsgotestingbenchmarksvalidate cpu IntelR CoreTM i99980HK CPU 240GHz BenchmarkNumCPU16 85 38004899 nsop When running the benchmark in isolation the performance of the numCPU function increased Now numCPU is 55 faster That is a huge improvement on such a small data set Imagine if the single test ended up beating numCPU when all the benchmarks were run together I would have thought single was faster than numCPU Funny enough this was the
case in earlier versions of Go Why the difference in performance Because the unlimited benchmark is creating a lot of Goroutines that are still being cleaned up by the time the numCPU benchmark starts running This extra load on the runtime is not allowing the numCPU benchmark to be accurate Once I run the benchmark in isolation that extra load doesnt exist and cant affect the results Rule number one of running a benchmark is that the machine needs to be idle 173
Chapter 9 Generics In this chapter I will learn about the new syntax coming to Go 118 for writing generic functions and types Generics in this first release is about providing the ability to write concrete polymorphic functions with the support of type parameter lists This functionality defined by the generics spec will be represented in the first release of generics  Functions can have an additional type parameter list that uses square brackets but otherwise looks like an ordinary parameter list func FT anyp T     Type parameters can be used by the regular parameters and in the function body  Types can also have a type parameter list type MT any T  Each type parameter has a type constraint just as each ordinary parameter has a type func FT Constraintp T     Type constraints are interface types  The new predeclared name any is a type constraint that permits any type  Interface types used as type constraints can have a list of predeclared types only type arguments that match one of those types satisfy the constraint  an arbitrary type T restricts to that type  an approximation element T restricts to all types whose underlying type is T  a union element T1  T2   restricts to any of the listed elements  Generic functions may only use operations permitted by the type constraint  Using a generic function or type requires passing type arguments  Type inference permits omitting the type arguments of a function call in common cases  Generics will be great for replacing existing Go code that uses the empty interface where having a concrete type would have been better An example of this is with container types lists stacks queues which cant be written effectively with the empty interface I also expect to see new packages like a concurrency package that can provide support for pooling fan outin and drop patterns  174  
91 Basic Syntax If I want to write a single print function that can output a slice of any given type and not use reflection I can use the new generics syntax Listing 91 func printT anyslice T fmtPrintGeneric for v range slice fmtPrintv fmtPrintn This is an implementation of a single print function that can output a slice of any given type using the new generics syntax Whats nice about this syntax is that the code inside the function can use syntax and builtin functions that would work with a concrete type This is not the case when I use the empty interface to write generic code I need a way to tell the compiler that I wont be declaring type T explicitly but it has to be determined by the compiler at compile time The new syntax uses square brackets for this The brackets define a list of generic type identifiers that represent types specific to the function that need to be determined at compile time Its how I tell the compiler that types with these names wont be declared before the program is compiled These types need to be figured out at compile time Note I can have multiple type identifiers defined inside the brackets though the current example is only using one Ex T S R any I can name these type identifiers anything I want to help with the readability of the code In this case Im using the capital letter T to describe that
a slice of some type T to be determined at compile time will be passed in I like the use of single capitalized letters when it comes to collections and its also a convention that goes back to older programming languages like C and Java There is the use of the word any inside the brackets as well This represents a constraint on what type T can be The compiler requires that all generic types have a well defined constraint The any constraint is predeclared by the compiler and states there are no constraints on what type T can be Listing 92 numbers int1 2 3 printintnumbers strings stringA B C printstringstrings 175 floats float6417 22 314 printfloat64floats This is how to make calls to the generic print function where the type information for T is explicitly provided at the call site The syntax emulates the idea that the function declaration func nameT anyslice T defines two sets of parameters The first set is the type that maps to the corresponding type identifiers and the second is the data that maps to the corresponding input variables Luckily the compiler can infer the type and eliminate the need to explicitly pass in the type information at the call site Listing 93 numbers int1 2 3 printnumbers strings stringA B C printstrings floats float6417 22 314 printfloats This code shows how I can call the generic print functions without the need to pass the type information explicitly At the function call site
the compiler is able to identify the type to use for T and construct a concrete version of the function to support slices of that type The compiler has the ability to infer the type with the information it has at the call site from the data being passed in
92 Underlying Types What if I wanted to declare my own generic type using an underlying type Listing 94 type vectorT any T func v vectorT last T error var zero T if lenv 0 return zero errorsNewempty return vlenv1 nil This example shows a generic vector type that restricts the construction of a vector to a single type of data The use of square brackets declares that type T is a generic type to be determined at compile time The use of the constraint any describes there is no constraint on what type T can become The last method is declared with a value receiver of type vectorT to represent a value of type vector with an underlying slice of some type T The method returns a 176 value of that same type T Listing 95 func main fmtPrintvectorint vGenInt vector10 1 i err vGenIntlast if i 0 fmtPrintnegative integer fmtPrintfvalue d error vn i err fmtPrintvectorstring vGenStr vectorA B stringbyte0xff s err vGenStrlast if utf8ValidStrings fmtPrintnonvalid string fmtPrintfvalue q error vn s err Output vectorint negative integer value 1 error nil vectorstring nonvalid string value xff error nil This is how to construct a value of type vector with an underlying type of int when I will set values in the vector at construction An important aspect of this code is the construction calls Listing 96 Zero Value Construction var vGenInt vectorint var vGenStr vectorstring NonZero Value Construction vGenInt vector10 1 vGenStr vectorA B stringbyte0xff When it comes
to constructing these generic types to their zero value its not possible for the compiler to infer the type However in cases where there is initialization during construction the compiler can infer the type There is an aspect of the spec that focuses on the construction of a generic type to its zero value state 177 Listing 97 type vectorT any T func v vectorT last T error var zero T if lenv 0 return zero errorsNewempty return vlenv1 nil I need to focus on the method declaration for the last method and how the method returns a value of the generic type T On the first return is a situation where I need to return the zero value for type T The current draft provides two solutions to write this code The first solution I see already A variable named zero is constructed to its zero value state of type T and then that variable is used for the return The other option is to use the builtin function new and dereference the returned pointer within the return statement Listing 98 type vectorT any T func v vectorT last T error if lenv 0 return newT errorsNewempty return vlenv1 nil This version of the last method is using the builtin function new for zero value construction and dereferencing of the returned pointer to satisfy return type T Note I might think why not use T to perform zero value construction The problem is this syntax does not work with
all types such as the scalar types int string bool So its not an option
93 Struct Types What if I wanted to declare my own generic type using a struct type Listing 99 type nodeT any struct Data T next nodeT prev nodeT This struct type is declared to represent a node for the linked list Each node contains an individual piece of data that is stored and managed by the list The use of square brackets declares that type T is a generic type to be determined at 178 compile time The use of the constraint any describes there is no constraint on what type T can become With type T declared the Data field can now be defined as a field of some type T to be determined later The next and prev fields need to point to a node of that same type T These are the pointers to the next and previous node in the linked list respectively To make this connection the fields are declared as pointers to a node that is bound to type T through the use of the square brackets Listing 910 type listT any struct first nodeT last nodeT The second struct type is named list and represents a collection of nodes by pointing to the first and last node in a list These fields need to point to a node of some type T just like the next and prev fields from the node type Once again the identifier T is defined as a generic type to be determined later that can be substituted
for any concrete type Then the first and last fields are declared as pointers to a node of some type T using the square bracket syntax Listing 911 func l listT adddata T nodeT n nodeT Data data prev llast if lfirst nil lfirst n llast n return n llastnext n llast n return n This is an implementation of a method named add for the list type No formal generic type list declaration is required as with functions since the method is bound to the list through the receiver The add methods receiver is declared as a pointer to a list of some type T and the return is declared as a pointer to a node of the same type T The code after the construction of a node will always be the same regardless of what type of data is being stored in the list since that is just pointer manipulation Its only the construction of a new node that is affected by the type of data that will be managed Thanks to generics the construction of a node can be bound to type T which gets substituted later at compile time 179 Without generics this entire method would need to be duplicated since the construction of a node would need to be hard coded to a known declared type prior to compilation Since the amount of code for the entire list implementation that needs to change for different data types is very small being able to declare
a node and list to manage data of some type T reduces the cost of code duplication and maintenance Listing 912 type user struct name string func main Store values of type user into the list var lv listuser n1 lvadduserbill n2 lvadduserale fmtPrintlnn1Data n2Data Store pointers of type user into the list var lp listuser n3 lpadduserbill n4 lpadduserale fmtPrintlnn3Data n4Data Output bill ale bill ale Here is a small application A type named user is declared and then a list is constructed to its zero value state to manage values of type user A second list is then constructed to its zero value state and this list manages pointers to values of type user The only difference between these two lists is one manages values of type user and the other pointers of type user Since type user is explicitly specified during the construction of the list type the add method in turn accepts values of type user Since a pointer of type user is explicitly specified during the construction of the list type the add method accepts pointers of type user I can see in the output of the program the Data field for the nodes in the respective lists match the data semantic used in the construction
94 Behavior As Constraint Every generic type requires a constraint to be declared so the compiler knows what concrete type substitutions it can accept or reject at compile time This is required even if there is no real constraint on what the generic type can be hence the predeclared constraint identifier any  180  Interesting enough the concept of a constraint already exists in the language Listing 913 type User struct  name string  func u User String string  return uname  type Stringer interface  String string  func Concreteu User  uString  func Polymorphics Stringer  sString   The code defines a concrete type named User and implements a method named String that returns the users name Then an interface type is declared named Stringer which declares one act of behavior String which returns a string Thanks to the method declared for User I can say that the concrete type User implements the Stringer interface using value semantics The Concrete function is just that a function that accepts concrete data based on what it is The Polymorphic is just that as well a function that accepts concrete data based on what it can do This is the primary difference between a concrete and polymorphic function One is limited to one type of data the other isnt However there is a constraint on what concrete data can be passed into the polymorphic function The Stringer interface defines that constraint by declaring a method set of behavior that concrete data must be able to exhibit When applied as the input type the compiler can guarantee the behavioral constraint is met every time the function is called There are generic functions that will require the same type of behavioral constraint Listing 914 func stringifyT fmtStringerslice T string  ret  makestring 0 lenslice for  value  range slice  ret  appendret valueString  return ret   181  Here is the generic function stringify It accepts a slice of some type T and returns a slice of string values that contain a stringified version of each value from the input collection The key to making this function work is the method call to String against each value of type T The problem is that the compiler needs to know and verify that values of type T do have a method named String When the generic type T is declared the fmtStringer interface is provided as the constraint The compiler now knows to check any type substitution and data being passed into the function for this method set of behavior This is excellent because the interface is being used again for the same purpose and the language doesnt need a new keyword  
95 Type As Constraint Generic functions create a new type of constraint that cant be resolved by declaring a method set of behavior Listing 915 func AddT v1 T v2 T T return v1 v2 Here is a generic function that wants to accept two values of some type T add them together and then return the sum back to the caller This is an interesting problem because the compiler needs to constrain the call to the function for only values that can be used in an add operation Currently there is no mechanic for declaring this kind of constraint The decision was to continue to use the interface to declare the constraint and add something new Listing 916 type addOnly interface string int int8 int16 int32 int64 float64 I can declare an interface that defines a set of types that form the constraint Then apply this interface to the generic function Listing 917 func AddT addOnlyv1 T v2 T T return v1 v2 Now the compiler can validate that the set of types is compliant with the operations the function needs to perform against values of those types When the interface is 182 using the builtin types the interfaces are reusable across packages When the list of types represent userdefined types from the package I must remember these generic functions are bound to the packages types and nothing more Interfaces declared with a set of types cant be used in a traditional polymorphic function This wouldnt make sense anyway
but its something that doesnt feel like Go in the sense that this change to the interface is not orthogonal One idea is to have predeclared identifiers for common operation constraints Listing 918 func indexT comparablelist T find T int for i v range list if v find return i return 1 The comparable constraint is declared by the language and applies a constraint that types must be capable of being used in a comparison statement In this example both v and find are variables of type T and are being compared There is an idea that a package in the standard library could provide a common set of constraints as well There is no restriction on an interface being declared with both a set of types and a method set of behavior Listing 919 type matcherT any interface type person food matchv T bool func matchT matcherTlist T find T int for i v range list if vmatchfind return i return 1 A generic interface is declared where T is the type of value to be passed into a method named match The interface also constrains its use to only values of userdefined type person and food When I look at the match function there isnt an obvious need to restrict the function to just the person and food types If this is the case the match function 183 should be a traditional polymorphic function not a generic function If there was a good reason a generic function can
be used to apply this type of constraint As a side note I am not sure this functionality is necessary or makes sense This is something the community will need to figure out over time
96 MultiType Parameters Im not restricted to using just one generic type at a time Listing 920 func PrintL any V fmtStringerlabels L vals V  for i v  range vals  fmtPrintlnlabelsi vString    The Print function accepts a collection of some type L and a collection of some type V Type L can be anything but type V is constrained to values that know how to String The collection of some type V is iterated over and printed with the corresponding label from the collection of type L The name of the generic type can be anything The naming convention for generic types is something that needs to be better defined for best practices For now I try to stick to single letter capital letters when it works for readability  
97 Field Access This is an interesting aspect of generics that I believe can add real value in situations where declaring a method set of behavior would be overkill Listing 921 type User struct  ID int64 Name string Email string  type Customer struct  ID int64 Name string Email string   Here are two concrete types that each represent data to be inserted into a database I write an individual function for each type to perform the insert which is what I want to do  184  Listing 922 func InsertUserdb sqlDB u User User error  const query  insert into users name email values 1 2 result err  ExecuteQueryquery uName uEmail if err  nil  return User err  id err  resultLastInsertId if err  nil  return User err     uID  id return u nil  func InsertCustomerdb sqlDB c Customer Customer error  const query  insert into customers name email values 1 2 result err  ExecuteQueryquery cName cEmail if err  nil  return Customer err  id err  resultLastInsertId if err  nil  return Customer err     cID  id return c nil  On close inspection the difference between the two implementations ends up being a query string and the setting of the ID field If the setting of the ID field could be done in a generic way I could move the bulk of the code out to a generic function Listing 923 type entities interface  User  Customer   I define a new interface named entities that represent the collection of the userdefined types I have  185  Listing 924 func insertT entitiesdb sqlDB entity T query string args interface T error  var zero T result err  ExecuteQueryquery args if err  nil  return zero err  id err  resultLastInsertId if err  nil  return zero err     entityID  id return entity nil  Then I write a generic insert function that defines one generic type T to represent the entity being passed in The key to this generic function is that the compiler can accept the assignment of the id value to the entityID field This is because the interface restricts the concrete types to those that all have an ID field The compiler can see that Listing 925 func InsertUserdb sqlDB u User User error  const query  insert into users name email values 1 2 u err  insertdb u query uName uEmail if err  nil  return User err  return u nil  func InsertCustomerdb sqlDB c Customer Customer error  const query  insert into customers name email values 1 2 c err  insertdb c query cName cEmail if err  nil  return Customer err  return c nil   Now I change the implementation of the two insert functions keeping the query string here and passing everything else into the generic insert function for processing Without this functionality the only solution would be adding a SetID method against the concrete types to assign the id That would result in a setter based interface and code that bloats the solution I prefer to not use setters and getters  
98 Slice Constraints There may be a time where I define a userdefined slice based on an underlying 186  type Listing 926 type Numbers int  Here the userdefined Numbers type has an underlying type that is a slice of integers The compiler allows me to convert variables based on a slice of integers with variables of type Numbers This is usually good and what I want Because of this functionality I can write a generic function that can operate on a slice respecting the underlying type Listing 927 type operateFuncT any funct T T func operateT anyslice T fn operateFuncT T  ret  makeT lenslice for i v  range slice  reti  fnv  return ret   Here the operate function declares a generic type T that can be anything The type is used to declare a parameter named slice that accepts a slice of that same type T The function also accepts a generic function of the same type T and returns a slice of T as well Listing 928 type Numbers int func Doublen Numbers Numbers  fn  funcn int int  return 2  n     numbers  operaten fn fmtPrintfT numbers return numbers  Output int  The Double function accepts a value of type Numbers and passes that value to the operate function In this case the compiler leverages the underlying type in for type T and the Numbers value can be passed into the function However what is returned is a slice of type int as seen in the output If I need to make sure that only a Numbers value can be passed in and is returned  187  by the operate function I can make the following changes Listing 929 type SliceT any interface   T   This interface declares a constraint to restrict a generic type to an actual slice of some type T The use of the approximation element  restricts to all types whose underlying type is T With this interface I can change the operate function Listing 929 type operateFuncT any funct T T type SliceT any interface   T   func operateT anyslice T fn operateFuncT T   ret  makeT lenslice  for i v  range slice   reti  fnv    return ret   func operateS SliceT T anyslice S fn operateFuncT S  ret  makeS lenslice for i v  range slice  reti  fnv  return ret   I change the operate function to declare two generic types Type S which represents a slice value of some type T and T which is a type that can be anything The function returns a value of type S Listing 930 type Numbers int func Doublen Numbers Numbers  fn  funcn int int  return 2  n  numbers  operaten fn fmtPrintfT numbers return numbers   Output mainNumbers  This time when I pass the Numbers value into the operate function the slice that is 188  returned is of type Numbers The underlying type is ignored and the userdefined type is respected  
99 Channels I wanted to explore how the Go team could add a package of concurrency patterns into the standard library thanks to generics This would require declaring channels and functions using generic types Listing 931 type workFnResult any funccontextContext Result In this example I declare a type that represents a function which accepts a context and returns a value of generic type Result This function declaration describes a function that implements the concurrent work that will be performed and the result of that work Listing 932 func doWorkResult anyctx contextContext work workFnResult chan Result ch makechan Result 1 go func ch workctx fmtPrintlndoWork work complete return ch Now I write a function named doWork that executes the specified work function concurrently and returns a channel so the caller can receive the result of the work performed by the work function A generic type named Result is declared to represent the return type for the work function and the type for the channel In the implementation of the doWork function a buffered channel of one is constructed of generic type Result Thats the channel returned to the caller to receive the result of the concurrent work In the middle of the function a goroutine is constructed to execute the work function concurrently Once the work function returns the return argument is sent back to the caller through the channel To test the use of the doWork function I built a small program 189 Listing 933 func main duration 100 timeMillisecond
ctx cancel contextWithTimeoutcontextBackground duration defer cancel dwf funcctx contextContext string timeSleeptimeDurationrandIntn200 timeMillisecond return work complete result doWorkctx dwf select case v result fmtPrintlnmain v case ctxDone fmtPrintlnmain timeout Output doWork work complete main work complete The program starts by declaring a context that will timeout in 100 milliseconds Then a work function is declared that waits for up to 200 milliseconds before returning the string work complete With the context and the work function in place a call to doWork is made and a channel of type string is returned and assigned to the variable result The compiler is able to determine the concrete type to use for the generic type Result by inspecting the return type of the literal work function that is passed into the doWork function This is brilliant because it means I didnt have to pass the type in on the call to doWork With the channel of type string assigned to the variable result a select case is used to wait for the result to be returned on time or for the timeout to occur The doWork function can be used to perform this concurrent work for any concrete type required This same idea could be applied to a pool of goroutines that could execute work on a generic input and return a generic result Listing 934 type workFnInput any Result any funcinput Input Result In this example I changed the function type to accept a generic input and return a generic result 190 Listing
935 func poolWorkInput any Result any size int work workFnInput Result chan Input func var wg syncWaitGroup wgAddsize ch makechan Input for i 0 i size i go func defer wgDone for input range ch result workinput fmtPrintlnpollWork result cancel func closech wgWait return ch cancel In the poolWork function the same two generic types are declared to represent the input and return type for the work function A WaitGroup is constructed to manage the lifecycle of the Goroutines in the pool Then a channel is constructed of the generic Input type This channel is used by the Goroutines in the pool to receive the input data for the work function Then the pool of Goroutines are created with each Goroutine waiting in a receive operation using a forrange loop against the channel Finally a cancel function is constructed to allow the caller to shutdown the pool and wait for all the Goroutines to signal they have terminated To test the use of the poolWork function I built a second small program 191 Listing 936 func main size runtimeGOMAXPROCS0 pwf funcinput int string timeSleeptimeDurationrandIntn200 timeMillisecond return fmtSprintfd received input ch cancel poolWorksize pwf defer cancel for i 0 i 4 i ch i Output pollWork 3 received pollWork 2 received pollWork 1 received pollWork 0 received The size of the pool is calculated based on the number of Goroutines that can run in parallel Then a work function is constructed to sleep for a random amount of time and then
return a string that represents the input With that in place the poolWork function is executed and the channel and cancel function returned The cancel function is deferred and a loop is constructed to send 4 values into the pool The output will be different each time I run the program since this work is happening concurrently These little examples provide some insight into how a concurrent package could be implemented
910 Hash Tables A hash table is a classic example of a container type that can take real advantage of generics This implementation was coded by Matt Layher mdlayer in a blog post he wrote I think its a great example of what is possible with generics This code is a bit more complex than what I have so far Its what I think I can expect to see from real world implementations Throughout this section I will provide two views of the code One before and after applying the new syntax for generics I think this is the best way to write generic code in Go Listing 937 type hashFunc funckey K buckets int int This type declares a hash function signature that is used by the hash table to calculate a bucket position for data storage and retrieval The user must implement and provide this function when constructing a hash table The function accepts a 192 key and the number of buckets it can choose from Since I want this system to be generic in terms of the types used for the key and value I declare a parameter named key with a type of the single capital letter K Next I can apply the generics syntax to make K an actual generic type Listing 938 type hashFuncK comparable funckey K buckets int int CHANGED After the type name I add the square brackets with the generic type K and a constraint of comparable Since values of the
key type need to be used in a compare operation I feel like documenting this now makes sense even if the implementation of the hash function doesnt require it Consistency is everything in terms of readability comprehension and maintainability over time This type represents a keyvalue pair of data that will be stored by the hash table Listing 939 type keyValuePair struct Key K Value V The job of this type is to hold the actual data with the corresponding key I declared a key field of type K and a value field of type V Now I can apply the generics syntax to make K and V an actual generic type Listing 940 type keyValuePairK comparable V any struct Key K Value V CHANGED After the type name I add the square brackets with the generic types K and V In this declaration K represents the key as before and V represents a value which can be anything This type represents a hash table that manages a hash function and a set of buckets for keyvalue data storage Listing 941 type Table struct hashFunc hashFunc buckets int data keyValuePair 193 The Table type has three fields a hash function the number of buckets and the data which is represented as a slice of a slice of keyvalue pairs The outer slice represents buckets and the inner slice represents keyvalue pairs that are stored inside a bucket Now I can apply the generics syntax to declare the key and value
generic types and apply them to the field declarations Listing 942 type TableK comparable V any struct hashFunc hashFuncK buckets int data keyValuePairK V CHANGED CHANGED CHANGED After the type name I add the square brackets with the generic types K and V The hashFunc type declaration requires information about the concrete type to use for the key The keyValuePair type declaration requires information about the concrete type for the key and value This is a factory function that can construct a Table for use Listing 943 func New buckets int hf hashFunc Table return Table hashFunc hf buckets buckets data makekeyValuePair buckets The factory function accepts the number of buckets to manage and a hash function for selecting a bucket for data storage and lookup When a Table value is constructed the number of buckets is used to construct the slice setting the length of the outer slice to the number of buckets that will be used Now I can apply the generics syntax to declare the key and value generic types and apply them to the types that need to be constructed 194 Listing 944 func NewK comparable V any buckets int hf hashFuncK TableK V return TableK V hashFunc hf buckets buckets data makekeyValuePairK V buckets CHANGED CHANGED CHANGED CHANGED CHANGED After the type name I add the square brackets and the generic types K and V Then K is applied to the hf input parameter to complete the hashFunc type declaration The K and V types
are applied to the Table type being constructed and returned Finally the initialization of the data field requires K and V to be applied to the construction syntax for the keyValuePair type This is a method that can insert values into the hash table based on a specified key Listing 945 type TableK comparable V any struct hashFunc hashFuncK buckets int table keyValuePairK V func t Table Insertkey K value V bucket thashFunckey tbuckets for idx kvp range ttablebucket if key kvpKey ttablebucketidxValue value return kvp keyValuePair Key key Value value ttablebucket appendttablebucket kvp The Insert method is declared to accept a key and value of the same generic types that are declared with the Table type The first step of inserting is to identify the bucket to use for storage That is performed by calling the hash function with the specified key The hash function returns an integer value that represents the bucket to use Then the function checks to see if the specified key has already been used to store a value in the selected bucket This is performed by ranging over the existing set of keyvalue pairs in the bucket If the key already exists the value for that key is updated If the key is not found then a new keyvalue pair value is constructed 195 initialized and appended to the slice for the selected bucket Now I can apply the generics syntax to declare the key and value generic types and apply them to the
types that need to be constructed Listing 946 func t TableK V Insertkey K value V bucket thashFunckey tbuckets for idx kvp range ttablebucket if key kvpKey ttablebucketidxValue value return kvp keyValuePairK V Key key Value value ttablebucket appendttablebucket kvp CHANGED CHANGED After the type name in the receiver I add the square brackets and the generic types K and V The only other change is to apply K and V to the construction syntax of the keyValuePair type This is a method that can retrieve values from the hash table based on a specified key Listing 947 func t Table Retrievekey K V bool bucket thashFunckey tbuckets for idx kvp range tdatabucket if key kvpKey return tdatabucketidxValue true var zero V return zero false The Retrieve method is declared to accept a key and return a copy of the value stored for that key The first step of retrieving is to identify the bucket that was used for storage That is performed by calling the hash function with the specified key The hash function returns an integer value that represents the bucket to look at Then the function iterates over the collection of keyvalue pairs stored inside the bucket looking for the specified key one by one If the key is found a copy of the value is returned and true is provided to the caller If the key is not found zero 196 value is returned and false is provided to the caller Now I can apply the
generics syntax to declare the key and value generic types and apply them to the types that need to be constructed Listing 948 func t TableK V Getkey K V bool bucket thashFunckey tbuckets for idx kvp range tdatabucket if key kvpKey return tdatabucketidxValue true CHANGED var zero V return zero false After the type name in the receiver I add the square brackets and the generic types K and V No other code changes are required This is a small program to test the hash table implementation Listing 949 func main const buckets 8 I start with a constant that defines the number of buckets to use in the hash table Listing 950 import hashfnv func main hashFunc1 funckey string buckets int int h fnvNew32 hWritebytekey return inthSum32 buckets Next I declare a hash function that declares a string for the key The implementation uses the fnv package from the standard library which implements the FNV1 and FNV1a noncryptographic hash functions created by Glenn Fowler Landon Curt Noll and Phong Vo FNV stands for the FowlerNollVo hash function 197 The modulus operation with the buckets value forces the final value to fall within the range for the number of buckets Listing 951 import hashfnv func main table1 Newkey string value intbuckets hashFunc1 Next I construct a hash table explicitly stating that the key will be of type string and the value of type int There is nothing in the input parameters that can help the compiler infer this information
To show the nature of the hash table being generic I defined a second hash function and table Listing 952 import hashfnv func main hashFunc2 funckey int buckets int int return key buckets table2 Newkey int value stringbuckets hashFunc2 This hash function declares an integer for the key and performs a simple modulus operation with the bucket value against the key Then a new table is constructed where the key is specified to be an integer and the value a string The reverse of the first table 198 Listing 953 import hashfnv func main words stringfoo bar baz for i word range words table1Insertword i table2Inserti word for i s range appendwords nope v1 ok1 table1Retrieves fmtPrintft1Rtrv v vn s v1 ok1 v2 ok2 table2Retrievei fmtPrintft2Rtrv v vn i v2 ok2 Output t1Rtrfoo 0 true t2Rtr0 foo true t1Rtrbar 1 true t2Rtr1 bar true t1Rtrbaz 2 true t2Rtr2 baz true t1Rtrnope 0 false t2Rtr3 false Finally I can write some code to store and retrieve values from the two respective tables 199 Chapter 10 Profiling In this chapter I will learn how to profile code using benchmarks Even though I will be generating profiles from a benchmark a lot of what is shared can be used regardless of how the profile is generated
101 Introduction I can use the go tooling to inspect and profile my programs Profiling is both a journey and detective work It requires some understanding about the application and expectations The profiling data in and of itself is just raw numbers I have to give it meaning and understanding  
1011 The Basics of Profiling Those who can make you believe absurdities can make you commit atrocities Voltaire How does a profiler work A profiler runs my program and configures the operating system to interrupt it at regular intervals This is done by sending SIGPROF to the program being profiled which suspends and transfers execution to the profiler The profiler then grabs the program counter for each executing thread and then continues running the program Profiling dos and donts Before I profile I must have a stable environment to get repeatable results   The machine must be idledont profile on shared hardware dont browse the web while waiting for a long benchmark to run    Watch out for power saving and thermal scaling    Avoid virtual machines and shared cloud hosting they are too noisy for consistent measurements  If I can afford it buy dedicated performance test hardware Rack them disable all the power management and thermal scaling and never update the software on those machines If I cant have a before and after sample and run them multiple times to get consistent results  
1012 Types of Profiling There are several types of profiling I can perform in Go CPU profiling CPU profiling is the most common type of profile When CPU profiling is enabled the runtime will interrupt itself every 10ms and record the stack trace of the currently running Goroutines Once the profile is saved to disk we can analyze it to 200  determine the hottest code paths The more times a function appears in the profile the more time that code path is taking as a percentage of the total runtime Memory profiling Memory profiling records the stack trace when a heap allocation is made Memory profiling like CPU profiling is sample based By default samples are profiled at 1 alloc for every 512kb This rate can be changed Stack allocations are assumed to be free and are not tracked in the memory profile Because memory profiling is sample based and because it tracks allocations not used using memory profiling to determine my applications overall memory usage is difficult Blocking profiling Blocking profiling is quite unique A block profile is similar to a CPU profile but it records the amount of time a Goroutine spent waiting for a shared resource This can be useful for determining concurrency bottlenecks in my application Blocking profiling can show me when a large number of Goroutines could make progress but were blocked Blocking includes   Sending or receiving on an unbuffered channel    Sending to a full channel receiving from an empty one    Trying to Lock a syncMutex that is locked by another Goroutine    Block profiling is a very specialized tool it should not be used until I believe Ive eliminated all my CPU and memory usage bottlenecks  One profile at at time Profiling is not free Profiling has a moderate but measurable impact on program performanceespecially if I increase the memory profile sample rate Most tools will not stop me from enabling multiple profiles at once If I enable multiple profiles at the same time they will observe their own interactions and skew my results Do not enable more than one kind of profile at a time  
1013 Hints to interpret what I see in the profile If I see lots of time spent in runtimemallocgc function the program potentially makes an excessive amount of small memory allocations The profile will tell me where the allocations are coming from See the memory profiler section for suggestions on how to optimize this case If lots of time is spent in channel operations syncMutex code and other synchronization primitives or system components the program probably suffers from contention Consider restructuring the program to eliminate frequently accessed  shared  resources  Common  techniques  for  this  include  shardingpartitioning local bufferingbatching and copyonwrite technique  201  If lots of time is spent in syscallReadWrite the program potentially makes an excessive amount of small reads and writes Bufio wrappers around osFile or netConn can help in this case If lots of time is spent in the GC component the program either allocates too many transient objects or the heap size is very small so garbage collections happen too frequently   Large objects affect memory consumption and GC pacing while large numbers of tiny allocations affect marking speed    Combine values into larger values This will reduce the number of memory allocations faster and also reduce pressure on the garbage collector faster garbage collections    Values that do not contain any pointers are not scanned by the garbage collector Removing pointers from actively used values can positively impact garbage collection time  
1014 Rules of Performance Basic rules around performance 1 Never guess about performance 2 Measurements must be relevant 3 Profile before I decide something is performance critical 4 Test to know Im correct  
1015 Go and OS Tooling time program The time command provides information that can help me get a sense how my program is performing Perf program If Im on linux then perf1 is a great tool for profiling applications Since Go has frame pointers perf can profile Go applications 7026140760 taskclock msec 1665 contextswitches 39 cpumigrations 77362 pagefaults 21769537949 cycles 11671235864 stalledcyclesfrontend 6839727058 stalledcyclesbackend 27157950447 instructions 5351057260 branches 118150150 branchmisses 5476816754 seconds time elapsed  202              1283 CPUs utilized 0237 Ksec 0006 Ksec 0011 Msec 3098 GHz 8341 5361 frontend cycles idle 8331 3142 backend cycles idle 6665 125 insns per cycle 043 stalled cycles per insn 8325 761593 Msec 8349 221 of all branches 8315  
102 Example Code I have a program that takes a stream of data looking for the name elvis with a lowercase e If that name is found in the stream the name is corrected by replacing the lowercase e with a capital E Listing 101 var data struct input byte output byte byteabc byteabc byteelvis byteElvis byteaElvis byteaElvis byteabcelvis byteabcElvis byteeelvis byteeElvis byteaelvis byteaElvis byteaabeeeelvis byteaabeeeElvis bytee l v i s bytee l v i s byteaa bb e l v i saa byteaa bb e l v i saa byte elvi s byte elvi s byteelvielvis byteelviElvis byteelvielvielviselvi1 byteelvielviElviselvi1 byteelvielviselvis byteelviElvisElvis This is a data table that represents the potential data from the stream It lays out the input stream and the expected output stream Listing 102 func assembleInputStream byte var in byte for d range data in appendin dinput return in func assembleOutputStream byte var out byte for d range data out appendout doutput return out These functions assemble the input and output into a single stream for processing With this in place here is an algorithm I wrote to solve the problem 203 Listing 103 func algOnedata byte find byte repl byte output bytesBuffer input bytesNewBufferdata size lenfind buf makebyte size end size 1 if n err ioReadFullinput bufend err nil outputWritebufn return for if err ioReadFullinput bufend err nil outputWritebufend return if bytesEqualbuf find outputWriterepl if n err ioReadFullinput bufend err nil outputWritebufn return continue outputWriteBytebuf0 copybuf buf1 My algorithm is based on the idea of
creating a buffer of 5 bytes and comparing those 5 bytes with the lowercase version of elvis If there is a match then the uppercase version of Elvis is sent through the output stream If there is no match the first byte of the buffer is sliced off and a new byte from the input stream is added to the end of the buffer Then those 5 bytes are compared again Luckily my friend Tyler took the time to provide a different implementation that solves the same problem 204 Listing 104 func algTwodata byte find byte repl byte output bytesBuffer input bytesNewReaderdata size lenfind idx 0 for b err inputReadByte if err nil break if b findidx idx if idx size outputWriterepl idx 0 continue if idx 0 outputWritefindidx inputUnreadByte idx 0 continue outputWriteByteb idx 0 Tylers algorithm is based on the idea of reading one byte at a time out of the input stream and comparing that byte with the lowercase version of elvis based on a moving index The algorithm starts with the index at zero which means the first byte out of the input stream is compared with the lowercase e in elvis If there is a match the index is incremented so the next byte pulled from the input stream can be compared to the letter l and so on and so on If there are five matches in a row the lowercase version of elvis is found and the uppercase version of Elvis is sent
through the output stream If there is no match the index is reset to zero the last byte is unread the process starts over again I found this solution by Tyler really cool
103 Benchmarking With the two algorithms in place that solve the same exact problem in two different ways the next step is to write benchmark functions to compare the two algorithms  205  Listing 105 var output bytesBuffer var in  assembleInputStream var find  byteelvis var repl  byteElvis func BenchmarkAlgorithmOneb testingB  for i  0 i  bN i  outputReset algOnein find repl output   func BenchmarkAlgorithmTwob testingB  for i  0 i  bN i  outputReset algTwoin find repl output    With the benchmark functions in place I can now compare the two algorithms using the testing tool I will add the benchmem flag so the tooling reports allocations Listing 106  go test bench  benchtime 3s benchmem goos darwin goarch amd64 pkg githubcomardanlabsgotrainingtopicsgoprofilingmemcpu cpu IntelR CoreTM i99980HK CPU  240GHz BenchmarkAlgorithmOne16 BenchmarkAlgorithmTwo16  2120113 9103246  1594 nsop 3877 nsop  53 Bop 0 Bop  2 allocsop 0 allocsop  PASS ok  I see two things in the results First Tylers algorithm is roughly 4 times faster than mine Second Tyler wrote a zero allocation algorithm and mine is allocating two values on the heap worth a total of 52 bytes At this point I want to find and remove those two values allocating on the heap with the hope that maybe my algorithm will be as fast or at least closer to the same performance as Tylers algorithm To find these allocations I need a memory profile  
104 Memory Profiling To generate a memory profile I need to add the memprofile flag to the go test call 206 Listing 107 go test bench benchtime 3s benchmem memprofile pout goos darwin goarch amd64 pkg githubcomardanlabsgotrainingtopicsgoprofilingmemcpu cpu IntelR CoreTM i99980HK CPU 240GHz BenchmarkAlgorithmOne16 BenchmarkAlgorithmTwo16 2260207 8515495 1566 nsop 4225 nsop 53 Bop 0 Bop 2 allocsop 0 allocsop PASS ok The output doesnt change but I do end up with two new files in the local directory Listing 108 ls la drwxrxrx drwxrxrx rwrrrwxrxrx rwrrrwrrrwrr 7 bill 14 bill 1 bill 1 bill 1 bill 1 bill 1 bill staff staff staff staff staff staff staff 224B May 7 1059 448B Jan 4 0816 23K Apr 28 0909 READMEmd 30M May 7 1059 memcputest 488B May 7 1059 pout 46K Apr 30 1556 streamgo 773B Apr 28 0909 streamtestgo I can see a pout file that is fairly small at 488 bytes This is the memory profile data I requested with the memprofile flag There is also a file named memcputest This is the test binary that was built by the compiler to run the benchmarks When I ask for a memory profile the test binary is not removed after the benchmark is run This is so extra information can be provided by the profile tooling Go has a profiling tool that I can use to review the profile data Listing 109 go tool pprof memcputest pout File memcputest Type allocspace Time May 7 2021 at 1059am CDT Entering
interactive mode type help for commands o for options pprof Im using the terminal based profile tooling however if I want to use the browser version of the tooling I just need to add the http flag with a port number Listing 1010 go tool pprof http 8080 memcputest pout For now I will stay in the terminal window Since I know which function is allocating 207 I can ask the tool to provide information specific about the algOne function using the list command Listing 1011 pprof list algOne Total 16751MB 1250MB 16751MB flat cum 100 of Total 78 79 algOne is one way to solve the problem 80func algOnedata byte find byte repl 81 82 Use a bytes Buffer to provide a stream to 15501MB 83 input bytesNewBufferdata 84 85 The number of bytes we are looking for 86 size lenfind 87 88 Declare the buffers we need to process the 1250MB 1250MB 89 buf makebyte size 90 end size 1 91 92 Read in an initial number of bytes we need 93 if n err ioReadFullinput bufend 94 outputWritebufn pprof The list command gives me a detailed view of a function and takes a regular expression to find the functions I want to see Thanks to the list command I can see there is an allocation happening on line 83 and on line 89 Listing 1012 1250MB 16751MB flat cum 100 of Total 15501MB 83 input bytesNewBufferdata 1250MB 1250MB 89 buf makebyte size pprof At closer inspection I
see there are two columns in the report The first column represents flat allocations and the second represents cumulative allocations Flat means the value allocated to the heap is represented on that line of code Cumulative means the values allocating to the heap are represented inside the call chain originating from that function call I was looking for two allocations and it seems lines 83 and 89 contain the construction points for the two values allocating in the heap Thats only part of the story I still dont know why and the profile can never tell me this Before I begin to try and understand why the values allocated I want to see this same list view in the browser I can do this by using the weblist command 208 Listing 1013 pprof weblist algOne The weblist command is similar to the list command except it brings up the browser tooling Figure 101 This is the browser based tool showing the same list output One reason to use the browser over the terminal is when I include the test binary This allows me to see the list output down to the assembly level Figure 102 When I click on the code at line 89 the assembly behind that line of code is shown This only works when I provide the test binary If I look closer at the list output from the browser I notice a difference from the terminal view That is the allocation on line 83 is a
flat allocation 209 Figure 103 See how the first column shows the same allocation number as the second column This begs the question is the allocation flat which means the input variable is being constructed on the heap or is the allocation cumulative which means the allocation is coming from inside the bytesNewBuffer function From my point of view the browser view is more accurate because it takes into account an important compiler optimization that took place when the code ran called inlining
105 Inlining Inlining is an optimization that removes function calls and replaces them with a copy of the code inside the function in question For some reason the terminal view has inlining turned off How can I get the terminal to show the list command with inlining turned on Listing 1014  go tool pprof noinlines  memcputest pout  I need to add the noinlines flag to the call to go tool pprof which is supposed to turn off inlining but for the terminal view it turns it on  210  Listing 1015 pprof list algOne Total 16751MB 16751MB 16751MB flat cum 100 of Total   78   79 algOne is one way to solve the problem   80func algOnedata byte find byte repl   81   82  Use a bytes Buffer to provide a stream to 15501MB 15501MB 83 input  bytesNewBufferdata   84   85  The number of bytes we are looking for   86 size  lenfind   87   88  Declare the buffers we need to process the 1250MB 1250MB 89 buf  makebyte size   90 end  size  1   91   92  Read in an initial number of bytes we need   93 if n err  ioReadFullinput bufend   94 outputWritebufn pprof  Now the terminal view is showing line 83 as a flat allocation How does inlining actually work Start with the bytesNewBuffer function which we know is being inlined Listing 1016 func NewBufferbuf byte Buffer  return Bufferbuf buf   I can see this is a factory function using pointer semantics The caller gets shared access to the Buffer value being constructed If the function is called then the Buffer would have to be constructed in the heap This is because of the ownership rule Any function that constructs a value is the owner of that value If the value needs to exist after the owning function returns then the value must be constructed in the heap If the function can be inlined the ownership of the construction moves up to the calling function This means the algOne function owns the construction of Buffer  211  Listing 1017 func algOne   Before inlining input  bytesNewBufferdata   Original Call   After inlining input  bytesBufferbuf data   After Inlining Optimization    I can see that after the inlining optimization the algOne function becomes the owner of the construction of the Buffer value not the NewBuffer function Therefore the value doesnt need to escape to the heap unless the value still needs to exist after the algOne function returns Its important to note that Tylers algorithm is a zero allocation algorithm in part thanks to the same inlining optimization Listing 1018 func algOne   Before inlining input  bytesNewReaderdata   Original Call   After inlining input  bytesReaderbuf data   After Inlining Optimization    Since the NewReader function is inlined the construction of the Reader value is owned by Tylers algorithm Somehow Tyler is not doing anything in algTwo that is causing the Reader value to escape where I am doing something in algOne with the Buffer value  
106 Escape Analysis How do I know inlining is absolutely happening and I still dont know why algOne is causing allocations where algTwo is zero allocation Only one tool can tell me the why and thats the compiler The compiler is making the decision using the escape analysis algorithm Luckily the compiler builds the test binary before anything runs so I can ask the compiler to generate an escape analysis report before running the benchmark Listing 1019 go test bench benchtime 3s benchmem memprofile pout gcflags m2 I added gcflags m2 as flags on the call to go test This produces an escape analysis report before running the benchmark 212 Listing 1020 streamgo8326 inlining call to bytesNewBuffer funcbyte bytesBuffer return bytesBuffer streamgo8326 bytesBuffer escapes to heap streamgo8326 flow R0 storage for bytesBuffer streamgo8326 from bytesBuffer spill at streamgo8326 streamgo8326 from R0 N assignpair at streamgo8326 streamgo8326 flow input R0 streamgo8326 from input bytesBufferR0 assign at streamgo838 streamgo8326 flow ior input streamgo8326 from input interfaceconverted at streamgo11328 streamgo8326 from ior iobuf input bufend assignpair at streamgo11328 streamgo8326 flow heap ior streamgo8326 from ioReadAtLeastior iobuf leniobuf call parameter at streamgo11328 These are the lines in the escape analysis report related to line 83 from the list output The very first line states the compiler is choosing to inline the call to bytesNewBuffer This is the proof I wanted to be absolutely sure the bytesNewBuffer function was being inlined How does the compiler decide what functions can be inlined or not Listing 1021
streamgo806 cannot inline algOne function too complex cost 636 exceeds budget 80 streamgo1316 cannot inline algTwo function too complex cost 315 exceeds budget 80 Here are two lines from the report that show the scoring for the algOne and algTwo functions I can see both functions scored above 80 points and therefore are not a candidate for inlining The compiler uses a complex scoring system to determine if a function can be inlined and every function is scored The code for the inlining algorithm can be found in the inlgo file that is part of the compiler httpsgithubcomgolanggoblobmastersrccmdcompileinternalinlineinlgo How can I almost guarantee a factory function like NewBuffer or NewReader will score under 80 I need to make it a leaf function In other words make sure no other function calls are made so the call tree ends with the factory function 213 Now I absolutely know the algOne function owns the construction of the bytesBuffer value I still dont know why the input variable has allocated especially since it doesnt need to exist after the algOne function returns I need to look at the escape analysis report for more information Listing 1022 streamgo8326 bytesBuffer escapes to heap streamgo8326 flow R0 storage for bytesBuffer streamgo8326 from bytesBuffer spill at streamgo8326 streamgo8326 from R0 N assignpair at streamgo8326 streamgo8326 flow input R0 streamgo8326 from input bytesBufferR0 assign at streamgo838 streamgo8326 flow ior input streamgo8326 from input interfaceconverted at streamgo11328 streamgo8326 from ior iobuf input bufend assignpair at streamgo11328 streamgo8326 flow heap
ior streamgo8326 from ioReadAtLeastior iobuf leniobuf call parameter at streamgo11328 This section of the report has the final why Its sharing information about the construction of the bytesBuffer on line 83 where the function was inlined Listing 1023 flow ior input from input interfaceconverted at streamgo11328 from ior iobuf input bufend assignpair at streamgo11328 An interface conversion at line 113 seems to be the reason What code is on line 113 Listing 1024 113 if n err ioReadFullinput bufend err nil 114 outputWritebufn 115 return 116 Its the call to ioReadFull and I can see the input variable is being passed into the function as the first parameter Passing a value down the call stack doesnt create an allocation so whats different about this call I need to look at the ioReadFull function Listing 1025 func ReadFullr Reader buf byte n int err error return ReadAtLeastr buf lenbuf 214 I can see the ioReadFull function is a polymorphic function Its accepting the input value being passed not based on what it is but on what it does Its using the ioReader interface to accept the data Thats the reason behind the interfaceconverted and assignpair message in the report Tyler is not using the io package but the method set of the input variable and hes getting the inline optimization as well This explains why Tyler doesnt have an allocation So if I want to get rid of this allocation on line 83 I need to stop using the io package
and switch to the method set Listing 1026 func algOnedata byte find byte repl byte output bytesBuffer input bytesNewBufferdata size lenfind buf makebyte size end size 1 if n err inputReadbufend err nil outputWritebufn return for var err error bufend0 err inputReadByte if err nil outputWritebufend return if bytesEqualbuf find outputWriterepl if n err inputReadbufend err nil outputWritebufn return continue REPLACED REPLACED REPLACED outputWriteBytebuf0 copybuf buf1 I have replaced the three calls using the io package for methods against the input variable This should remove the allocation on line 83 215 Listing 1027 go test bench benchtime 3s benchmem memprofile pout gcflags m2 goos darwin goarch amd64 pkg githubcomardanlabsgotrainingtopicsgoprofilingmemcpu cpu IntelR CoreTM i99980HK CPU 240GHz BenchmarkAlgorithmOne16 BenchmarkAlgorithmTwo16 3658340 9730435 9753 nsop 3776 nsop 5 Bop 0 Bop 1 allocsop 0 allocsop PASS ok Sure enough I am down to 1 allocation worth 5 bytes Where is this last 5 bytes allocating from Time to use the list command again to see the line number and then inspect the escape analysis report Listing 1028 go tool pprof noinlines pout pprof list algOne 20MB 20MB flat cum 100 of Total 83 84 The number of bytes we are looking for 85 size lenfind 86 87 Declare the buffers we need to process the 20MB 20MB 88 buf makebyte size 89 end size 1 90 91 Read in an initial number of bytes we need 92 if n err inputReadbufend err nil 93 outputWritebufn The final allocation is on line 88 related to
the construction of the slice value Listing 1029 streamgo8813 makebyte size escapes to heap streamgo8813 flow heap storage for makebyte size streamgo8813 from makebyte size nonconstant size at streamgo8813 The report is stating nonconstant size which means the compiler doesnt know the size of the backing array at compile time This can be fixed if I hard code the value of 5 on the call to make If I only ever search for a 5 byte string then Ill have no problems with this change Hahahaha 216 Listing 1030 func algOnedata byte find byte repl byte output bytesBuffer input bytesNewBufferdata size lenfind buf makebyte 5 REPLACED After changing out the size variable for the literal number 5 on the call to make does the allocation go away Listing 1031 go test bench benchtime 3s benchmem memprofile pout gcflags m2 goos darwin goarch amd64 pkg githubcomardanlabsgotrainingtopicsgoprofilingmemcpu cpu IntelR CoreTM i99980HK CPU 240GHz BenchmarkAlgorithmOne16 BenchmarkAlgorithmTwo16 4129378 9716834 8685 nsop 3760 nsop 0 Bop 0 Bop 0 allocsop 0 allocsop PASS ok Now I have a zero allocation algorithm However Tylers algorithm is still faster than mine The next thing I can do is run a cpu profile to possibly find some inefficient code that can be changed Listing 1032 go test bench benchtime 3s benchmem cpuprofile pout goos darwin goarch amd64 pkg githubcomardanlabsgotrainingtopicsgoprofilingmemcpu cpu IntelR CoreTM i99980HK CPU 240GHz BenchmarkAlgorithmOne16 BenchmarkAlgorithmTwo16 4079672 8689845 9064 nsop 4143 nsop 0 Bop 0 Bop 0 allocsop 0 allocsop PASS ok By using the cpuprofile flag
instead of the memprofile flag I will get a cpu profile 217 Listing 1033 go tool pprof pout Type cpu pprof list algOne 950ms 110ms 310ms 270ms 100ms 160ms 397s flat cum 5336 of Total 87 Declare the buffers we need to process the 88 buf makebyte 5 89 end size 1 90 91 Read in an initial number of bytes we need 120ms 92 if n err inputReadbufend err nil 93 outputWritebufn 94 return 95 96 97 for 98 99 Read in one byte from the input 100 var err error 440ms 101 bufend0 err inputReadByte 102 if err nil 103 104 Flush the reset of the bytes 105 outputWritebufend 106 return 107 108 109 If we have a match replace the 170s 110 if bytesEqualbuf find 650ms 111 outputWriterepl 112 113 Read a new initial number of 240ms 114 if n err inputReadbufend 10ms 115 outputWritebufn 116 return 117 118 119 continue 120 121 122 Write the front byte since it has 650ms 123 outputWriteBytebuf0 124 125 Slice that front byte out 160ms 126 copybuf buf1 127 128 129 Looks like there is almost four seconds of cumulative time taken in the algOne function and the majority of it is on line 110 Listing 1034 270ms 170s 650ms 109 110 111 If we have a match replace the if bytesEqualbuf find outputWriterepl So if I could find a way to replace the call to bytesEqual I could possibly get even 218 closer to the same performance as Tylers
algorithm Here is the reality I should have just gone with Tylers algorithm from the beginning if that level of performance was really needed Honestly my algorithm really wasnt that slow using the io package Read functions Even after removing the allocations Im only looking at a difference of 500 nanoseconds per operation 219
Chapter 11 Profiling Live Code In this chapter I will learn how to profile code that is running This will require the use of the http package in the standard library Its best I read chapter 10 before reading this chapter That chapter goes deep into the use of the profile tooling and that knowledge is assumed here  
111 Example Code I have a program that implements a search engine using news feeds as the searchable content The news feeds are downloaded and cached when the program starts This is the code that is being used in this chapter httpsgithubcomardanlabsgotrainingtreemastertopicsgoprofilingproject To start I need to build and run the project Listing 111  cd Cloned Locationtopicsgoprofilingproject  go build  project 20210510 112921616793 servicego64 Listening on 00005000  Once the project is running I can open the browser to run the website by using the URL localhost5000search  220  Figure 111 httplocalhost5000search  This is what the logs look like after running a search for the term biden If I look at the logs I can see 12 feeds were downloaded and cached from the BBC CNN and the NY Times Listing 112 servicego64 Listening on 00005000 rssgo109 reloaded cache httpfeedsbbcicouknewsrssxml rssgo109 reloaded cache httpfeedsbbcicouknewsworldrssxml rssgo109 reloaded cache httprsscnncomrsscnntopstoriesrss rssgo109 reloaded cache httpfeedsbbcicouknewspoliticsrssxml rssgo109 reloaded cache httprsscnncomrsscnnworldrss rssgo109 reloaded cache httpfeedsbbcicouknewsworldusandcanadarssxml rssgo109 reloaded cache httprsscnncomrsscnnusrss rssgo109 reloaded cache httprsscnncomrsscnnallpoliticsrss rssgo109 reloaded cache httprssnytimescomservicesxmlrssnytHomePagexml rssgo109 reloaded cache httprssnytimescomservicesxmlrssnytUSxml rssgo109 reloaded cache httprssnytimescomservicesxmlrssnytPoliticsxml rssgo109 reloaded cache httprssnytimescomservicesxmlrssnytBusinessxml  One of the first checks that I like to perform is to generate a garbage collection GC trace for the program while its executing a fixed amount of load This will provide an initial understanding if there is a memory leak and its nice to gather  221  some stats about how much work the garbage collector is performing  
112 Generating a GC Trace Before I run any load through the program I want to look at an initial GC trace that is produced from the first search This is when the feeds are downloaded for the first time To see a GC trace I need to use the GODEBUG environment variable when running the project Listing 113  GODEBUGgctrace1 project  devnull  The GODEBUG variable has several different options but this is how I can ask the Go runtime to write a trace to stderr each time the GC runs Since I dont want my logs to compete with the trace output Im writing anything being sent to stdout to the devnull device If I restart the program using these options and hit the search button again I will see 4 GCs take place Listing 114 gc 1 95466s 0 00420540026 ms clock 067032073029043 ms cpu 440 MB 5 MB goal 16 P gc 2 95635s 0 00310390029 ms clock 051024080068047 ms cpu 441 MB 5 MB goal 16 P gc 3 95707s 0 00470400012 ms clock 076007209210020 ms cpu 441 MB 5 MB goal 16 P gc 4 95976s 0 00480470016 ms clock 0780131013025 ms cpu 442 MB 5 MB goal 16 P  Seeing 4 GCs makes sense since the GC will initially run when the first 4 meg of memory is allocated on the heap Since the program is downloading and caching the feeds the other 3 GCs happen quickly as the GC tries to keep the heap at 4 meg Here is a breakdown of the 4th GC that took place  222  Listing 115  General gc 4 95976s 0   gc 4 95976s 0  The 4th GC run since the program started  95976 seconds since the program started  Zero percent of the programs time has been spent in GC   Wall Clock 00480470016 ms clock 0048ms  STW  Mark Setup  Write Barrier on 047ms  Concurrent  Marking 0016ms  STW  Mark Termination  Write Barrier off  Clean Up  CPU Clock  0780131013025 ms cpu 078ms  STW  Mark Setup 013ms  Concurrent  Mark  Assist Time 10ms  Concurrent  Mark  Background GC time 13ms  Concurrent  Mark  Idle GC time 025ms  STW  Mark Termination  Memory 4MB 4MB 2MB   442 MB  Heap memory inuse before the Marking started  Heap memory inuse after the Marking finished  Heap memory marked as live after the Marking finished   Goal 5MB   5 MB goal  Collection goal for heap memory inuse after Marking finished   Threads 16P   16 P  Number of logical Ps or threads used to run Goroutines  The comments explain what each of the different numbers mean For my purpose in evaluating potential memory leaks and if the GC is overworking the memory numbers and the percent of time in GC is what I will evaluate  
113 Generating Load And Evaluation To apply load on the program I will use a tool named hey The tool can be found on Github at httpsgithubcomrakyllhey With the service already started in one terminal I will run the hey tool in a second terminal to send 10k requests over 100 concurrent connections Listing 116  hey m POST c 100 n 10000 httplocalhost5000search termbidencnnonbbconnyton  Once the load is sent into the program I will see GC traces flowing After the last request is processed the hey tooling provides the results  223  Listing 117 Hey Summary Total 26945 secs Slowest 02664 secs Fastest 00011 secs Average 00248 secs Requestssec 37113009 GC Trace last 4 gc 837 181810s 0 00380640027 ms clock 0610241622044 ms cpu 673 MB 8 MB goal 16 P gc 838 181812s 0 00300600052 ms clock 0480381324084 ms cpu 663 MB 7 MB goal 16 P gc 839 181815s 0 00440650032 ms clock 0710341813052 ms cpu 673 MB 7 MB goal 16 P gc 840 181819s 0 00370560035 ms clock 0590141426056 ms cpu 773 MB 8 MB goal 16 P  The hey report doesnt seem terribly bad and neither does the end of the GC trace It seems that 836 GCs needed to take place to complete the work Im subtracting the first 4 GCs that took place before I ran the load The program was able to process 3711 requests a second with the slowest request taking 266ms Im confident there is no memory leak since the memory is at 7 meg in use and 3 meg marked live In fact if I wait a bit the GC will force itself to run and I can see if anything is reduced more Listing 118 GC forced gc 841 302003s 0 00720840005 ms clock 11024450083 ms cpu 553 MB 6 MB goal 16 P GC forced gc 842 422004s 0 010100004 ms clock 17032320077 ms cpu 332 MB 6 MB goal 16 P GC forced gc 843 542255s 0 0096100004 ms clock 15033480071 ms cpu 222 MB 4 MB goal 16 P  I can see after waiting a few minutes the memory in use on the heap dropped to 2 meg which matches what is live This is a good sign of no memory leak  
114 Adding Profile Endpoints With these performance numbers from hey what I need next is a memory profile that is representative of these 10k requests I sent through the program Luckily I can get this profile because I set up a couple things in the code  224  Listing 119 package main import   nethttppprof  call init function   The first thing to do is add this import to the source code file hosting the main function This import allows the compiler to find an init function in the pprof package that sets debug routes in the default server mux Listing 1110 srcnethttppprofpprofgo func init  httpHandleFuncdebugpprof Index httpHandleFuncdebugpprofcmdline Cmdline httpHandleFuncdebugpprofprofile Profile httpHandleFuncdebugpprofsymbol Symbol httpHandleFuncdebugpproftrace Trace   Next I need to bind the default server mux to an ip address and port I want this to be separate from the application traffic so these endpoints can be protected behind a firewall Listing 1111 package main import   nethttppprof  call init function  func main  debugHost  5000    go func  logPrintfmain Debug Listening s debugHost err  httpListenAndServedebugHost httpDefaultServeMux if err  nil  logPrintfmain Debug Listener closed v err         A Goroutine is created to block on the httpListenAndServe call to handle the debug routes Since I already set this up before I ran load through the program I can use the debugpprof endpoint now to get a memory profile  225  Figure 112 httplocalhost5000debugpprof  There is so much profiling data I can extract from my running program In this case I am interested in the heap profile which is available from the very first link If I click on the allocs link I can see what a raw memory profile looks like Figure 113 httplocalhost5000debugpprofallocsdebug1  226  The raw memory profile breaks things down by stack however at the very top there is some information thats interesting Listing 1112 heap profile 7 59424 12335 286125512  heap1048576 7 59424 12335 286125512  Currently live objects Amount of memory occupied by live objects Total number of allocations Amount of memory occupied by all allocations  Those first four numbers represent program wide information for this snapshot of the memory profile Then for each stack there is similar information Listing 1113 1 40960 245 10035200  1 40960 245 10035200  Currently live objects Amount of memory occupied by live objects Total number of allocations Amount of memory occupied by all allocations  Its nice to have some understanding of the raw numbers but this is not a productive way to read the profile data  
115 Viewing Memory Profile Im going to use the profile tooling as explained in chapter 10 to explore the memory profile Whats cool is I can give the pprof tool the URL to the memory profile and it can use that to read it in Listing 1114 go tool pprof noinlines httplocalhost5000debugpprofallocs Fetching profile over HTTP from httplocalhost5000debugpprofallocs Type allocspace Time May 10 2021 at 148pm CDT Entering interactive mode type help for commands o for options pprof The problem now is I dont know what Im specifically looking for I really want to find any low hanging fruit therefore I will use the top command to get a list of the top 15 functions that are allocating the most memory 227 Listing 1115 pprof top 15 cum Showing nodes accounting for 330039MB 5367 of 614889MB total Dropped 90 nodes cum 3074MB Showing top 15 nodes out of 47 flat flat sum cum cum 0 0 0 367530MB 5977 nethttpconnserve 38568MB 627 627 365377MB 5942 githubcomardanlabsgotrainingtopicsgoprofilingprojectservicehandler 0 0 627 365377MB 5942 githubcombraintreemanners gracefulHandlerServeHTTP 0 0 627 365377MB 5942 nethttpServeMuxServeHTTP 0 0 627 365377MB 5942 nethttpHandlerFuncServeHTTP 0 0 627 365377MB 5942 nethttpserverHandlerServeHTTP 61776MB 1005 1632 305501MB 4968 githubcomardanlabsgotrainingtopicsgoprofilingprojectservicerender 4801MB 078 1710 244506MB 3976 githubcomardanlabsgotrainingtopicsgoprofilingprojectsearchrssSearch 1MB 0016 1712 243726MB 3964 githubcomardanlabsgotrainingtopicsgoprofilingprojectserviceexecuteT emplate 0 0 1712 243626MB 3962 htmltemplate TemplateExecute 250MB 0041 1716 243626MB 3962 texttemplate Templateexecute 0 0 1716 243376MB 3958 texttemplatestatewalk 223593MB 3636 5352 239353MB 3893 stringsToLower 951MB 015 5367 200699MB 3264 githubcomardanlabsgotrainingtopicsgoprofilingprojectsearchCNNSearc h 0 0 5367 193506MB 3147 fmtFprint pprof
I added the cum switch to the top command to sort the list by the cumulative value Seeing the socket connection and mux at the top of the list makes sense since all traffic flows through that code Having the template calls next makes sense because rendering HTML will produce allocations What comes next is the call to the searchrssSearch function Listing 1116 flat flat sum cum cum 4801MB 078 1710 244506MB 3976 githubcomardanlabsgotrainingtopicsgoprofilingprojectsearchrssSearch Top is telling me that this function represents 3976 of the total allocations made which is worth a total of 24 gig This is a function that I should look closer at Maybe I can find some nonproductive allocations that could be removed to reduce the amount of GC that needs to take place to complete this work If I can reduce allocations and therefore reduce the number of GCs I will get better performance I will use the list command to review the searchrssSearch function 228 Listing 1117 pprof list rssSearch Total 6GB ROUTINE projectsearchrssgo 4801MB 239GB flat cum 3976 of Total 2050MB 2050MB 79 var d Document 352MB 102 if err xmlNewDecoderrespBod 234GB 119 if stringsContainsstringsToLowerite 2751MB 2751MB 120 results appendresults Result pprof Ive trimmed down the list output to just those lines that are showing allocations Out of the 239 GB of allocation coming from this function line 119 represents 234GB of that Thats the line of code I need to focus on Listing 1118 118 for item range dChannelItems 119 if stringsContainsstringsToLoweritemDescription
stringsToLowerterm 120 results appendresults Result 121 Engine engine 122 Title itemTitle 123 Link itemLink 124 Content itemDescription 125 126 127 When I look at the code on line 119 from the searchrssgo file I see there is a call to stringsContains and stringsToLower inside a loop That loop is checking if the search term matches against each item description for one specific feed Since I have 12 feeds for each request this loop is run 12 separate times I know a large number of allocations are happening here on line 119 but how can I tell if its because of the call to stringsContains or stringsToLower Looking at a call graph that is isolated to the searchrssSearch function will help Listing 1119 pprof web rssSearch For the call graph to be generated its important to make sure Graphviz is installed This is the website for more information httpswwwgraphvizorg 229 Figure 114 The call graph makes it clear that the call to stringsToLower is causing all the allocations
116 Removing Allocations Knowing this I need to find a way to remove the call to stringsToLower Listing 1120 118 for  item  range dChannelItems  119 if stringsContainsstringsToLoweritemDescription stringsToLowerterm  120 results  appendresults Result 121 Engine engine 122 Title itemTitle 123 Link itemLink 124 Content itemDescription 125  126  127   On closer inspection there are two things I can do  230  Listing 1121 122 123 124 125 126 127 128 129 130 131 132 133  term  stringsToLowerterm for  item  range dChannelItems  if stringsContainsitemDescription term  results  appendresults Result Engine engine Title itemTitle Link itemLink Content itemDescription      ADDED  CHANGED  First I move the call to make the term lower outside of the loop Having that call in the loop was really bad since it was performing the same operation over and over That was producing a lot of little values on the heap Second I removed the call to make the description lower That still needs to take place so I moved it up with the code that caches the feeds Listing 1122 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108  resp err  httpGeturi if err  nil  return Result err  defer respBodyClose if err  xmlNewDecoderrespBodyDecoded err  nil  return Result err  for i  range dChannelItems   ADDED lower  stringsToLowerdChannelItemsiDescription dChannelItemsiDescription  lower  cacheSeturi d expiration  The new code converts the description to lowercase before storing the feed in the cache This eliminates the need to do this inside the loop Granted I should create a new field for the lowercase version of the description With these changes in place I can build and run the program again Then apply the same exact load and get the new results  231  Listing 1123 Summary Total 14367 secs Slowest 01706 secs Fastest 00005 secs Average 00131 secs Requestssec 69606340 gc 484 7989s 4 00380760025 ms clock 0620521518040 ms cpu 674 MB 7 MB goal 16 P gc 485 7991s 4 00560830094 ms clock 091048231915 ms cpu 784 MB 9 MB goal 16 P gc 486 7994s 4 00890650023 ms clock 140551812038 ms cpu 784 MB 8 MB goal 16 P gc 487 7996s 4 00410470012 ms clock 0650221520020 ms cpu 784 MB 8 MB goal 16 P  With the code changes there are definite improvements Now it seems that only 483 GCs needed to take place to complete the work Im subtracting the first 4 GCs again that took place before I ran the load The program was able to process 6960 requests a second with the slowest request taking 170ms Thats an improvement of 88 on the requests per second If I wanted to focus more on a specific function that the top command might present I can go back to chapter 10 and use benchmarks  232  
Chapter 12 Tracing In this chapter I will be learning how to capture a trace of a running program and view it with the trace tool In this exercise I will be generating a trace using the standard library However a trace can also be generated when running a benchmark or by using the http debugpprof endpoint  
121 Example Code I have a program that counts the frequency a topic is found in a collection of RSS news feed documents This program uses a single threaded algorithm named freq that iterates over the collection processing each document one at a time and returns the number of times the topic is found Listing 121 func freqtopic string docs string int var found int for doc range docs file fmtSprintfsxml doc8 f err osOpenFilefile osORDONLY 0 if err nil logPrintfOpening Document s ERROR v doc err return 0 data err ioReadAllf fClose if err nil logPrintfReading Document s ERROR v doc err return 0 var d document if err xmlUnmarshaldata d err nil logPrintfDecoding Document s ERROR v doc err return 0 for item range dChannelItems if stringsContainsitemTitle topic found continue if stringsContainsitemDescription topic found return found The freq function breaks the work down into four stages opening reading unmarshaling and search To test the freq function the main function constructs a 233 collection of 4k files and calls freq Listing 122 type item struct XMLName xmlName xmlitem Title string xmltitle Description string xmldescription channel struct XMLName xmlName xmlchannel Items item xmlitem document struct XMLName xmlName xmlrss Channel channel xmlchannel func main docs makestring 4000 for i range docs docsi fmtSprintfnewsfeed4dxml i topic president n freqtopic docs logPrintfSearching d files found s d times lendocs topic n The code that constructs the slice of 4k documents is creating a pretend set of unique files based on a real file
I have named newfeedxml Listing 123 newsfeedxml xml version10 encodingUTF8 xmlstylesheet titleXSLformatting typetextxsl rss channel titleCDATABBC News US Canadatitle descriptionCDATABBC News US Canadadescription item titleCDATAPresident China visit US leader striktitle descriptionCDATAThe US president praisesdescription item channel rss This is a very small sample of the newsfeedxml file Instead of keeping 4k actual files I keep just one file and pretend it represents 4k of them The freq algorithm strips the extra characters from the file name before opening the file It will be good to get an initial idea of how long it takes to process these 4000 files with the single threaded version of freq I can do that using the time command in 234 conjunction with running the program Listing 124 go build time trace 20210512 093052 Searching 4000 files found president 28000 times trace 263s user 018s system 101 cpu 2763 total I can see that the program took 27 second to process the 4k files If I was only going to process 4k files or maybe even a few thousand more Id say this program is done However I would like the ability to process a million files and not have it take hours I need to figure out a way to speed up this program I could use a memory profile but I know this program uses a lot of transient memory and there isnt much I can do about that I could use a cpu profile but trust me its going to tell me
Im spending most of my time in system calls thanks to the call to osOpenFile The problem with using a profiler here is that a profiler can only tell me what is happening I need to know what is and isnt happening to find a way to speed up this program When I need to see what is not happening the trace tooling is a good option
122 Generating Traces Since this program starts and stops within a few seconds I can use the standard library to generate a trace of this program In general creating traces for more than a few seconds of run time can be overwhelming to review since a trace generates a large amount of data I want to focus on small targeted traces Listing 125 import  runtimetrace  func main  traceStartosStdout defer traceStop   ADDED   ADDED  ADDED  docs  makestring 4000 for i  range docs  docsi  fmtSprintfnewsfeed4dxml i     topic  president n  freqtopic docs logPrintfSearch d files found s d times lendocs topic n  After adding the trace package into the imports I can use the traceStart and Stop 235  functions Writing the trace to stdout makes it simple to capture and redirect the trace data to a file Now I can run the program again Listing 126  go build  time trace  tout 20210512 115706 Search 4000 files found president 28000 times trace  tout 271s user 019s system 102 cpu 2827 total  time trace  tout 20210512 115711 Search 4000 files found president 28000 times trace  tout 273s user 018s system 108 cpu 2683 total  ls l total 9136 rwrrrwrrrwrrrwxrxrx rwrr  1 bill 1 bill 1 bill 1 bill 1 bill  staff staff staff staff staff  2108 Jan 4 0816 READMEmd 25544 Jan 4 0816 newsfeedxml 1501618 May 12 1157 tout 2470208 May 12 1157 trace 8135 May 12 1156 tracego  I always run the program twice to make sure the machine is warmed up The second run usually runs better than the first and I can see a 144ms difference between these two runs I can also see the tout file in the listing has captured 15 meg of data over the 26 seconds of run time  
123 Viewing Traces To review the trace I need to use the trace tool Listing 127 go tool trace tout 20210512 120019 Parsing trace 20210512 120019 Splitting trace 20210512 120019 Opening browser Trace viewer is listening on http12700164321 Once the file is parsed and split a browser tab is opened with a set of links 236 Figure 121 I want to choose the first link which will open a Chrome based UI that can present the tracing data This tooling only works in Chrome Figure 122 All of the information that is presented is recorded down to the microsecond of when it happened On the left hand side I can see the different categories of graphed information 237 At any given time in the trace Goroutines The number of Goroutines Heap The amount of memory inuse on the heap Threads The number of operating system threads GC The startend of each GC with details Syscalls The startend of any system call with details Procs The activity on each logical processor The first thing I should do is measure how long the program ran based on the trace data I can do that with the timing tool which is the last tool in the toolbar Figure 123 If I select the timeline for the entire trace I get 267 seconds which is fairly close to what the time command provided The next thing I want to know is how consistent the size of the heap was maintained and how large the
heap grew I will expand the view of the graph and then use the selection tool first tool in the toolbar to select the top portion of the heap graph 238 Figure 124 There are two colors or shades that are represented in the heap graph Orange the darker shade represents the amount of memory inuse on the heap at each microsecond Green the lighter shade represents when the next GC will be triggered based on when that amount of memory is once again inuse on the heap Figure 125 This is why when the orange area reaches the top of the green area there is a line in the GC section Each line represents a GC that took place BTW the dot is the selection point for the heap information listed in the figure Since the green area is in a consistent straight line for the entire run of the program I can select any orange point to see the max size of the heap In this case 239 that is 4 meg of memory If I want more clarity on this number I can use GODEBUG and run the program again Listing 128 time GODEBUGgctrace1 trace tout gc 1 0016s 0 00200290024 ms clock 032018012012039 ms cpu 440 MB 5 MB goal 16 P gc 2 0029s 0 00300230002 ms clock 04901501700010037 ms cpu 440 MB 5 MB goal 16 P gc 3 0040s 0 00330260019 ms clock 0520150150059031 ms cpu 440 MB 5 MB goal 16 P
gc 273 2793s 0 00430460040 ms clock 070024018025065 ms cpu 440 MB 5 MB goal 16 P gc 274 2804s 0 00430350002 ms clock 0690280370160042 ms cpu 440 MB 5 MB goal 16 P gc 275 2814s 0 00320280017 ms clock 0520200260026028 ms cpu 440 MB 5 MB goal 16 P 20210512 151724 Searching 4000 files found president 28000 times Sure enough the heap was maintained at 4 meg and all the memory was transient since each GC resulted in the number of live values to be zero Exactly what I am seeing in the trace What I really need is more information about the GCs Its not about 1 GC its about all the GCs that needed to take place I can get this information by using the selection tool and double clicking on any blue GC line Figure 126 These numbers are helpful in determining how much work the GC is doing in the execution of the program With these numbers I have a baseline for the programs performance 240 Table 121 Single Runtime Top Memory 2670ms 4 Meg GC Occurrences 275 GC Avg Duration 387us GC Wall Duration 106ms GC Time Spent 4 With the baseline numbers I still dont have an answer as to how I could speed up the program Maybe looking at a larger portion of the graph will help Figure 127 If I look closer at this section of the trace I can see the problem The program is only using one logical
processor at any given time to execute the application work The GC however is using more If I change the algorithm to be concurrent and friendly to using all the available cores on my machine this should help with the performance
124 FanOut One concurrency pattern I could use is a fanout pattern This would allow me to have a concurrent algorithm with the least amount of code change The idea of this pattern is to create a Goroutine for each file that needs to be processed and let the scheduler manage all the Goroutine scheduling them across all the logical processors I can fanout all these Goroutines because the order of the work doesnt matter What matters is all the work is performed 241 Listing 129 func freqConcurrenttopic string docs string int var found int g lendocs var wg syncWaitGroup wgAddg ADDED ADDED ADDED for doc range docs wgWait return int ADDED To start I need to add code that makes sure the freq function doesnt return until all the work is complete This is best implemented by using a WaitGroup I start with identifying how many Goroutines need to be created which is represented by the length of the docs collection Then I add that number to the WaitGroup and at the end of the function wait for the WaitGroup to get back down to zero Listing 1210 func freqConcurrenttopic string docs string int var found int g lendocs var wg syncWaitGroup wgAddg for doc range docs go funcdoc string defer wgDone doc ADDED ADDED ADDED wgWait return int Next I wrap the existing code inside the for range loop around a Goroutine where each document iterated over is passed into the new Goroutine to prevent any closure bugs
With the Goroutine in place I can finalize the WaitGroup code by making sure each Goroutine calls wgDone right before it terminates The last thing I need to do is remove the return value of 0 from all the error conditions inside the loop when processing a file I might think this code is ready since all the orchestration with the WaitGroup is in place but what happens if I run this code with the race detector 242 Listing 1211 go build race trace tout WARNING DATA RACE Read at 0x00c000157548 by goroutine 21 mainfreqfunc1 Usersbillcodegosrcgithubcomardanlabsgotrainingtopicsgo profilingtracetracego103 0x644 Previous write at 0x00c000157548 by goroutine 66 mainfreqfunc1 Usersbillcodegosrcgithubcomardanlabsgotrainingtopicsgo profilingtracetracego103 0x65d Looks like I have a data race The first two lines in the output says there is a read and a previous write to the same memory location on the same line of code inside the literal function What code is on line 103 Listing 1212 96 for item range dChannelItems 97 if stringsContainsitemTitle topic 98 found 99 continue 100 101 102 if stringsContainsitemDescription topic 103 found 104 105 If I add the line numbers to the code I can see its related to the increment of the found variable This code is actually used again on line 98 so both lines of code 98 103 contain a data race To fix this data race I need to change the increment of the found variable to use an atomic instruction To do this I can use the atomic package Listing 1212
96 for item range dChannelItems 97 if stringsContainsitemTitle topic 98 atomicAddInt32found 1 99 continue 100 101 102 if stringsContainsitemDescription topic 103 atomicAddInt32found 1 104 105 CHANGED CHANGED I have replaced lines 98 and 103 to use the atomicAddInt32 function The problem 243 is this function requires an int32 not an int so I need to make two more changes Listing 1213 func freqtopic string docs string int var found int32 CHANGED wgWait return intfound CHANGED These changes are necessary to use the atomicAddInt32 function and still return an integer With these changes the data race will be gone However there is still another problem with the code Its not as cache friendly as it otherwise could be
125 Cache Friendly The problem is with the found variable Its technically a global variable to the 4k Goroutines that will be accessing that memory Though using the atomic package helps to serialize access each core on my machine that is executing Goroutines will receive and operate on its own copy of the found variable The moment any copy inside of any core is incremented it will mark all the other copies in all the other cores dirty When the next Goroutine goes to increment their copy and that copy is marked dirty a fresh copy needs to be pulled from main memory This will result in the thrashing of memory slowing everything down Over 8 cores and 4k files this thrashing may not result in much of a performance hit When I run this on 128 cores over 1MM files this thrashing will take its toll To alleviate the thrashing I need each Goroutine to use a local variable for counting the number of times the term is found However before the Goroutine terminates it must add the result of its local variable to the global variable For the current algorithm that means we reduce the thrashing from 28k times to 4k times Not perfect but much better  244  Listing 1214 func freqtopic string docs string int  var found int32 g  lendocs var wg syncWaitGroup wgAddg for  doc  range docs  go funcdoc string  var lFound int32 defer func  atomicAddInt32found lFound wgDone    ADDED  CHANGED  MOVED  MOVED     for  item  range dChannelItems  if stringsContainsitemTitle topic  lFound  CHANGED continue        doc  if stringsContainsitemDescription topic  lFound  CHANGED   wgWait return intfound  I added a new variable named lFound to represent a found variable that is local to every Goroutine This means that I can remove the atomicAddInt32 calls from inside the loop that searches for topic matches Since I still have to update the global found variable I do that inside the defer before I call wgDone  
126 FanOut Results With these changes in place I can try the new fanout based concurrent algorithm Listing 1215  go build  time trace  tout 20210513 121051 Search 4000 files found president 28000 times trace  tout 730s user 071s system 1106 cpu 0725 total  time trace  tout 20210513 121053 Search 4000 files found president 28000 times trace  tout 721s user 076s system 1297 cpu 0615 total  I can already see from the time command a performance improvement of 2 seconds Now I can look at the trace  245  Figure 128  This is a good sign that all of the cpu capacity on my machine is being used If I scroll down I will see all 16 logical processors are in use I need to gather all the measurements for this algorithm Table 122 Single  FanOut  2670ms  580ms  4 Meg  53 Meg  GC Occurrences  275  62  GC Avg Duration  387us  4ms  GC Wall Duration  106ms  250ms  4  43  Runtime Top Memory  GC Time Spent  I can see there is a 2 second performance improvement but that came with an order of magnitude more memory and time spent in GC However the number of GCs was reduced by 113 because of the larger memory profile There are a few other interesting aspects of this trace  246  Figure 129  I am looking at the first 45ms of time where all 4k Goroutines had a chance to run and be put into a waiting state by the scheduler I can see how dense the context switches are and how the Mark Setup STW Sweep Termination phase starts in the middle of the GC It means that it took 19ms before the GC attempted to stop all the application work so the marking phase could be started In the top area of the graph that shows Goroutines I can see large peeks of Goroutines in running and runnable states throughout the first GC If I were to expand a part of this view I will see the context switches more clearly Figure 1210  247  As I expand the graph the context switches become clearly defined All of these Goroutines are going from runnable running and then waiting If I look at the call graph for Goroutine G2111 I can see that when the Goroutine made the call to runtimemallocgc the scheduler identified a GC was running and the Goroutine was context switched Figure 1211  On the backend of the first GC at the 39ms mark the heap is at 49 meg If I look closely I can see all the Goroutines in mark assist to help slowdown allocations and get the initial GC finished Once this first GC is complete the heap is at 52 meg and the collector maintains that heap size for the rest of the running program  
127 Pooling For 4k files over 8 cores the program is running well using a fanout However what if I want to process 1MM files I dont feel great about creating 1MM Goroutines Plus I want to be more efficient with resources especially since I cant open 1MM files at the same time I can change the algorithm to use a pooling pattern and limit the number of Goroutines and files being processed at any time  248  Listing 1216 func freqtopic string docs string int  var found int32 g  runtimeGOMAXPROCS0 var wg syncWaitGroup wgAddg   CHANGED  ch  makechan string g   ADDED  for i  0 i  g i   CHANGED go func   CHANGED var lFound int32 defer func  atomicAddInt32found lFound wgDone     for doc  range ch        ADDED  CHANGED         The first change is to set the number of Goroutines to use to match the number of Goroutines that can be run in parallel The runtimeGOMAXPROCS function when passing zero is the best way to get that number Using more Goroutines in a situation like this rarely improves the performance in any significant way Next a channel is added to have the ability to signal work into the pool The size of the buffer matches the number of Goroutines since staging more work than their are Goroutines doesnt add value The for loop is changed to reflect only creating g Goroutines I dont need to pass a document into the Goroutine anymore since thats coming from the channel The defer function requires no changes Inside the Goroutine a forrange loop is added for receiving documents through the channel The existing code to process a document is placed inside the channel loop and doesnt need to change Outside the for loop that creates the pool of Goroutines is a second loop to feed the documents into the pool  249  Listing 1217 func freqtopic string docs string int     for i  0 i  g i      for  doc  range docs  ch  doc  closech     ADDED  ADDED  ADDED  ADDED  wgWait return intfound  While the pool of Goroutines is waiting for work this second loop begins to signal work into the pool Once the last signal is sent the channel is closed and then freq waits for all the Goroutines to process the remaining work and report they are done One big benefit to this version of freq is its more sympathetic with the caching system Listing 1218 func freqtopic string docs string int     for i  0 i  g i  go func  var lFound int32 defer func  atomicAddInt32found lFound wgDone             Regardless of the number of documents to process the defer function only needs to execute g number of times On my machine that is 16 times All of the counting happens on the local lFound variable eliminating atomic synchronization and cache thrashing  
128 Pooling Results With the more efficient pooling algorithm in place I should see better performance  250  Listing 1219  go build  time trace  tout 20210513 140711 Searching 4000 files found president 28000 times trace  tout 809s user 175s system 834 cpu 1179 total  time trace  tout 20210513 140713 Searching 4000 files found president 28000 times trace  tout 813s user 176s system 935 cpu 1057 total  Surprisingly this pooling version is slower than the fanout by 400ms Time to look at the trace and gather the numbers Figure 1212  On the surface this trace looks better with the number of Goroutines the heap and utilizing the full cpu capacity of the machine Figure 1213  251  A closer look at the first 30ms of the trace tells a different story The memory looks consistent right from the beginning and the context traces look uniform but there appears to be a lot of GC happening Table 123 Single  FanOut  Pooling  2670ms  580ms  1000ms  4 Meg  53 Meg  5 Meg  GC Occurrences  275  62  876  GC Avg Duration  387us  4ms  690us  GC Wall Duration  106ms  250ms  604ms  4  43  60  Runtime Top Memory  GC Time Spent  After gathering the numbers its clear the GC is working very hard with this version of the algorithm The reason seems to be related to the amount of memory the GC held the program to which is 5 meg Since there are only 16 Goroutines doing the work the GC was able to keep the memory small but at the cost of performance Would the performance change if I told the GC its ok for the program to use 50 meg of memory This is the amount of memory the fanout algorithm uses  
129 GC Percentage There is a knob I can use to ask the GC to use more memory This is the GC percentage knob I can access this knob in two ways first with the GOGC environment variable and second with the debug package in the standard library The default value for this knob is 100 which represents two important things First that the GC should start when 4 meg of memory is inuse on the heap Second that the next GC should start when 100 more memory is allocated on the heap based on the result of the marked live value from the previous GC Since the GC is keeping the memory at roughly 4 meg I need to get the GC to not start the first collection until the memory inuse reaches 40 meg To do this I need to change the GOGC value by an order of magnitude which is 1000  252  Listing 1219  time GOGC1000 trace  tout 20210513 144306 Searching 4000 files found president 28000 times GOGC1000 trace  tout 548s user 029s system 1404 cpu 0411 total  time GOGC1000 trace  tout 20210513 144308 Searching 4000 files found president 28000 times GOGC1000 trace  tout 541s user 030s system 1370 cpu 0417 total  After using the GOGC environment variable setting the GC percentage to 1000 I see the performance I was expecting The performance of this algorithm is faster than the fanout Figure 1214  That looks much better plus the heap graph looks similar to the single threaded version Now I need to gather the numbers Table 124 Single  FanOut  Pooling  GC 1000  2670ms  580ms  1000ms  397ms  4 Meg  53 Meg  5 Meg  40 Meg  GC Occurrences  275  62  876  27  GC Avg Duration  387us  4ms  690us  706us  GC Wall Duration  106ms  250ms  604ms  19ms  4  43  60  4  Runtime Top Memory  GC Time Spent  Brilliant By running with a 40 meg heap the number of GCs went down considerably and the amount of time spent in GC is back down to 4 253  I would like to make sure this program always runs with the GC Percentage set to 1000 That requires a simple code change Listing 1220 import  runtimedebug  func main  debugSetGCPercent1000      With the call to debugSetGCPercentage I can guarantee the program always runs with the larger heap Be careful when hardcoding this value since its favoring memory usage over GC Different workloads could affect this decision in negative ways  
1210 Tasks And Regions With the pooling version of freq working pretty well it would be interesting to gather information about each individual file that the program is processing Maybe there is a file or two that is causing the algorithm to not run as fast as it could With the use of tasks and regions I can find out Listing 1221 for i  0 i  g i  go func     for doc  range ch  ctx task  traceNewTaskcontextBackground doc      The first thing to do is to create a task for each file that is being processed Here I am adding the call to traceNewTask giving it an empty parent context and using the document for the name of the task That call returns a new context and a task The next step is to add regions around the blocks of code I want to measure  254  Listing 1222 for doc  range ch  ctx task  traceNewTaskcontextBackground doc reg  traceStartRegionctx OpenFile  ADDED file  fmtSprintfsxml doc8 f err  osOpenFilefile osORDONLY 0 if err  nil  logPrintfOpening Document s  ERROR  v doc err return  regEnd  ADDED reg  traceStartRegionctx ReadAll  ADDED data err  ioReadAllf fClose if err  nil  logPrintfReading Document s  ERROR  v doc err return  regEnd  ADDED reg  traceStartRegionctx Unmarshal  ADDED var d document if err  xmlUnmarshaldata d err  nil  logPrintfDecoding Document s  ERROR  v doc err return  regEnd  ADDED reg  traceStartRegionctx Contains for  item  range dChannelItems  if stringsContainsitemTitle topic  lFound continue    ADDED  if stringsContainsitemDescription topic  lFound      regEnd taskEnd   ADDED  ADDED  I added four regions each named for the operation that is being performed At the end of the last region I call taskEnd to end the task With this in place I can run the program again and open the trace output Listing 1223  go build  time trace  tout 20210517 123248 Search 4000 files found president 28000 times trace  tout 533s user 029s system 1292 cpu 0435 total  go tool trace tout  255  Once I do that I can use the Userdefined tasks link to see the data Figure 1215  After selecting that link I get this view Figure 1216  I now have a table with an entry for every document that was processed including how long it took to process that file If I click on the time I get to this page  256  Figure 1217  This page breaks down each region from this task and how long it took It also provides information about GC time at the bottom That final link gives me this same information but in graph form Figure 1218  Now I can see when the file was processed and more information from the tracing tool  257  
Chapter 13 Stack Traces  Core Dumps In this chapter I will learn how to read stack traces and generate core dumps Understanding the information in a stack trace can sometimes mean the difference between finding the bug now or needing to add more logging and waiting for it to happen again  
131 ABI Changes In 117 As of version 117 Go changed the ABI application binary interface to implement a new way of passing function input and output arguments using registers instead of memory on the stack This is enabled for Linux MacOS and Windows on the 64bit x86 architectures This means that some function arguments wont be copied on the stack but some may depending on the viability of using registers Because of this ABI change stack trace output has changed in 117 and function input values that are placed in registers wont appear accurately in the stack trace anymore The format of how input values are displayed has changed as well Accuracy is something the Go team is looking to improve on in future releases of Go For now this chapter will focus on how to read a stack trace in version 116 or less I have added a section to show the changes of the stack output and how they are no longer accurate  
132 Basic Example I will start with stack traces and learn how to read all the information that is presented Here is a program that uses the builtin function panic to cause a stack trace Listing 131 01 package main 02 03 func main  04 examplemakestring 2 4 hello 10 05  06 07 gonoinline 08 func exampleslice string str string i int error  09 panicWant stack trace 10   After running the program the following stack trace is presented  258  Listing 132  go run example1go panic Want stack trace goroutine 1 running mainexample0xc000054738 0x2 0x4 0x1073c53 0x5 0xa 0x0 0xc000054778 gotrainingtopicsgoprofilingstacktraceexample1example1go9 0x39 mainmain gotrainingtopicsgoprofilingstacktraceexample1example1go4 0x85 exit status 2  The line numbers to where the Goroutines were in the call stack prior to the panic is something I can figure out fairly quickly Listing 133  go run example1go panic Want stack trace mainexample example1go9 0x39 mainmain example1go4 0x85  If I get rid of some of the noise and focus on this information Im being told that the Goroutine started on line 4 in package main function main From there the goroutine executed code on line 9 in package main function example That is where the panic occurred with a message that I wanted a stack trace This is great but there is more information that isnt obvious Listing 134 mainexample0xc000054738 0x2 0x4 0x1073c53 0x5 0xa 0x0 0xc000054778  In the stack trace the call to mainexample looks like a function call with parameters The stack trace shows words of data that are passed into and out of each function The first words are the input and the remaining words are the output In a panic situation the words representing the output are technically garbage However the input is not If I look at the call to mainexample again I can see what values are being passed in  259  Listing 135 01 package main 02 03 func main  04 examplemakestring 2 4 hello 10 05  06 07 gonoinline 08 func exampleslice string str string i int error  09 panicWant stack trace 10   There is a slice value with a length of 2 and capacity of 4 a string with a length of 5 bytes and the number 10 If I look again at the values inside the parentheses I should be able to match them up Listing 136 examplemakestring 2 4 hello 10 example0xc000054738 0x2 0x4  0x1073c53 0x5  0xa  0x0 0xc000054778 Slice Value 0xc000054738 0x2 0x4 String Value 0x1073c53 0x5 Integer Value 0xa Return Value 0x0 0xc000054778 interface  As I can see the first three words represent the three words of the slice value the next two words represent the words of the string value the next word represents the integer finally the last two words represent the interface value I can see all the values being passed into the function Since this is panicking the return values are garbage  
133 Word Packing Like I said the stack trace shows word of data at a time What if multiple parameters can fit inside a word of data Listing 137 01 package main 02 03 func main  04 exampletrue false true 25 05  06 07 gonoinline 08 func exampleb1 b2 b3 bool i uint8 error  09 panicWant stack trace 10   In this example four values are passed into the example function three boolean values and an unsigned 1 byte integer All four values can be stored inside of a single word of data This is what the stack trace looks like when the program is run 260  Listing 138  go run example2go panic Want stack trace goroutine 1 running mainexample0x19010001 0x1064ee0 0xc00008c058 gotrainingtopicsgoprofilingstacktraceexample2example2go9 0x39 mainmain gotrainingtopicsgoprofilingstacktraceexample2example2go4 0x29 exit status 2  In this stack trace I only see a single word value for the input and two word values for the return interface argument If I read the digits of the first word value from right to left for little endian and remember every two digits represent a byte of data I know what values were passed in Listing 139  Word value 0xc019010001 Bits 0007 0815 1623 2431  Binary 0000 0001 0000 0000 0000 0001 0001 1001  Hex 01 00 01 19  Value true false true 25  Return Arguments 0x1064ee0 0xc00007605  Byte packaging is nice to reduce the amount of noise in the stack trace  
134 Go 117 ABI Changes I will run the first program again using both Go 116 and Go 117 to see the difference in the stack trace output  261  Listing 1310  go116 run example1go panic Want stack trace goroutine 1 running mainexample0xc000054738 0x2 0x4 0x1073c53 0x5 0xa 0x0 0xc000054778 Usersbillcodegosrcgithubcomardanlabsgotrainingtopicsgo profilingstacktraceexample1example1go13 0x39 mainmain Usersbillcodegosrcgithubcomardanlabsgotrainingtopicsgo profilingstacktraceexample1example1go8 0x85 exit status 2  go117 run example1go panic Want stack trace goroutine 1 running mainexample0x60 0x10bb6c0 0xc0000002e8 0xc000024060 0x0 0xc0000001a0 Usersbillcodegosrcgithubcomardanlabsgotrainingtopicsgo profilingstacktraceexample1example1go13 0x27 mainmain Usersbillcodegosrcgithubcomardanlabsgotrainingtopicsgo profilingstacktraceexample1example1go8 0x59 exit status 2  The first difference I see is how in 117 each individual value is broken into a document like syntax Listing 1311 Go 116 mainexample0xc000054738 0x2 0x4 0x1073c53 0x5 0xa 0x0 0xc000054778 Slice 0xc000054738 0x2 0x4 String 0x1073c53 0x5 Int 0xa Go 117 mainexample0x60 0x10bb6c0 0xc0000002e8 0xc000024060 0x0 0xc0000001a0 Slice 0x60 0x10bb6c0 0xc0000002e8 String 0xc000024060 0x0 Int 0xc0000001a0  However if I look closer I see the values are not accurate in Go 117 Since these input values are now being passed using registers when the stack trace occurs the values are not available for proper display This is a bummer but I expect this to 262  improve over time  
135 Generating Core Dumps I can generate a core dump from a running Go program which is useful if I feel the program is hung or not responding Listing 1312 01 package main 02 03 import  04 encodingjson 05 log 06 nethttp 07  08 09 func main  10 httpHandleFuncsendjson sendJSON 11 12 logPrintlnlistener  Started  Listening on httplocalhost8080 13 httpListenAndServe8080 nil 14  15 16  sendJSON returns a simple JSON document 17 func sendJSONrw httpResponseWriter r httpRequest  18 u  struct  19 Name string 20 Email string 21  22 Name bill 23 Email billardanlabscom 24  25 26 rwHeaderSetContentType applicationjson 27 rwWriteHeader200 28 jsonNewEncoderrwEncodeu 29   Now I can build and run the program Listing 1313  go build  example3 20210517 140905 listener  Started  Listening on httplocalhost4000  Next I can send a signal to quit the process Listing 1314  ps 6575 ttys002  kill 3 6575  00001 example3  This will result in the Go program producing a core dump  263  Listing 1315 SIGQUIT quit PC0x7fff20533c62 m0 sigcode0 goroutine 0 idle runtimekevent0x4 0x0 0x0 0x7ffeefbfee78 0x40 0x0 0x0 usrlocalgosrcruntimesysdarwingo349 0x39 runtimenetpoll0xffffffffffffffff 0x0 usrlocalgosrcruntimenetpollkqueuego127 0xae runtimefindrunnable0xc000028000 0x0 usrlocalgosrcruntimeprocgo2923 0x3ee runtimeschedule usrlocalgosrcruntimeprocgo3169 0x2d7 runtimeparkm0xc000000180 usrlocalgosrcruntimeprocgo3318 0x9d runtimemcall0x106a196 usrlocalgosrcruntimeasmamd64s327 0x5b goroutine 1 IO wait internalpollruntimepollWait0x2555eb8 0x72 0x0 usrlocalgosrcruntimenetpollgo222 0x55 internalpollpollDescwait0xc000026098 0x72 0x0 0x0 0x12a6473 usrlocalgosrcinternalpollfdpollruntimego87 0x45 internalpollpollDescwaitRead usrlocalgosrcinternalpollfdpollruntimego92 internalpollFDAccept0xc000026080 0x0 0x0 0x0 0x0 0x0 0x0 0x0 usrlocalgosrcinternalpollfdunixgo401 0x212 netnetFDaccept0xc000026080 0x30 0x30 0x1544108 usrlocalgosrcnetfdunixgo172 0x45 netTCPListeneraccept0xc00000e150 0xc0000b1d88 0x100f578 0x30 usrlocalgosrcnettcpsockposixgo139 0x32 netTCPListenerAccept0xc00000e150 0x12895c0 0xc000094db0 0x125d980 0x144ac50 usrlocalgosrcnettcpsockgo261 0x65 nethttpServerServe0xc0000e6000 0x12fd100 0xc00000e150 0x0 0x0 usrlocalgosrcnethttpservergo2981 0x285 nethttpServerListenAndServe0xc0000e6000 0xc0000e6000 0x1 usrlocalgosrcnethttpservergo2910 0xba nethttpListenAndServe usrlocalgosrcnethttpservergo3164 mainmain gotrainingtopicsgoprofilingstacktraceexample3example3go13 0xd6 rax 0x4 rbx 0x7ffeefbfee00 rcx 0x7ffeefbfed48 rdx 0x0 rdi 0x4 rsi 0x0 rbp 0x7ffeefbfed50 rsp 0x7ffeefbfed48 r8 0x40 r9 0x0 r10 0x7ffeefbfee78 r11 0x206 r12 0x0 r13 0x14575e0 r14 0x12f243f r15 0x0 rip 0x7fff20533c62 rflags 0x207 cs 0x7 fs 0x0 gs 0x0  264  Now I have a stack trace of every Goroutine and register information I can get a larger core dump by using the GOTRACEBACK variable with the crash option when running the program Listing 1316  GOTRACEBACKcrash example3  This will provide stack traces for the runtime Goroutines like the netpoller and scavenger  265  
Chapter 14 Blog Posts This chapter contains the material on the blog posts that are listed in the book for convenience  
141 Stacks And Pointer Mechanics Prelude As of version 117 Go changed the ABI application binary interface to implement a new way of passing function input and output arguments using registers instead of memory on the stack This is enabled for Linux MacOS and Windows on the 64bit x86 architectures This means that some function arguments wont be copied on the stack but some may depending on the viability of using registers This doesnt change any of the semantics described in this post Introduction Im not going to sugar coat it pointers are difficult to comprehend When used incorrectly pointers can produce nasty bugs and even performance issues This is especially true when writing concurrent or multithreaded software Its no wonder so many languages attempt to hide pointers away from programmers However if youre writing software in Go there is no way for you to avoid them Without a strong understanding of pointers you will struggle to write clean simple and efficient code Frame Boundaries Functions execute within the scope of frame boundaries that provide an individual memory space for each respective function Each frame allows a function to operate within their own context A function has direct access to the memory inside its frame through the frame pointer but access to memory outside its frame requires indirect access For a function to access memory outside of its frame that memory must be shared with the function The mechanics and restrictions established by these frame boundaries need to be understood
and learned first When a function is called there is a transition that takes place between two frames The code transitions out of the calling functions frame and into the called functions frame If data is required to make the function call then that data must be transferred from one frame to the other The passing of data between two frames is done by value in Go The benefit of passing data by value is readability The value you see in the function call is what is copied and received on the other side Its why I relate pass 266 by value with WYSIWYG because what you see is what you get All of this allows you to write code that does not hide the cost of the transition between the two functions This helps to maintain a good mental model of how each function call is going to impact the program when the transition takes place Look at this small program that performs a function call passing integer data by value Listing 1 01 package main 02 03 func main 04 05 Declare variable of type int with a value of 10 06 count 10 07 08 Display the value of and address of count 09 printlncounttValue Of count tAddr Of count 10 11 Pass the value of the count 12 incrementcount 13 14 printlncounttValue Of count tAddr Of count 15 16 17 gonoinline 18 func incrementinc int 19 20 Increment the value of inc 21 inc 22 printlninctValue
Of inc tAddr Of inc 23 When your Go program starts up the runtime creates the main Goroutine to start executing all the initialization code including the code inside the main function A Goroutine is a path of execution that is placed on an operating system thread that eventually executes on some core As of version 18 every Goroutine is given an initial 2048 byte block of contiguous memory which forms its stack space This initial stack size has changed over the years and could change again in the future The stack is important because it provides the physical memory space for the frame boundaries that are given to each individual function By the time the main Goroutine is executing the main function in Listing 1 the Goroutines stack at a very high level would look like this 267 Figure 1 You can see in Figure 1 a section of the stack has been framed out for the main function This section is called a stack frame and its this frame that denotes the main functions boundary on the stack The frame is established as part of the code that is executed when the function is called You can also see the memory for the count variable has been placed at address 0x10429fa4 inside the frame for main There is another interesting point made clear by Figure 1 All stack memory below the active frame is invalid but memory from the active frame to the top of the stack is
valid I need to be clear about the boundary between the valid and invalid parts of the stack Addresses Variables serve the purpose of assigning a name to a specific memory location for better code readability and to help you reason about the data you are working with If you have a variable then you have a value in memory and if you have a value in memory then it must have an address On line 09 the main function calls the builtin function println to display the value of and address of the count variable 268 Listing 2 09 printlncounttValue Of count tAddr Of count The use of the ampersand operator to get the address of a variables location is not novel other languages use this operator as well The output of line 09 should be similar to the output below if you run the code on a 32bit architecture like the playground Listing 3 count Value Of 10 Addr Of 0x10429fa4 Function Calls Next on line 12 the main function makes a call into the increment function Listing 4 12 incrementcount Making a function call means the Goroutine needs to frame a new section of memory on the stack However things are a bit more complicated To successfully make this function call data is expected to be passed across the frame boundary and placed into the new frame during the transition Specifically an integer value is expected to be copied and passed during the call You can see
this requirement by looking at the declaration of the increment function on line 18 Listing 5 18 func incrementinc int If you look at the function call to increment again on line 12 you can see the code is passing the value of the count variable This value will be copied passed and placed into the new frame for the increment function Remember the increment function can only directly read and write to memory within its own frame so it needs the inc variable to receive store and access its own copy of the count value being passed Just before the code inside the increment function starts executing the Goroutines stack at a very high level would look like this 269 Figure 2 You can see the stack now has two frames one for main and below that one for increment Inside the frame for increment you see the inc variable and it contains the value of 10 that was copied and passed during the function call The address of the inc variable is 0x10429f98 and is lower in memory because frames are taken down the stack which is just an implementation detail that doesnt mean anything Whats important is that the Goroutines took the value of count from within the frame for main and placed a copy of that value within the frame for increment using the inc variable The rest of the code inside of increment increments and displays the value of and address of the inc variable
270 Listing 6 21 22 inc printlninctValue Of inc tAddr Of inc The output of line 22 on the playground should look something like this Listing 7 inc Value Of 11 Addr Of 0x10429f98 This is what the stack looks like after the execution of those same lines of code Figure 3 After lines 21 and 22 are executed the increment function returns and control goes back to the main function Then the main function displays the value of and address of the local count variable again on line 14 Listing 8 14 printlncounttValue Ofcount tAddr Of count The full output of the program on the playground should look something like this 271 Listing 9 count inc count Value Of 10 Value Of 11 Value Of 10 Addr Of 0x10429fa4 Addr Of 0x10429f98 Addr Of 0x10429fa4 The value of count in the frame for main is the same before and after the call to increment Function Returns What actually happens to the memory on the stack when a function returns and control goes back up to the calling function The short answer is nothing This is what the stack looks like after the return of the increment function Figure 4 The stack looks exactly the same as Figure 3 except the frame associated with the increment function is now considered to be invalid memory This is because the frame for main is now the active frame The memory that was framed for the increment function is left untouched 272 It
would be a waste of time to clean up the memory of the returning functions frame because you dont know if that memory will ever be needed again So the memory is left the way it is Its during each function call when the frame is taken that the stack memory for that frame is wiped clean This is done through the initialization of any values that are placed in the frame Because all values are initialized to at least their zero value stacks clean themselves properly on every function call Sharing Values What if it was important for the increment function to operate directly on the count variable that exists inside the frame for main This is where pointers come in Pointers serve one purpose to share a value with a function so the function can read and write to that value even though the value does not exist directly inside its own frame If the word share doesnt come out of your mouth you dont need to use a pointer When learning about pointers its important to think using a clear vocabulary and not operators or syntax So remember pointers are for sharing You should replace the operator for the word sharing as you read code Pointer Types For every type that is declared either by you or the language itself you get for free a complement pointer type you can use for sharing There already exists a builtin type named int so there is a complement pointer
type called int If you declare a type named User you get for free a pointer type called User All pointer types have the same two characteristics First they start with the character Second they all have the same memory size and representation which is 4 or 8 bytes that represent an address On 32bit architectures like the playground pointers require 4 bytes of memory and on 64 bit architectures like your machine they require 8 bytes of memory In the spec pointer types are considered to be type literals which means they are unnamed types composed from an existing type httpsgolangorgrefspecPointerType httpsgolangorgrefspecTypes Indirect Memory Access Look at this small program that performs a function call passing an address by value This will share the count variable from the main stack frame with the 273 increment function Listing 10 01 package main 02 03 func main 04 05 Declare variable of type int with a value of 10 06 count 10 07 08 Display the value of and address of count 09 printlncounttValue Of count ttAddr Of count 10 11 Pass the address of count 12 incrementcount CHANGED 13 14 printlncounttValue Of count ttAddr Of count 15 16 17 gonoinline 18 func incrementinc int CHANGED 19 20 Increment the value of count that the pointer points to 21 inc CHANGED 22 printlninctValue Of inc tAddr Of inc tValue Points To inc 23 There are three interesting changes that were made to this program from the original Here is the first
change on line 12 Listing 11 12 incrementcount This time on line 12 the code is not copying and passing the value of count but instead the address of count You can now say I am sharing the count variable with the increment function This is what the operator says sharing Understand this is still a pass by value the only difference is the value you are passing is an address instead of an integer Addresses are values too this is what is being copied and passed across the frame boundary for the function call Since the value of an address is being copied and passed you need a variable inside the frame of increment to receive and store this integer based address This is where the declaration of the integer pointer variable comes in on line 18 274 Listing 12 18 func incrementinc int If you were passing the address of a User value then the variable would have needed to be declared as a User Even though all pointer variables store address values they cant be passed any address only addresses associated with the pointer type This is the key the reason to share a value is because the receiving function needs to perform a read or write to that value You need the type information of any value in order to read and write to it The compiler will make sure that only values associated with the correct pointer type are shared with that function This is
what the stack looks like after the function call to increment Figure 5 You can see in figure 5 what the stack looks like when a pass by value is performed using an address as the value The pointer variable inside the frame for the increment function is now pointing to the count variable which is located inside the frame for main Now using the pointer variable the function can perform an indirect read modify write operation to the count variable located inside the frame for main 275 Listing 13 21 inc This time the character is acting as an operator and being applied against the pointer variable Using the as an operator means the value that the pointer points to The pointer variable allows indirect memory access outside of the functions frame that declared it Sometimes this indirect read or write is called dereferencing the pointer The increment function still must have a pointer variable within its frame it can directly read to perform the indirect access In figure 6 you see what the stack looks like after the execution of line 21 Figure 6 Here is the final output of this program Listing 14 count inc count Value Of 10 Value Of 0x10429fa4 Value Of 11 Addr Of 0x10429fa4 Addr Of 0x10429f98 Addr Of 0x10429fa4 Value Points To 11 You can see the value of the inc pointer variable is the same as the address of the count variable This sets up the sharing relationship that allowed the
indirect 276 access to the memory outside of the frame to take place Once the write is performed by the increment function through the pointer the change is seen by the main function when control is returned Pointer Variables Are Not Special Pointer variables are not special because they are variables like any other variable They have a memory allocation and they hold a value It just so happens that all pointer variables regardless of the type of value they can point to are always the same size and representation What can be confusing is that the character is acting as an operator inside the code and is used to declare the pointer type If you can distinguish the type declaration from the pointer operation this can help alleviate some confusion Conclusion This post has described the purpose behind pointers and how stack and pointer mechanics work in Go This is the first step in understanding the mechanics design philosophies and guidelines needed for writing consistent and readable code In summary this is what you learned Functions execute within the scope of frame boundaries that provide an individual memory space for each respective function When a function is called there is a transition that takes place between two frames The benefit of passing data by value is readability The stack is important because it provides the physical memory space for the frame boundaries that are given to each individual function All stack memory below the active frame is invalid but
memory from the active frame and above is valid Making a function call means the Goroutine needs to frame a new section of memory on the stack Its during each function call when the frame is taken that the stack memory for that frame is wiped clean Pointers serve one purpose to share a value with a function so the function can read and write to that value even though the value does not exist directly inside its own frame For every type that is declared either by you or the language itself you get for free a compliment pointer type you can use for sharing The pointer variable allows indirect memory access outside of the functions frame that is using it Pointer variables are not special because they are variables like any other variable They have a memory allocation and they hold a value 277
142 Escape Analysis Mechanics Prelude As of version 117 Go changed the ABI application binary interface to implement a new way of passing function input and output arguments using registers instead of memory on the stack This is enabled for Linux MacOS and Windows on the 64bit x86 architectures This means that some function arguments wont be copied on the stack but some may depending on the viability of using registers This doesnt change any of the semantics described in this post Introduction In the first post I taught the basics of pointer mechanics by using an example in which a value was shared down a Goroutines stack What I did not show you is what happens when you share a value up the stack To understand this you need to learn about another area of memory where values can live the heap With that knowledge you can begin to learn about escape analysis Escape analysis is the process that the compiler uses to determine the placement of values that are created by your program Specifically the compiler performs static code analysis to determine if a value can be placed on the stack frame for the function constructing it or if the value must escape to the heap In Go there is no keyword or function you can use to direct the compiler in this decision Its only through the convention of how you write your code that you can impact this decision Heaps The heap is a second area
of memory in addition to the stack used for storing values The heap is not selfcleaning like stacks so there is a bigger cost to using this memory Primarily the costs are associated with the garbage collector GC which must get involved to keep this area clean When the GC runs it will use 25 of your available CPU capacity Plus it can potentially create microseconds of stop the world latency The benefit of having the GC is that you dont need to worry about managing heap memory which historically has been complicated and error prone Values on the heap constitute memory allocations in Go These allocations put pressure on the GC because every value on the heap that is no longer referenced by a pointer needs to be removed The more values that need to be checked and removed the more work the GC must perform on every run So the pacing algorithm is constantly working to balance the size of the heap with the pace it runs at 278 Sharing Stacks In Go no Goroutine is allowed to have a pointer that points to memory on another Goroutines stack This is because the stack memory for a Goroutine can be replaced with a new block of memory when the stack has to grow or shrink If the runtime had to track pointers to other Goroutine stacks it would be too much to manage and the stop the world latency in updating pointers on those stacks would be overwhelming
Here is an example of a stack that is replaced several times because of growth Look at the output for lines 2 and 6 You will see the address of the string value inside the stack frame of main changes twice Listing 1 package main const size 4096 func main s HELLO stackCopys 0 sizeint func stackCopys string c int a sizeint printlnc s s c if c 10 return stackCopys c a Output 0 0xc00011ff68 HELLO 1 0xc00011ff68 HELLO 2 0xc00015ff68 HELLO 3 0xc00015ff68 HELLO 4 0xc00015ff68 HELLO 5 0xc00015ff68 HELLO CHANGED 6 0xc0001dff68 HELLO 7 0xc0001dff68 HELLO 8 0xc0001dff68 HELLO 9 0xc0001dff68 HELLO CHANGED 279 Escape Mechanics When a value is shared outside the scope of a functions stack frame it will be placed or allocated on the heap Its the job of the escape analysis algorithms to find these situations and maintain a level of integrity in the program The integrity is in making sure that access to any value is always accurate consistent and efficient Look at this example to learn the basic mechanics behind escape analysis Listing 2 01 package main 02 03 type user struct 04 name string 05 email string 06 07 08 func main 09 u1 createUserV1 10 u2 createUserV2 11 12 printlnu1 u1 u2 u2 13 14 15 gonoinline 16 func createUserV1 user 17 u user 18 name Bill 19 email billardanlabscom 20 21 22 printlnV1 u 23 return u 24 25 26 gonoinline 27 func createUserV2 user 28 u user
29 name Bill 30 email billardanlabscom 31 32 33 printlnV2 u 34 return u 35 I am using the gonoinline directive to prevent the compiler from inlining the code for these functions directly in main Inlining would erase the function calls and 280 complicate this example In Listing 2 you see a program with two different functions that create a user value and return the value back to the caller Version 1 of the function is using value semantics on the return Listing 3 16 func createUserV1 user 17 u user 18 name Bill 19 email billardanlabscom 20 21 22 printlnV1 u 23 return u 24 I said the function is using value semantics on the return because the user value created by this function is being copied and passed up the call stack This means the calling function is receiving a copy of the value In listing 3 you can see the construction of a user value being performed on lines 17 through 20 Then on line 23 a copy of the user value is passed up the call stack and back to the caller After the function returns the stack looks like this 281 Figure 1 You can see in Figure 1 a user value exists in both frames after the call to createUserV1 In Version 2 of the function pointer semantics are being used on the return Listing 4 27 func createUserV2 user 28 u user 29 name Bill 30 email billardanlabscom 31 32 33 printlnV2 u
34 return u 35 Note I said the function is using pointer semantics on the return because the user value created by this function is being shared up the call stack This means the calling function is receiving a copy of the address for the value You can see the same struct literal being used on lines 28 through 31 to construct a user value but on line 34 the return is different Instead of passing a copy of the 282 user value back up the call stack a copy of the address for the user value is passed up Based on this you might think that the stack looks like this after the call but this is NOT what is happening Figure 2 If what you see in Figure 2 was really happening you would have an integrity issue The pointer is pointing down the call stack into memory that is no longer valid On the next function call by main that memory being pointed to is going to be reframed and reinitialized This is where escape analysis begins to maintain integrity In this case the compiler will determine its not safe to construct the user value inside the stack frame of createUserV2 so instead it will construct the value on the heap This will happen immediately during construction on line 28 Readability As you learned in the last post a function has direct access to the memory inside its frame through the frame pointer but access to memory
outside its frame requires indirect access This means access to values that escape to the heap must be done indirectly through a pointer as well Remember what the code looks like for createUserV2 283 Listing 5 27 func createUserV2 user 28 u user 29 name Bill 30 email billardanlabscom 31 32 33 printlnV2 u 34 return u 35 The syntax is hiding what is really happening in this code The variable u declared on line 28 represents a value of type user Construction in Go doesnt tell you where a value lives in memory so its not until the return statement on line 34 whether you know the value will need to escape This means even though u represents a value of type user access to this user value must be happening through a pointer underneath the covers You could visualize the stack looking like this after the function call Figure 3 284 The u variable on the stack frame for createUserV2 represents a value that is on the heap not the stack This means using u to access the value requires pointer access and not the direct access the syntax is suggesting You might think why not make u a pointer then since access to the value it represents requires the use of a pointer anyway Listing 6 27 func createUserV2 user 28 u user 29 name Bill 30 email billardanlabscom 31 32 33 printlnV2 u 34 return u 35 If you do this you are walking away from
an important readability gain you can have in your code Step away from the entire function for a second and just focus on the return Listing 7 34 35 return u What does this return tell you All that it says is that a copy of u is being passed up the call stack However what does the return tell you when you use the operator Listing 8 34 35 return u Thanks to the operator the return now tells you that u is being shared up the call stack and therefore escaping to the heap Remember pointers are for sharing Replace the operator for the word sharing as you read code This is very powerful in terms of readability something you dont want to lose Here is another example where constructing values using pointer semantics hurts readability Listing 9 01 var u user 02 err jsonUnmarshalbyter u 03 return u err 285 You must share the pointer variable with the jsonUnmarshal call on line 02 for this code to work The jsonUnmarshal call will create the user value and assign its address to the pointer variable httpsplaygolangorgpkoI8EjpeIx What does this code say 01 Create a pointer of type user set to its zero value 02 Share u with the jsonUnmarshal function 03 Return a copy of u to the caller It is not obviously clear that a user value which was created by the jsonUnmarshal function is being shared with the caller How does readability change when using value
semantics during construction Listing 10 01 var u user 02 err jsonUnmarshalbyter u 03 return u err What does this code say 01 Create a value of type user set to its zero value 02 Share u with the jsonUnmarshal function 03 Share u with the caller Everything is very clear Line 02 is sharing the user value down the call stack into jsonUnmarshal and line 03 is sharing the user value up the call stack back to the caller This share will cause the user value to escape Use value semantics when constructing a value and leverage the readability of the operator to make it clear how values are being shared Compiler Reporting To see the decisions the compiler is making you can ask the compiler to provide a report All you need to do is use the gcflags switch with the m option on the go build call There are actually 4 levels of m you can use but beyond 2 levels the information is overwhelming I will be using the 2 levels of m 286 Listing 11 go build gcflags m2 example4go166 cannot inline createUserV1 marked gonoinline example4go276 cannot inline createUserV2 marked gonoinline example4go86 cannot inline main function too complex cost 132 exceeds budget 80 example4go282 u escapes to heap example4go282 flow r0 u example4go282 from u addressof at example4go349 example4go282 from return u return at example4go342 example4go282 moved to heap u You can see the compiler is reporting the escape decisions What is the compiler saying
First look at the createUserV1 and createUserV2 functions again for reference Listing 12 16 func createUserV1 user 17 u user 18 name Bill 19 email billardanlabscom 20 21 22 printlnV1 u 23 return u 24 27 func createUserV2 user 28 u user 29 name Bill 30 email billardanlabscom 31 32 33 printlnV2 u 34 return u 35 Look at these lines in the report Listing 13 example4go282 u escapes to heap example4go282 flow r0 u example4go282 from u addressof at example4go349 example4go282 from return u return at example4go342 example4go282 moved to heap u These lines are saying the user value associated with the u variable which is constructed on line 28 is escaping because of the return on line 34 287 Reading these reports can be confusing and can slightly change depending on whether the type of variable in question is based on a named or literal type Change u to be of the literal type user instead of the named type user that it was before Listing 14 27 func createUserV2 user 28 u user 29 name Bill 30 email billardanlabscom 31 32 33 printlnV2 u 34 return u 35 Run the report again Listing 15 go build gcflags m2 example4go166 cannot inline createUserV1 marked gonoinline example4go276 cannot inline createUserV2 marked gonoinline example4go86 cannot inline main function too complex cost 132 exceeds budget 80 example4go287 user escapes to heap example4go287 flow u storage for user example4go287 from user spill at example4go287 example4go287 from u user assign at example4go284 example4go287
flow r0 u example4go287 from return u return at example4go342 example4go287 user escapes to heap Now the report is saying the address to a user value referenced by the u variable and constructed on line 28 is escaping because of the return on line 34 Conclusion The construction of a value doesnt determine where it lives Only how a value is shared will determine what the compiler will do with that value Anytime you share a value up the call stack it is going to escape There are other reasons for a value to escape which I will explore in the next post What these posts are trying to lead you to is guidelines for choosing value or pointer semantics for any given type Each semantic comes with a benefit and cost Value semantics keep values on the stack which reduces pressure on the GC 288 However there are different copies of any given value that must be stored tracked and maintained Pointer semantics place values on the heap which can put pressure on the GC However they are efficient because there is only one value that needs to be stored tracked and maintained The key is using each semantic correctly consistently and in balance 289
143 Scheduling In Go OS Scheduler Introduction The design and behavior of the Go scheduler allows your multithreaded Go programs to be more efficient and performant This is thanks to the mechanical sympathies the Go scheduler has for the operating system OS scheduler However if the design and behavior of your multithreaded Go software is not mechanically sympathetic with how the schedulers work none of this will matter Its important to have a general and representative understanding of how both the OS and Go schedulers work to design your multithreaded software correctly This multipart article will focus on the higherlevel mechanics and semantics of the schedulers I will provide enough details to allow you to visualize how things work so you can make better engineering decisions Even though there is a lot that goes into the engineering decisions you need to make for multithreaded applications the mechanics and semantics form a critical part of the foundational knowledge you need OS Scheduler Operating system schedulers are complex pieces of software They have to take into account the layout and setup of the hardware they run on This includes but is not limited to the existence of multiple processors and cores CPU caches and NUMA httpfrankdennemannl20160706introduction2016numadeepdiveseries Without this knowledge the scheduler cant be as efficient as possible Whats great is you can still develop a good mental model of how the OS scheduler works without going deep into these topics Your program is just a series of machine instructions that need to
be executed one after the other sequentially To make that happen the operating system uses the concept of a Thread Its the job of the Thread to account for and sequentially execute the set of instructions its assigned Execution continues until there are no more instructions for the Thread to execute This is why I call a Thread a path of execution Every program you run creates a Process and each Process is given an initial Thread Threads have the ability to create more Threads All these different Threads run independently of each other and scheduling decisions are made at the Thread level not at the Process level Threads can run concurrently each taking a turn on an individual core or in parallel each running at the same time on different 290 cores Threads also maintain their own state to allow for the safe local and independent execution of their instructions The OS scheduler is responsible for making sure cores are not idle if there are Threads that can be executing It must also create the illusion that all the Threads that can execute are executing at the same time In the process of creating this illusion the scheduler needs to run Threads with a higher priority over lower priority Threads However Threads with a lower priority cant be starved of execution time The scheduler also needs to minimize scheduling latencies as much as possible by making quick and smart decisions A lot goes into the algorithms to make this
happen but luckily there are decades of work and experience the industry is able to leverage To understand all of this better its good to describe and define a few concepts that are important Executing Instructions The program counter PC which is sometimes called the instruction pointer IP is what allows the Thread to keep track of the next instruction to execute In most processors the PC points to the next instruction and not the current instruction 291 Figure 1 httpswwwslidesharenetJohnCutajarassemblylanguage8086intermediate If you have ever seen a stack trace from a Go program you might have noticed these small hexadecimal numbers at the end of each line Look for 0x39 and 0x72 Listing 1 goroutine 1 running mainexample0xc000042748 0x2 0x4 0x106abae 0x5 0xa stacktraceexample1example1go13 0x39 LOOK HERE mainmain stacktraceexample1example1go8 0x72 LOOK HERE Those numbers represent the PC value offset from the top of the respective function The 0x39 PC offset value represents the next instruction the Thread would have executed inside the example function if the program hadnt panicked The 0x72 PC offset value is the next instruction inside the main function if control happens to go back to that function More importantly the instruction prior to that pointer tells you what instruction was executing Look at the program below in Listing 2 which caused the stack trace from Listing 1 292 Listing 2 httpsgithubcomardanlabsgotrainingblobmastertopicsgoprofiling stacktraceexample1example1go 07 func main 08 examplemakestring 2 4 hello 10 09 12 func exampleslice string str string i int 13 panicWant stack trace 14 The
hex number 0x39 represents the PC offset for an instruction inside the example function which is 57 base 10 bytes below the starting instruction for the function In Listing 3 below you can see an objdump of the example function from the binary Find the 12th instruction which is listed at the bottom Notice the line of code above that instruction is the call to panic Listing 3 go tool objdump S s mainexample example1 func exampleslice string str string i int 0x104dfa0 65488b0c2530000000 MOVQ GS0x30 CX 0x104dfa9 483b6110 CMPQ 0x10CX SP 0x104dfad 762c JBE 0x104dfdb 0x104dfaf 4883ec18 SUBQ 0x18 SP 0x104dfb3 48896c2410 MOVQ BP 0x10SP 0x104dfb8 488d6c2410 LEAQ 0x10SP BP panicWant stack trace 0x104dfbd 488d059ca20000 LEAQ runtimetypes41504SB AX 0x104dfc4 48890424 MOVQ AX 0SP 0x104dfc8 488d05a1870200 LEAQ mainstatictmp0SB AX 0x104dfcf 4889442408 MOVQ AX 0x8SP 0x104dfd4 e8c735fdff CALL runtimegopanicSB 0x104dfd9 0f0b UD2 LOOK HERE PC0x39 Remember the PC is the next instruction not the current one Listing 3 is a good example of the amd64 based instructions that the Thread for this Go program is in charge of executing sequentially Thread States Another important concept is Thread state which dictates the role the scheduler takes with the Thread A Thread can be in one of three states Waiting Runnable or Executing 293 Waiting This means the Thread is stopped and waiting for something in order to continue This could be for reasons like waiting for the hardware disk network the operating system system calls or synchronization calls atomic mutexes These types
of latencies are a root cause for bad performance Runnable This means the Thread wants time on a core so it can execute its assigned machine instructions If you have a lot of Threads that want time then Threads have to wait longer to get time Also the individual amount of time any given Thread gets is shortened as more Threads compete for time This type of scheduling latency can also be a cause of bad performance Executing This means the Thread has been placed on a core and is executing its machine instructions The work related to the application is getting done This is what everyone wants Types Of Work There are two types of work a Thread can do The first is called CPUBound and the second is called IOBound CPUBound This is work that never creates a situation where the Thread may be placed in Waiting states This is work that is constantly making calculations A Thread calculating Pi to the Nth digit would be CPUBound IOBound This is work that causes Threads to enter into Waiting states This is work that consists in requesting access to a resource over the network or making system calls into the operating system A Thread that needs to access a database would be IOBound I would include synchronization events mutexes atomic that cause the Thread to wait as part of this category Context Switching If you are running on Linux Mac or Windows you are running on an OS that
has a preemptive scheduler This means a few important things First it means the scheduler is unpredictable when it comes to what Threads will be chosen to run at any given time Thread priorities together with events like receiving data on the network make it impossible to determine what the scheduler will choose to do and when Second it means you must never write code based on some perceived behavior that you have been lucky to experience but is not guaranteed to take place every time It is easy to allow yourself to think because Ive seen this happen the same way thousands of times this is guaranteed behavior You must control the synchronization and orchestration of Threads if you need determinism in your 294 application The physical act of swapping Threads on a core is called context switching A context switch happens when the scheduler pulls an Executing thread off a core and replaces it with a Runnable Thread The Thread that was selected from the run queue moves into the Executing state The Thread that was pulled can move back into the Runnable state if it still has the ability to run or into the Waiting state if was replaced because of an IOBound type of request Context switches are considered to be expensive because it takes time to swap Threads on and off a core The amount of latency incurred during a context switch depends on different factors but its not unreasonable for it to take between
1000 and 1500 nanoseconds httpselithegreenplacenet2018measuringcontextswitchingandmemoryoverheadsforlinuxthreads Considering the hardware should be able to reasonably execute on average 12 instructions per nanosecond per core a context switch can cost you 12k to 18k instructions of latency httpswwwyoutubecomwatchvjEG4Qyo4Bcfeatureyoutubet266 In essence your program is losing the ability to execute a large number of instructions during a context switch If you have a program that is focused on IOBound work then context switches are going to be an advantage Once a Thread moves into a Waiting state another Thread in a Runnable state is there to take its place This allows the core to always be doing work This is one of the most important aspects of scheduling Dont allow a core to go idle if there is work Threads in a Runnable state to be done If your program is focused on CPUBound work then context switches are going to be a performance nightmare Since the Thread always has work to do the context switch is stopping that work from progressing This situation is in stark contrast with what happens with an IOBound workload Less Is More Improving scheduler latency httpslwnnetArticles404993 In the early days when processors had only one core scheduling wasnt overly complicated Because you had a single processor with a single core only one Thread could execute at any given time The idea was to define a scheduler period 295 and attempt to execute all the Runnable Threads within that period of time No problem take the scheduling period and divide
it by the number of Threads that need to execute As an example if you define your scheduler period to be 1000ms 1 second and you have 10 Threads then each thread gets 100ms each If you have 100 Threads each Thread gets 10ms each However what happens when you have 1000 Threads Giving each Thread a time slice of 1ms doesnt work because the percentage of time youre spending in context switches will be significantly related to the amount of time youre spending on application work What you need is to set a limit on how small a given time slice can be In the last scenario if the minimum time slice was 10ms and you have 1000 Threads the scheduler period needs to increase to 10000ms 10 seconds What if there were 10000 Threads now you are looking at a scheduler period of 100000ms 100 seconds At 10000 threads with a minimal time slice of 10ms it takes 100 seconds for all the Threads to run once in this simple example if each Thread uses its full time slice Be aware this is a very simple view of the world There are more things that need to be considered and handled by the scheduler when making scheduling decisions as discussed in this post httpsblogacolyerorg20160426thelinuxscheduleradecadeofwastedcores You control the number of Threads you use in your application When there are more Threads to consider and IOBound work happening there is more chaos and nondeterministic behavior Things take longer to schedule
and execute This is why the rule of the game is Less is More Less Threads in a Runnable state means less scheduling overhead and more time each Thread gets over time More Threads in a Runnable state mean less time each Thread gets over time That means less of your work is getting done over time as well Find The Balance There is a balance you need to find between the number of cores you have and the number of Threads you need to get the best throughput for your application When it comes to managing this balance Thread pools are a great answer I will show you in the next post that this is no longer necessary with Go I think this is one of the nice things Go did to make multithreaded application development easier Prior to coding in Go I wrote code in C and C on NT On that operating system 296 the use of IOCP IO Completion Ports thread pools were critical to writing multithreaded software As an engineer you needed to figure out how many Thread pools you needed and the max number of Threads for any given pool to maximize throughput for the number of cores that you were given When writing web services that talked to a database the magic number of 3 Threads per core seemed to always give the best throughput on NT In other words 3 Threads per core minimized the latency costs of context switching while maximizing
execution time on the cores When creating an IOCP Thread pool I knew to start with a minimum of 1 Thread and a maximum of 3 Threads for every core I identified on the host machine If I used 2 Threads per core it took longer to get all the work done because I had idle time when I could have been getting work done If I used 4 Threads per core it also took longer because I had more latency in context switches The balance of 3 Threads per core for whatever reason always seemed to be the magic number on NT What if your service is doing a lot of different types of work That could create different and inconsistent latencies Maybe it also creates a lot of different systemlevel events that need to be handled It might not be possible to find a magic number that works all the time for all the different work loads When it comes to using Thread pools to tune the performance of a service it can get very complicated to find the right consistent configuration Cache Lines Accessing data from main memory has such a high latency cost 100ns or 1200 instructions that processors and cores have local caches to keep data close to the hardware threads that need it Accessing data from caches has a much lower cost 13ns to 123ns or 16 to 160 instructions depending on the cache being accessed Today one aspect of performance is about how
efficiently you can get data into the processor to reduce these dataaccess latencies Writing multithreaded applications that mutate state need to consider the mechanics of the caching system 297 Figure 2 Data is exchanged between the processor and main memory using cache lines Note This talk by Scott Meyers has these details and more httpswwwyoutubecomwatchvWDIkqP4JbkE A cache line is a 64byte chunk of memory that is exchanged between main memory and the caching system Each core is given its own copy of any cache line it needs which means the hardware uses value semantics This is why mutations to memory in multithreaded applications can create performance nightmares When multiple Threads running in parallel are accessing the same data value or even data values near one another they will be accessing data on the same cache line Any Thread running on any core will get its own copy of that same cache line 298 Figure 3 If one Thread on a given core makes a change to its copy of the cache line then through the magic of hardware all other copies of the same cache line have to be marked dirty When a Thread attempts to read or write access to a dirty cache line main memory access 100 to 300 clock cycles is required to get a new copy of the cache line Maybe on a 2core processor this isnt a big deal but what about a 32core processor running 32 threads in parallel all accessing and mutating data
on the same cache line What about a system with two physical processors with 16 cores each This is going to be worse because of the added latency for processortoprocessor communication The application is going to be thrashing through memory and the performance is going to be horrible and most likely you will have no understanding why This is called the cachecoherency problem and also introduces problems like false sharing When writing multithreaded applications that will be mutating shared state the caching systems have to be taken into account 299 Scheduling Decision Scenario Imagine Ive asked you to write the OS scheduler based on the highlevel information Ive given you Think about this one scenario that you have to consider Remember this is one of many interesting things the scheduler has to consider when making a scheduling decision You start your application and the main Thread is created and is executing on core 1 As the Thread starts executing its instructions cache lines are being retrieved because data is required The Thread now decides to create a new Thread for some concurrent processing Here is the question Once the Thread is created and ready to go should the scheduler 1 Context switch the main Thread off of core 1 Doing this could help performance as the chances that this new Thread needs the same data that is already cached is pretty good But the main Thread does not get its full time slice 2 Have the Thread wait for core
1 to become available pending the completion of the main Threads time slice The Thread is not running but latency on fetching data will be eliminated once it starts 3 Have the Thread wait for the next available core This would mean cache lines for the selected core would be flushed retrieved and duplicated causing latency However the Thread would start more quickly and the main Thread could finish its time slice Having fun yet These are interesting questions that the OS scheduler needs to take into account when making scheduling decisions Luckily for everyone Im not the one making them All I can tell you is that if there is an idle core its going to be used You want Threads running when they can be running Conclusion This first part of the post provides insights into what you have to consider regarding Threads and the OS scheduler when writing multithreaded applications These are the things the Go scheduler takes into consideration as well 300
144 Scheduling In Go Go Scheduler Introduction When your Go program starts up its given a Logical Processor P for every virtual core that is identified on the host machine If you have a processor with multiple hardware threads per physical core HyperThreading each hardware thread will be presented to your Go program as a virtual core To better understand this take a look at the system report for my MacBook Pro httpsenwikipediaorgwikiHyperthreading Figure 1 You can see I have a single processor with 4 physical cores What this report is not exposing is the number of hardware threads I have per physical core The Intel Core i7 processor has HyperThreading which means there are 2 hardware threads per physical core This will report to the Go program that 8 virtual cores are available for executing OS Threads in parallel To test this consider the following program 301 Listing 1 package main import fmt runtime func main GOMAXPROCS returns the number of logical CPUs currently being used by the current process fmtPrintlnruntimeGOMAXPROCS0 When I run this program on my local machine the result of the GOMAXPROCS0 function call will be the value of 8 Any Go program I run on my machine will be given 8 Ps Every P is assigned an OS Thread M The M stands for machine This Thread is still managed by the OS and the OS is still responsible for placing the Thread on a Core for execution This means when I run a Go
program on my machine I have 8 threads available to execute my work each individually attached to a P Every Go program is also given an initial Goroutine G which is the path of execution for a Go program A Goroutine is essentially a Coroutine but this is Go so we replace the letter C with a G and we get the word Goroutine httpsenwikipediaorgwikiCoroutine You can think of Goroutines as applicationlevel threads They are similar to OS Threads in many ways eg just as OS Threads are contextswitched on and off a core Goroutines are contextswitched on and off an M The last piece of the puzzle is the run queues There are two different run queues in the Go scheduler the Global Run Queue GRQ and the Local Run Queue LRQ Each P is given a LRQ that manages the Goroutines assigned to be executed within the context of a P These Goroutines take turns being contextswitched on and off the M assigned to that P The GRQ is for Goroutines that have not been assigned to a P yet There is a process to move Goroutines from the GRQ to a LRQ that we will discuss later Figure 2 provides an image of all these components together 302 Figure 2 Cooperating Scheduler As we discussed in the first post the OS scheduler is a preemptive scheduler Essentially that means you cant predict what the scheduler is going to do at any given time The kernel is making
decisions and everything is nondeterministic Applications that run on top of the OS have no control over what is happening inside the kernel with scheduling unless they leverage synchronization primitives like atomic instructions and mutex calls httpsenwikipediaorgwikiLinearizability httpsenwikipediaorgwikiLockcomputerscience The Go scheduler is part of the Go runtime and the Go runtime is built into your application This means the Go scheduler runs in user space above the kernel httpsenwikipediaorgwikiUserspace The current implementation of the Go scheduler is a preemptive scheduler httpsenwikipediaorgwikiPreemptioncomputing You cant predict what the Go scheduler is going to do This is because decision making for this scheduler doesnt rest in the hands of developers but in the Go runtime Its important to understand the scheduler is nondeterministic Goroutine States Just like Threads Goroutines have the same three highlevel states These dictate the role the Go scheduler takes with any given Goroutine A Goroutine can be in one 303 of three states Waiting Runnable or Running Waiting This means the Goroutine is stopped and waiting for something in order to continue This could be for reasons like waiting for the operating system system calls or synchronization calls atomic and mutex operations These types of latencies are a root cause for bad performance Runnable This means the Goroutine wants time on an M so it can execute its assigned instructions If you have a lot of Goroutines that want time then Goroutines have to wait longer to get time Also the individual amount of time any given Goroutine gets
is shortened as more Goroutines compete for time This type of scheduling latency can also be a cause of bad performance Running This means the Goroutine has been placed on an M and is executing its instructions The work related to the application is getting done This is what everyone wants Context Switching The Go scheduler requires welldefined userspace events that occur at safe points in the code to context switch from These events and safe points manifest themselves within function calls or in between instructions There are several classes of events that occur in your Go programs that allow the scheduler to make scheduling decisions This doesnt mean it will always happen on one of these events It means the scheduler gets the opportunity These events are The use of the keyword go Garbage collection System calls Synchronization and Orchestration The use of the keyword go The keyword go is how you create Goroutines Once a new Goroutine is created it gives the scheduler an opportunity to make a scheduling decision Garbage collection Since the GC runs using its own set of Goroutines those Goroutines need time on an M to run This causes the GC to create a lot of scheduling chaos However the scheduler is very smart about what a Goroutine is doing and it will leverage that 304 intelligence to make smart decisions One smart decision is context switching a Goroutine that wants to touch the heap with those that dont touch the heap during GC
When GC is running a lot of scheduling decisions are being made System calls If a Goroutine makes a system call that will cause the Goroutine to block the M sometimes the scheduler is capable of context switching the Goroutine off the M and contextswitch a new Goroutine onto that same M However sometimes a new M is required to keep executing Goroutines that are queued up in the P How this works will be explained in more detail in the next section Synchronization and Orchestration If an atomic mutex or channel operation call will cause the Goroutine to block the scheduler can contextswitch a new Goroutine to run Once the Goroutine can run again it can be requeued and eventually contextswitched back on an M Asynchronous System Calls When the OS you are running on has the ability to handle a system call asynchronously something called the network poller can be used to process the system call more efficiently This is accomplished by using kqueue MacOS epoll Linux or iocp Windows within these respective OSs httpsgolangorgsrcruntimenetpollgo Networkingbased system calls can be processed asynchronously by many of the OSs we use today This is where the network poller gets its name since its primary use is handling networking operations By using the network poller for networking system calls the scheduler can prevent Goroutines from blocking the M when those system calls are made This helps to keep the M available to execute other Goroutines in the Ps LRQ without the
need to create new Ms This helps to reduce scheduling load on the OS The best way to see how this works is to run through an example 305 Figure 3 Figure 3 shows our base scheduling diagram Goroutine1 is executing on the M and there are 3 more Goroutines waiting in the LRQ to get their time on the M The network poller is idle with nothing to do Figure 4 In figure 4 Goroutine1 wants to make a network system call so Goroutine1 is moved to the network poller and the asynchronous network system call is processed Once Goroutine1 is moved to the network poller the M is now available to execute a different Goroutine from the LRQ In this case Goroutine2 is context switched on the M 306 Figure 5 In figure 5 the asynchronous network system call is completed by the network poller and Goroutine1 is moved back into the LRQ for the P Once Goroutine1 can be context switched back on the M the Go related code its responsible for can execute again The big win here is that to execute network system calls no extra Ms are needed The network poller has an OS Thread and it is handling an efficient event loop Synchronous System Calls What happens when the Goroutine wants to make a system call that cant be done asynchronously In this case the network poller cant be used and the Goroutine making the system call is going to block the M
This is unfortunate but theres no way to prevent this from happening One example of a system call that cant be made asynchronously is filebased system calls If you are using CGO there may be other situations where calling C functions will block the M as well Lets walk through what happens with a synchronous system call like file IO that will cause the M to block 307 Figure 6 Figure 6 is showing our basic scheduling diagram again but this time Goroutine1 is going to make a synchronous system call that will block M1 Figure 7 In figure 7 the scheduler is able to identify that Goroutine1 has caused the M to block At this point the scheduler detaches M1 from the P with the blocking Goroutine1 still attached Then the scheduler brings in a new M2 to service the P At that point Goroutine2 can be selected from the LRQ and context switched on M2 If an M already exists because of a previous swap this transition is quicker than having to create a new M 308 Figure 8 In figure 8 the blocking system call that was made by Goroutine1 finishes At this point Goroutine1 can move back into the LRQ and be serviced by the P again M1 is then placed on the side for future use if this scenario needs to happen again Work Stealing Another aspect of the scheduler is that its a workstealing scheduler This helps in a few areas to keep scheduling
efficient For one the last thing you want is an M to move into a waiting state because once that happens the OS will contextswitch the M off the Core This means the P cant get any work done even if there is a Goroutine in a runnable state until an M is contextswitched back on a Core The work stealing also helps to balance the Goroutines across all the Ps so the work is better distributed and getting done more efficiently Lets run through an example 309 Figure 9 In figure 9 we have a multithreaded Go program with two Ps servicing four Goroutines each and a single Goroutine in the GRQ What happens if one of the Ps services all of its Goroutines quickly Figure 10 In figure 10 P1 has no more Goroutines to execute But there are Goroutines in a runnable state both in the LRQ for P2 and in the GRQ This is a moment where P1 needs to steal work The rules for stealing work are as follows httpsgolangorgsrcruntimeprocgo 310 Listing 2 runtimeschedule only 161 of the time check the global runnable queue for a G if not found check the local queue if not found try to steal from other Ps if not check the global runnable queue if not found poll network So based on these rules in Listing 2 P1 needs to check P2 for Goroutines in its LRQ and take half of what it finds Figure 11 In figure 11
half the Goroutines are taken from P2 and now P1 can execute those Goroutines What happens if P2 finishes servicing all of its Goroutines and P1 has nothing left in its LRQ 311 Figure 12 In figure 12 P2 finished all its work and now needs to steal some First it will look at the LRQ of P1 but it wont find any Goroutines Next it will look at the GRQ There it will find Goroutine9 Figure 13 In figure 13 P2 steals Goroutine9 from the GRQ and begins to execute the work What is great about all this work stealing is that it allows the Ms to stay busy and not go idle This work stealing is considered internally as spinning the M This spinning has other benefits that JBD explains well in her workstealing blog post httpsrakyllorgscheduler 312 Practical Example With the mechanics and semantics in place I want to show you how all of this comes together to allow the Go scheduler to execute more work over time Imagine a multithreaded application written in C where the program is managing two OS Threads that are passing messages back and forth to each other Figure 14 In figure 14 there are 2 Threads that are passing a message back and forth Thread 1 gets context switched on Core 1 and is now executing which allows Thread 1 to send its message to Thread 2 Note How the message is being passed is unimportant Whats important is the state
of the Threads as this orchestration proceeds 313 Figure 15 In figure 15 once Thread 1 finishes sending the message it now needs to wait for the response This will cause Thread 1 to be context switched off Core 1 and moved into a waiting state Once Thread 2 is notified about the message it moves into a runnable state Now the OS can perform a context switch and get Thread 2 executing on a Core which happens to be Core 2 Next Thread 2 processes the message and sends a new message back to Thread 1 314 Figure 16 In figure 16 Threads context switch once again as the message by Thread 2 is received by Thread 1 Now Thread 2 context switches from the executing state to the waiting state and Thread 1 context switches from the waiting state to the runnable state and finally back to the executing state which allows it to process and send a new message back All these context switches and state changes require time to be performed which limits how fast the work can get done With each context switching potential incurring a latency of 1000ns and hopefully the hardware executing 12 instructions per nanosecond you are looking at 12k instructions more or less not executing during these context switches Since these Threads are also bouncing between different Cores the chances of incurring additional latency due to cacheline misses are also high Lets take this same example but use Goroutines and the
Go scheduler instead 315 Figure 17 In figure 17 there are two Goroutines that are in orchestration with each other passing a message back and forth G1 gets context switched on M1 which happens to be running on Core 1 which allows G1 to be executing its work The work is for G1 to send its message to G2 316 Figure 18 In figure 18 once G1 finishes sending the message it now needs to wait for the response This will cause G1 to be context switched off M1 and moved into a waiting state Once G2 is notified about the message it moves into a runnable state Now the Go scheduler can perform a context switch and get G2 executing on M1 which is still running on Core 1 Next G2 processes the message and sends a new message back to G1 317 Figure 19 In figure 19 things context switch once again as the message sent by G2 is received by G1 Now G2 context switches from the executing state to the waiting state and G1 context switches from the waiting state to the runnable state and finally back to the executing state which allows it to process and send a new message back Things on the surface dont appear to be any different All the same context switches and state changes are occurring whether you use Threads or Goroutines However there is a major difference between using Threads and Goroutines that might not be obvious at first
glance In the case of using Goroutines the same OS Thread and Core is being used for all the processing This means that from the OSs perspective the OS Thread never moves into a waiting state not once As a result all those instructions we lost to context switches when using Threads are not lost when using Goroutines Essentially Go has turned IOBound work into CPUBound work at the OS level 318 Since all the context switching is happening at the application level we dont lose the same 12k instructions on average per context switch that we were losing when using Threads In Go those same context switches are costing you 200ns or 24k instructions The scheduler is also helping with gains on cacheline efficiencies and NUMA This is why we dont need more Threads than we have virtual cores In Go its possible to get more work done over time because the Go scheduler attempts to use less Threads and do more on each Thread which helps to reduce load on the OS and the hardware Conclusion The Go scheduler is really amazing in how the design takes into account the intricacies of how the OS and the hardware work The ability to turn IOBound work into CPUBound work at the OS level is where we get a big win in leveraging more CPU capacity over time This is why you dont need more OS Threads than you have virtual cores You can reasonably expect to get all of
your work done CPU and IO Bound with just one OS Thread per virtual core Doing so is possible for networking apps and other apps that dont need system calls that block OS Threads As a developer you still need to understand what your app is doing in terms of the kinds of work you are processing You cant create an unlimited number of Goroutines and expect amazing performance Less is always more but with the understanding of these Goscheduler semantics you can make better engineering decisions 319
145 Scheduling In Go Concurrency Introduction When Im solving a problem especially if its a new problem I dont initially think about whether concurrency is a good fit or not I look for a sequential solution first and make sure that its working Then after readability and technical reviews I will begin to ask the question if concurrency is reasonable and practical Sometimes its obvious that concurrency is a good fit and other times its not so clear In this post I will begin to bring the mechanics and semantics of the OS and Go schedulers together to provide a deeper understanding on what concurrency is and isnt The goals of this post are Provide guidance on the semantics you must consider to determine if a workload is suitable for using concurrency Show you how different types of workloads change the semantics and therefore the engineering decisions you will want to make What is Concurrency Concurrency means undefined out of order execution Taking a set of instructions that would otherwise be executed in sequence and finding a way to execute them out of order and still produce the same result For the problem in front of you it has to be obvious that out of order execution would add value When I say value I mean add enough of a performance gain for the complexity cost Depending on your problem out of order execution may not be possible or even make sense Its important to understand that concurrency is not
the same as parallelism httpsbloggolangorgconcurrencyisnotparallelism Parallelism means executing two or more instructions at the same time This is a different concept from concurrency Parallelism is only possible when you have at least two operating system OS and hardware threads available to you and you have at least two Goroutines each executing instructions independently on separate OShardware threads 320 Figure 1 In figure 1 you see a diagram of two logical processors P each with their independent OS thread M attached to an independent hardware thread Core on the machine You can see two Goroutines G1 and G2 are executing in parallel executing their instructions on their respective OShardware thread at the same time Within each logical processor three Goroutines are taking turns sharing their respective OS thread All these Goroutines are running concurrently executing their instructions in no particular order and sharing time on the OS thread Heres the rub sometimes leveraging concurrency without parallelism can actually slow down your throughput Whats also interesting is sometimes leveraging concurrency with parallelism doesnt give you a bigger performance gain than you might otherwise think you can achieve Workloads How do you know when out of order execution may be possible or make sense Understanding the type of workload your problem is handling is a great place to start There are two types of workloads that are important to understand when thinking about concurrency CPUBound This is a workload that never creates a situation where Goroutines naturally move in and out of waiting
states This is work that is constantly making calculations A Thread calculating Pi to the Nth digit would be CPUBound IOBound This is a workload that causes Goroutines to naturally enter into waiting states This is work that consists in requesting access to a resource over the network or making system calls into the operating system or waiting for an event to occur A Goroutine that needs to read a file would be IOBound I would include synchronization events mutexes atomic that cause the Goroutine to wait as part of this category With CPUBound workloads you need parallelism to leverage concurrency A single OShardware thread handling multiple Goroutines is not efficient since the 321 Goroutines are not moving in and out of waiting states as part of their workload Having more Goroutines than there are OShardware threads can slow down workload execution because of the latency cost the time it takes of moving Goroutines on and off the OS thread The context switch is creating a Stop The World event for your workload since none of your workload is being executed during the switch when it otherwise could be With IOBound workloads you dont need parallelism to use concurrency A single OShardware thread can handle multiple Goroutines with efficiency since the Goroutines are naturally moving in and out of waiting states as part of their workload Having more Goroutines than there are OShardware threads can speed up workload execution because the latency cost of moving Goroutines on and off the
OS thread is not creating a Stop The World event Your workload is naturally stopped and this allows a different Goroutine to leverage the same OShardware thread efficiently instead of letting the OShardware thread sit idle How do you know how many Goroutines per hardware thread provides the best throughput Too few Goroutines and you have more idle time Too many Goroutines and you have more context switch latency time This is something for you to think about but beyond the scope of this particular post For now its important to review some code to solidify your ability to identify when a workload can leverage concurrency when it cant and if parallelism is needed or not Adding Numbers We dont need complex code to visualize and understand these semantics Look at the following function named add that sums a collection of integers Listing 1 36 func addnumbers int int 37 var v int 38 for n range numbers 39 v n 40 41 return v 42 In listing 1 on line 36 a function named add is declared that takes a collection of integers and returns the sum of the collection It starts on line 37 with the declaration of the v variable to contain the sum Then on line 38 the function traverses the collection linearly and each number is added to the current sum on line 39 Finally on line 41 the function returns the final sum back to the caller 322 Question is the add function a
workload that is suitable for out of order execution I believe the answer is yes The collection of integers could be broken up into smaller lists and those lists could be processed concurrently Once all the smaller lists are summed the set of sums could be added together to produce the same answer as the sequential version However there is another question that comes to mind How many smaller lists should be created and processed independently to get the best throughput To answer this question you must know what kind of workload add is performing The add function is performing a CPUBound workload because the algorithm is performing pure math and nothing it does would cause the goroutine to enter into a natural waiting state This means using one Goroutine per OShardware thread is all that is needed for good throughput Listing 2 below is my concurrent version of add Listing 2 44 func addConcurrentgoroutines int numbers int int 45 var v int64 46 totalNumbers lennumbers 47 lastGoroutine goroutines 1 48 stride totalNumbers goroutines 49 50 var wg syncWaitGroup 51 wgAddgoroutines 52 53 for g 0 g goroutines g 54 go funcg int 55 start g stride 56 end start stride 57 if g lastGoroutine 58 end totalNumbers 59 60 61 var lv int 62 for n range numbersstartend 63 lv n 64 65 66 atomicAddInt64v int64lv 67 wgDone 68 g 69 70 71 wgWait 72 73 return intv 74 323 Note There are several ways and options you can
take when writing a concurrent version of add Dont get hung up on my particular implementation at this time If you have a more readable version that performs the same or better I would love for you to share it Listing 2 shows a concurrent version of the add function This version uses 26 lines of code as opposed to the 5 lines of code for the nonconcurrent version There is a lot of code so I will only highlight the important lines to understand Line 48 Each Goroutine will get their own unique but smaller list of numbers to add The size of the list is calculated by taking the size of the collection and dividing it by the number of Goroutines Line 53 The pool of Goroutines is created to perform the adding work Line 5759 The last Goroutine will add the remaining list of numbers which may be greater than the other Goroutines Line 66 The sum of the smaller lists are summed together into a final sum The concurrent version is definitely more complex than the sequential version But is the complexity worth it The best way to answer that question is to create a benchmark For these benchmarks I have used a collection of 10 million numbers with the garbage collector turned off Listing 3 shows a sequential version that uses the add function and a concurrent version that uses the addConcurrent function Listing 3 func BenchmarkSequentialb testingB for i 0 i bN i addnumbers
func BenchmarkConcurrentb testingB for i 0 i bN i addConcurrentruntimeNumCPU numbers Here are the results when only a single OShardware thread is available for all Goroutines The sequential version is using 1 Goroutine and the concurrent version is using runtimeNumCPU or 8 Goroutines on my machine In this case the concurrent version is leveraging concurrency without parallelism 324 Listing 4 10 Million Numbers using 8 goroutines with 1 core 29 GHz Intel 4 Core i7 Concurrency WITHOUT Parallelism GOGCoff go test cpu 1 run none bench benchtime 3s goos darwin goarch amd64 pkg githubcomardanlabsgotrainingtopicsgotestingbenchmarkscpubound BenchmarkSequential 1000 5720764 nsop 10 Faster BenchmarkConcurrent 1000 6387344 nsop BenchmarkSequentialAgain 1000 5614666 nsop 13 Faster BenchmarkConcurrentAgain 1000 6482612 nsop Note Running a benchmark on your local machine is complicated There are so many factors that can cause your benchmarks to be inaccurate Make sure your machine is as idle as possible and run benchmarks a few times You want to make sure you see consistency in the results Having the benchmark run twice by the testing tool is giving this benchmark the most consistent results The benchmark in listing 4 shows that the Sequential version is approximately 10 to 13 percent faster than the Concurrent when only a single OShardware thread is available for all Goroutines This is what I would have expected since the concurrent version has the overhead of context switches on that single OS thread and the management of the Goroutines Here are the results when an individual OShardware thread is available
for each Goroutine The sequential version is using 1 Goroutine and the concurrent version is using runtimeNumCPU or 8 Goroutines on my machine In this case the concurrent version is leveraging concurrency with parallelism Listing 5 10 Million Numbers using 8 goroutines with 8 cores 29 GHz Intel 4 Core i7 Concurrency WITH Parallelism GOGCoff go test cpu 8 run none bench benchtime 3s goos darwin goarch amd64 pkg githubcomardanlabsgotrainingtopicsgotestingbenchmarkscpubound BenchmarkSequential8 1000 5910799 nsop BenchmarkConcurrent8 2000 3362643 nsop 43 Faster BenchmarkSequentialAgain8 1000 5933444 nsop BenchmarkConcurrentAgain8 2000 3477253 nsop 41 Faster The benchmark in listing 5 shows that the concurrent version is approximately 41 to 43 percent faster than the sequential version when an individual OShardware 325 thread is available for each Goroutine This is what I would have expected since all the Goroutines are now running in parallel eight Goroutines executing their concurrent work at the same time Sorting Its important to understand that not all CPUBound workloads are suitable for concurrency This is primarily true when its very expensive to either break work up andor combine all the results An example of this can be seen with the sorting algorithm called Bubble sort Look at the following code that implements Bubble sort in Go Listing 6 01 package main 02 03 import fmt 04 05 func bubbleSortnumbers int 06 n lennumbers 07 for i 0 i n i 08 if sweepnumbers i 09 return 10 11 12 13 14 func sweepnumbers int currentPass int bool 15 var idx int
16 idxNext idx 1 17 n lennumbers 18 var swap bool 19 20 for idxNext n currentPass 21 a numbersidx 22 b numbersidxNext 23 if a b 24 numbersidx b 25 numbersidxNext a 26 swap true 27 28 idx 29 idxNext idx 1 30 31 return swap 32 33 34 func main 35 org int1 3 2 4 8 6 7 2 3 0 36 fmtPrintlnorg 326 37 38 39 40 bubbleSortorg fmtPrintlnorg In listing 6 there is an example of Bubble sort written in Go This sorting algorithm sweeps through a collection of integers swapping values on every pass Depending on the ordering of the list it may require multiple passes through the collection before everything is sorted Question is the bubbleSort function a workload that is suitable for out of order execution I believe the answer is no The collection of integers could be broken up into smaller lists and those lists could be sorted concurrently However after all the concurrent work is done there is no efficient way to sort the smaller lists together Here is an example of a concurrent version of Bubble sort Listing 8 01 func bubbleSortConcurrentgoroutines int numbers int 02 totalNumbers lennumbers 03 lastGoroutine goroutines 1 04 stride totalNumbers goroutines 05 06 var wg syncWaitGroup 07 wgAddgoroutines 08 09 for g 0 g goroutines g 10 go funcg int 11 start g stride 12 end start stride 13 if g lastGoroutine 14 end totalNumbers 15 16 17 bubbleSortnumbersstartend 18 wgDone 19 g 20
21 22 wgWait 23 24 Ugh we have to sort the entire list again 25 bubbleSortnumbers 26 In Listing 8 the bubbleSortConcurrent function is presented which is a concurrent version of the bubbleSort function It uses multiple Goroutines to sort portions of the list concurrently However what you are left with is a list of sorted values in chunks Given a list of 36 numbers split in groups of 12 this would be the resulting 327 list if the entire list is not sorted once more on line 25 Listing 9 Before 25 51 15 57 87 10 10 85 90 32 98 53 91 82 84 97 67 37 71 94 26 2 81 79 66 70 93 86 19 81 52 75 85 10 87 49 After 10 10 15 25 32 51 53 57 85 87 90 98 2 26 37 67 71 79 81 82 84 91 94 97 10 19 49 52 66 70 75 81 85 86 87 93 Since the nature of Bubble sort is to sweep through the list the call to bubbleSort on line 25 will negate any potential gains from using concurrency With Bubble sort there is no performance gain by using concurrency Reading Files Two CPUBound workloads have been presented but what about an IOBound workload Are the semantics different when Goroutines are naturally moving in and out of waiting states Look at an IOBound workload that reads files and performs a text search This first version is a
sequential version of a function called find Listing 10 42 func findtopic string docs string int 43 var found int 44 for doc range docs 45 items err readdoc 46 if err nil 47 continue 48 49 for item range items 50 if stringsContainsitemDescription topic 51 found 52 53 54 55 return found 56 In listing 10 you see the sequential version of the find function On line 43 a variable named found is declared to maintain a count for the number of times the specified topic is found inside a given document Then on line 44 the documents are iterated over and each document is read on line 45 using the read function 328 Finally on line 4953 the Contains function from the strings package is used to check if the topic can be found inside the collection of items read from the document If the topic is found the found variable is incremented by one Here is the implementation of the read function that is being called by find Listing 11 33 func readdoc string item error 34 timeSleeptimeMillisecond Simulate blocking disk read 35 var d document 36 if err xmlUnmarshalbytefile d err nil 37 return nil err 38 39 return dChannelItems nil 40 The read function in listing 11 starts with a timeSleep call for one millisecond This call is being used to mock the latency that could be produced if we performed an actual system call to read the document from disk The consistency of this
latency is important for accurately measuring the performance of the sequential version of find against the concurrent version Then on lines 3539 the mock xml document stored in the global variable file is unmarshaled into a struct value for processing Finally a collection of items is returned back to the caller on line 39 With the sequential version in place here is the concurrent version Note There are several ways and options you can take when writing a concurrent version of find Dont get hung up on my particular implementation at this time If you have a more readable version that performs the same or better I would love for you to share it 329 Listing 12 58 func findConcurrentgoroutines int topic string docs string int 59 var found int64 60 61 ch makechan string lendocs 62 for doc range docs 63 ch doc 64 65 closech 66 67 var wg syncWaitGroup 68 wgAddgoroutines 69 70 for g 0 g goroutines g 71 go func 72 var lFound int64 73 for doc range ch 74 items err readdoc 75 if err nil 76 continue 77 78 for item range items 79 if stringsContainsitemDescription topic 80 lFound 81 82 83 84 atomicAddInt64found lFound 85 wgDone 86 87 88 89 wgWait 90 91 return intfound 92 Listing 12 shows a concurrent version of the find function This version uses 30 lines of code as opposed to the 13 lines of code for the nonconcurrent version My goal in implementing the concurrent version
was to control the number of Goroutines that are used to process the unknown number of documents A pooling pattern where a channel is used to feed the pool of Goroutines was my choice There is a lot of code so I will only highlight the important lines to understand Lines 6164 A channel is created and populated with all the documents to process 330 Line 65 The channel is closed so the pool of Goroutines naturally terminates when all the documents are processed Line 70 The pool of Goroutines is created Line 7383 Each Goroutine in the pool receives a document from the channel reads the document into memory and checks the contents for the topic When there is a match the local found variable is incremented Line 84 The sum of the individual Goroutine counts are summed together into a final count The concurrent version is definitely more complex than the sequential version But is the complexity worth it The best way to answer this question again is to create a benchmark For these benchmarks I have used a collection of one thousand documents with the garbage collector turned off There is a sequential version that uses the find function and a concurrent version that uses the findConcurrent function Listing 13 func BenchmarkSequentialb testingB for i 0 i bN i findtest docs func BenchmarkConcurrentb testingB for i 0 i bN i findConcurrentruntimeNumCPU test docs Listing 13 shows the benchmark functions Here are the results when only a single
OShardware thread is available for all Goroutines The sequential is using 1 Goroutine and the concurrent version is using runtimeNumCPU or 8 Goroutines on my machine In this case the concurrent version is leveraging concurrency without parallelism 331 Listing 14 10 Thousand Documents using 8 goroutines with 1 core 29 GHz Intel 4 Core i7 Concurrency WITHOUT Parallelism GOGCoff go test cpu 1 run none bench benchtime 3s goos darwin goarch amd64 pkg githubcomardanlabsgotrainingtopicsgotestingbenchmarksiobound BenchmarkSequential 3 1483458120 nsop BenchmarkConcurrent 20 188941855 nsop 87 Faster BenchmarkSequentialAgain 2 1502682536 nsop BenchmarkConcurrentAgain 20 184037843 nsop 88 Faster The benchmark in listing 14 shows that the concurrent version is approximately 87 to 88 percent faster than the sequential version when only a single OShardware thread is available for all Goroutines This is what I would have expected since all the Goroutines are efficiently sharing the single OShardware thread The natural context switch happening for each Goroutine on the read call is allowing more work to get done over time on the single OShardware thread Here is the benchmark when using concurrency with parallelism Listing 15 10 Thousand Documents using 8 goroutines with 1 core 29 GHz Intel 4 Core i7 Concurrency WITH Parallelism GOGCoff go test run none bench benchtime 3s goos darwin goarch amd64 pkg githubcomardanlabsgotrainingtopicsgotestingbenchmarksiobound BenchmarkSequential8 3 1490947198 nsop BenchmarkConcurrent8 20 187382200 nsop 88 Faster BenchmarkSequentialAgain8 3 1416126029 nsop BenchmarkConcurrentAgain8 20 185965460 nsop 87 Faster The benchmark in listing 15 shows that bringing in the extra OShardware threads doesnt provide any better
performance Conclusion The goal of this post was to provide guidance on the semantics you must consider to determine if a workload is suitable for using concurrency I tried to provide examples of different types of algorithms and workloads so you could see the differences in semantics and the different engineering decisions that needed to be considered 332 You can clearly see that with IOBound workloads parallelism was not needed to get a big bump in performance This is the opposite of what you saw with the CPUBound work When it comes to an algorithm like Bubble sort the use of concurrency would add complexity without any real benefit of performance Its important to determine if your workload is suitable for concurrency and then identify the type of workload you have to use the right semantics 333
146 Garbage Collection Semantics Introduction Garbage collectors have the responsibility of tracking heap memory allocations freeing up allocations that are no longer needed and keeping allocations that are still inuse How a language decides to implement this behavior is complex but it shouldnt be a requirement for application developers to understand the details in order to build software Plus with different releases of a languages VM or runtime the implementation of these systems is always changing and evolving Whats important for application developers is to maintain a good working model of how the garbage collector for their language behaves and how they can be sympathetic with that behavior without being concerned with the implementation As of version 116 the Go programming language uses a nongenerational noncompacting concurrent tricolor mark and sweep collector If you want to visually see how a mark and sweep collector works Ken Fox wrote this great article and provides an animation httpsspinatomicobjectcom20140903visualizinggarbagecollectionalgorithms The implementation of Gos collector has changed and evolved with every release of Go So any post that talks about the implementation details will no longer be accurate once the next version of the language is released With all that said the modeling I will do in this post will not focus on the actual implementation details The modeling will focus on the behavior you will experience and the behavior you should expect to see for years to come In this post I will share with you the behavior of the collector and explain
how to be sympathetic with that behavior regardless of the current implementation or how it changes in the future This will make you a better Go developer Here is more reading you can do about garbage collectors and Gos actual collector as well httpsgithubcomardanlabsgotrainingtreemasterreadinggarbagecollection I will never refer to the heap as a container that you can store or release values from Its important to understand that there is no linear containment of memory that defines the Heap Think that any memory reserved for application use in the process space is available for heap memory allocation Where any given heap memory allocation is virtually or physically stored is not relevant to our model This 334 understanding will help you better understand how the garbage collector works Collector Behavior When a collection starts the collector runs through three phases of work Two of these phases create Stop The World STW latencies and the other phase creates latencies that slow down the throughput of the application The collector does its best to keep the STW latencies under 100 microseconds for every GC I label these three phases as Mark Setup STW Marking Concurrent Mark Termination STW Here is a breakdown of each phase Mark Setup STW When a collection starts the first activity that must be performed is turning on the Write Barrier The purpose of the Write Barrier is to allow the collector to maintain data integrity on the heap during a collection since both the collector and application Goroutines will
be running concurrently In order to turn the Write Barrier on every application Goroutine running must be stopped That is as long as the application Goroutines are behaving properly Figure 1 Figure 1 shows 4 application Goroutines running before the start of a collection Each of those 4 Goroutine must be stopped The only way to do that is for the collector to watch and wait for each Goroutine to enter into a safe point A safe point is where the Goroutine is not in the middle of a read or write of memory Prior to the Goroutine being a preemptive scheduler function calls were the only guarantee that a Goroutine was at a safe point to be stopped Prior to the scheduler change in 114 what would happen if one of those four Goroutines didnt make a 335 function call Figure 2 Figure 2 shows a real problem The collection cant start until the Goroutine running on P4 is stopped and that cant happen because its in a tight loop performing some math Listing 1 01 func addnumbers int int 02 var v int 03 for n range numbers 04 v n 05 06 return v 07 Listing 1 shows the code that the Goroutine running on P4 is executing Depending on the size of the slice the Goroutine could run for an unreasonable amount of time with no opportunity to be stopped This is the kind of code that could stall a collection from starting Whats worse is
the other Ps cant service any other Goroutines while the collector waits Its critically important if you are using a version of Go prior to 114 that Goroutines make function calls in reasonable timeframes Marking Concurrent Once the Write Barrier is turned on the collector commences with the Marking phase The first thing the collector does is take 25 of the available CPU capacity for itself The collector uses Goroutines to do the collection work and needs the same Ps and Ms the application Goroutines use This means for our 4 threaded Go program one entire P will be dedicated to collection work 336 Figure 3 Figure 3 shows how the collector took P1 for itself during the collection Now the collector can start the Marking phase The Marking phase consists of marking values in heap memory that are still inuse This work starts by inspecting the stacks for all existing Goroutines to find root pointers to heap memory Then the collector must traverse the heap memory graph from those root pointers While the Marking work is happening on P1 application work can continue concurrently on P2 P3 and P4 This means the impact of the collector has been minimized to 25 of the current CPU capacity I wish that was the end of the story but it isnt What if its identified during the collection that the Goroutine dedicated to GC on P1 will not finish the Marking work before the heap memory inuse reaches its goal limit set
by the collection before starting In this case new allocations have to be slowed down and specifically from those Goroutines If the collector determines that it needs to slow down allocations it will recruit the application Goroutines to assist with the Marking work This is called a Mark Assist The amount of time any application Goroutine will be placed in a Mark Assist is proportional to the amount of data its adding to heap memory One positive side effect of Mark Assist is that it helps to finish the collection faster Figure 4 Figure 4 shows how the application Goroutine running on P3 is now performing a Mark Assist and helping with the collection work Hopefully the other application Goroutines dont need to get involved as well Applications that allocate heavy could 337 see the majority of the running Goroutines perform small amounts of Mark Assist during collections One goal of the collector is to eliminate the need for Mark Assists If any given collection ends up requiring a lot of Mark Assist the collector can start the next garbage collection earlier This is done in an attempt to reduce the amount of Mark Assist that will be necessary on the next collection Mark Termination STW Once the Marking work is done the next phase is Mark Termination This is when the Write Barrier is turned off various clean up tasks are performed and the next collection goal is calculated Goroutines in versions of Go prior to 114 that find
themselves in a tight loop during the Marking phase can also cause Mark Termination STW latencies to be extended Figure 5 Figure 5 shows how all the Goroutines are stopped while the Mark Termination phase completes This phase could be done without a STW but by using a STW the code is simpler and the added complexity is not worth the small gain Once the collection is finished every P can be used by the application Goroutines again and the application is back to full throttle Figure 6 Figure 6 shows how all of the available Ps are now processing application work 338 again once the collection is finished The application is back to full throttle as it was before the collection started Sweeping Concurrent There is another activity that happens after a collection is finished called Sweeping Sweeping is when the memory associated with values in heap memory that were not marked as inuse are reclaimed This activity occurs when application Goroutines attempt to allocate new values in heap memory The latency of Sweeping is added to the cost of performing an allocation in heap memory and is not tied to any latencies associated with garbage collection The following is a sample of a trace on my machine where I have 12 hardware threads available for executing Goroutines Figure 7 Figure 7 shows a partial snapshot of the trace You can see how during this collection keep your view within the blue GC line at the top three of
the twelve Ps are dedicated to GC You can see Goroutine 2450 1978 and 2696 during this time are performing moments of Mark Assist work and not its application work At the very end of the collection only one P is dedicated to GC and eventually performs the STW Mark Termination work After the collection is finished the application is back to running at full throttle Except you see a lot of rose colored lines underneath those Goroutines 339 Figure 8 Figure 8 shows how those rose colored lines represent moments when the Goroutine is performing the Sweeping work and not its application work These are moments when the Goroutine is attempting to allocate new values in heap memory Figure 9 Figure 9 shows the end of the stack trace for one of the Goroutines in the Sweep activity The call to runtimemallocgc is the call to allocate a new value in heap memory The call to runtimemcachenextFree is causing the Sweep activity Once there are no more allocations in heap memory to reclaim the call to nextFree wont be seen any longer The collection behavior that was just described only happens when a collection has started and is running The GC Percentage configuration option plays a big role in determining when a collection starts 340 GC Percentage There is a configuration option in the runtime called GC Percentage which is set to 100 by default This value represents a ratio of how much new heap memory can be allocated
before the next collection has to start Setting the GC Percentage to 100 means based on the amount of heap memory marked as live after a collection finishes the next collection has to start at or before 100 more new allocations are added to heap memory As an example imagine a collection finishes with 2MB of heap memory inuse Note The diagrams of the heap memory in this post do not represent a true profile when using Go The heap memory in Go will often be fragmented and messy and you dont have the clean separation as the images are representing These diagrams provide a way to visualize heap memory in an easier to understand way that is accurate towards the behavior you will experience Figure 10 Figure 10 shows the 2MB of heap memory inuse after the last collection finished Since the GC Percentage is set to 100 the next collection needs to start at or before 2 more MB of heap memory is added Figure 11 Figure 11 shows that 2 more MB of heap memory is now inuse This will trigger a collection A way to view all of this in action is to generate a GC trace for every collection that takes place GC Trace A GC trace can be generated by including the environment variable GODEBUG with the gctrace1 option when running any Go application Every time a collection happens the runtime will write the GC trace information to stderr 341 Listing 2 GODEBUGgctrace1 app
gc 1405 6068s 11 0058120083 ms clock 07025150099 ms cpu 7116 MB 10 MB goal 12 P gc 1406 6070s 11 0051180076 ms clock 06120250091 ms cpu 8116 MB 13 MB goal 12 P gc 1407 6073s 11 005218020 ms clock 0621522024 ms cpu 8148 MB 13 MB goal 12 P Listing 2 shows how to use the GODEBUG variable to generate GC traces The listing also shows 3 traces that were generated by the running Go application Here is a breakdown of what each value in the GC trace means by reviewing the first GC trace line in the listing Listing 3 gc 1405 6068s 11 0058120083 ms clock 07025150099 ms cpu 7116 MB 10 MB goal 12 P General gc 1405 6068s 11 The 1405 GC run since the program started Six seconds since the program started Eleven percent of the available CPU has been spent in GC WallClock 0058ms STW Mark Start Write Barrier on 12ms Concurrent Marking 0083ms STW Mark Termination Write Barrier off and clean up CPU Time 070ms STW Mark Start 25ms Concurrent Mark Assist Time GC performed in line with allocation 15ms Concurrent Mark Background GC time 0ms Concurrent Mark Idle GC time 099ms STW Mark Term Memory 7MB 11MB 6MB 10MB Heap memory inuse before the Marking started Heap memory inuse after the Marking finished Heap memory marked as live after the Marking finished Collection goal for heap memory inuse after Marking finished Threads 12P Number of logical processors or threads used
to run Goroutines Listing 3 shows the actual numbers from the first GC trace line broken down by 342 what the values mean I will eventually talk about most of these values but for now just focus on the memory section of the GC trace for trace 1405 Figure 12 Listing 4 Memory 7MB 11MB 6MB 10MB Heap memory inuse before the Marking started Heap memory inuse after the Marking finished Heap memory marked as live after the Marking finished Collection goal for heap memory inuse after Marking finished What this GC trace line is telling you in listing 4 is that the amount of heap memory inuse was 7MB before the Marking work started When the Marking work finished the amount of heap memory inuse reached 11MB Which means there was an additional 4MB of allocations that occurred during the collection The amount of heap memory that was marked as live after the Marking work finished was 6MB 343 This means the application can increase the amount of heap memory inuse to 12MB 100 of the live heap size of 6MB before the next collection needs to start You can see that the collector missed its goal by 1MB The amount of heap memory inuse after the Marking work finished was 11MB not 10MB Thats ok because the goal is calculated based on the current amount of the heap memory inuse the amount of heap memory marked as live and timing calculations about the additional allocations that will occur
while the collection is running In this case the application did something that required more heap memory to be inuse after Marking than expected If you look at the next GC trace line 1406 you will see how things changed within 2ms Figure 13 344 Listing 5 gc 1406 6070s 11 0051180076 ms clock 06120250091 ms cpu 8116 MB 13 MB goal 12 P Memory 8MB 11MB 6MB 13MB Heap memory inuse before the Marking started Heap memory inuse after the Marking finished Heap memory marked as live after the Marking finished Collection goal for heap memory inuse after Marking finished Listing 5 shows how this collection started 2ms after the start of the previous collection 6068s vs 6070s even though the heap memory inuse had only reached 8MB of the 12MB that was allowed Its important to note if the collector decides its better to start a collection earlier it will In this case it probably started earlier because the application is allocating heavily and the collector wanted to reduce the amount of Mark Assist latency during this collection Two more things of note The collector stayed within its goal this time The amount of heap memory inuse after Marking finished was 11MB not 13MB 2 MB less The amount of heap memory marked as live after Marking finished was the same at 6MB As side note you can get more details from the GC trace by adding the gcpacertrace1 flag This causes the collector to print information about
the internal state of the concurrent pacer Listing 6 export GODEBUGgctrace1gcpacertrace1 app Sample output gc 5 0071s 0 00180460071 ms clock 0140038014056 ms cpu 292929 MB 30 MB goal 8 P pacer sweep done at heap size 29MB allocated 0MB of spans swept 3752 pages at 6183550e004 pagesbyte pacer assist ratio1232155e000 scan 1 MB in 7071 MB workers20 pacer Hmprev30488736 ht2334071e001 HT37605024 ha1409842e000 Ha73473040 hg1000000e000 Hg60977472 ua2500000e001 ug2500000e001 Wa308200 goal7665929e001 actual1176435e000 uaug1000000e000 Running a GC trace can tell you a lot about the health of the application and the pace of the collector The pace at which the collector is running plays an important role in the collection process 345 Pacing The collector has a pacing algorithm which is used to determine when a collection is to start The algorithm depends on a feedback loop that the collector uses to gather information about the running application and the stress the application is putting on the heap Stress can be defined as how fast the application is allocating heap memory within a given amount of time Its that stress that determines the pace at which the collector needs to run Before the collector starts a collection it calculates the amount of time it believes it will take to finish the collection Then once a collection is running latencies will be inflicted on the running application that will slow down application work Every collection adds to the overall latency of the application One misconception is thinking that slowing down the pace of
the collector is a way to improve performance The idea being if you can delay the start of the next collection then you are delaying the latency it will inflict Being sympathetic with the collector isnt about slowing down the pace You could decide to change the GC Percentage value to something larger than 100 This will increase the amount of heap memory that can be allocated before the next collection has to start This could result in the pace of collection to slow down Dont consider doing this Figure 14 Figure 14 shows how changing the GC Percentage would change the amount of heap memory allowed to be allocated before the next collection has to start You can visualize how the collector could be slowed down as it waits for more heap memory to become inuse Attempting to directly affect the pace of collection has nothing to do with being sympathetic with the collector Its really about getting more work done between each collection or during the collection You affect that by reducing the amount or the number of allocations any piece of work is adding to heap memory Note The idea is also to achieve the throughput you need with the smallest heap possible Remember minimizing the use of resources like heap memory is 346 important when running in cloud environments Figure 15 Listing 15 shows some statistics of a running Go application The version in blue shows stats for the application without any optimizations when 10k requests
are processed through the application The version in green shows stats after 448GB of nonproductive memory allocations were found and removed from the application for the same 10k requests Look at the average pace of collection for both versions 208ms vs 196ms They are virtually the same at around 20ms What fundamentally changed between these two versions is the amount of work that is getting done between each collection The application went from processing 398 to 713 requests per collection That is a 791 increase in the amount of work getting done at the same pace As you can see the collection did not slow down with the reduction of those allocations but remained the same The win came from getting more work done inbetween each collection Adjusting the pace of the collection to delay the latency cost is not how you improve the performance of your application Its about reducing the amount of time the collector needs to run which in turn will reduce the amount of latency cost being inflicted The latency costs inflicted by the collector have been explained but let me summarize it again for clarity Collector Latency Costs There are two types of latencies every collection inflicts on your running application The first is the stealing of CPU capacity The effect of this stolen CPU capacity means your application is not running at full throttle during the collection The application Goroutines are now sharing Ps with the collectors Goroutines or helping with the collection Mark
Assist 347 Figure 16 Figure 16 shows how the application is only using 75 of its CPU capacity for application work This is because the collector has dedicated P1 for itself This is true for the majority of the collection Figure 17 Figure 17 shows how the application in this moment of time typically for just a few microseconds is now only using half of its CPU capacity for application work This is because the Goroutine on P3 is performing a Mark Assist and the collector has dedicated P1 for itself Note Marking usually takes 4 CPUmilliseconds per MB of live heap eg to estimate how many milliseconds the Marking phase will run for take the live heap size in MB and divide by 025 the number of CPUs Marking actually runs at about 1 MBms but only has a quarter of the CPUs The second latency that is inflicted is the amount of STW latency that occurs during the collection The STW time is when no application Goroutines are performing any of their application work The application is essentially stopped 348 Figure 18 Figure 18 is showing STW latency where all the Goroutines are stopped This happens twice on every collection If your application is healthy the collector should be able to keep the total STW time at or below 100 microsecond for the majority of collections You now know the different phases of the collector how memory is sized how pacing works and the different latencies the collector
inflicts on your running application With all that knowledge the question of how you can be sympathetic with the collector can finally be answered Being Sympathetic Being sympathetic with the collector is about reducing stress on heap memory Remember stress can be defined as how fast the application is allocating heap memory within a given amount of time When stress is reduced the latencies being inflicted by the collector will be reduced Its the GC latencies that are slowing down your application The way to reduce GC latencies is by identifying and removing unnecessary allocations from your application Doing this will help the collector to Maintain the smallest heap possible Find an optimal consistent pace Stay within the goal for every collection Minimize the duration of every collection STW and Mark Assist All these things help reduce the amount of latency the collector will inflict on your running application That will increase the performance and throughput of your application The pace of the collection has nothing to do with it Conclusion If you take the time to focus on reducing allocations you are doing what you can as a Go developer to be sympathetic with the garbage collector You are not going to 349 write zero allocation applications so its important to recognize the difference between allocations that are productive those helping the application and those that are not productive those hurting the application Then put your faith and trust in the garbage collector to keep the heap healthy and
your application running consistently Having a garbage collector is a nice tradeoff I will take the cost of garbage collection so I dont have the burden of memory management Go is about allowing you as a developer to be productive while still writing applications that are fast enough The garbage collector is a big part of making that a reality 350
